{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Autolab Docs Rationale Autolab is a course management platform that enables instructors to offer autograded programming assignments to their students. The two key ideas in Autolab are autograding \u2013that is, programs evaluating other programs, and scoreboards that display the latest autograded scores for each student. Autograding : The model for a traditional programming class is that students work on their code, hand it in once, and then get feedback a week or two later, at which point they've already moved on to the next assignment. Autograding, on the other hand, allows students to get immediate feedback on their performance and become more motivated to refine their coursework. Scoreboard : The scoreboard is a fun and powerful motivation for students. When coupled with autograding, it creates a sense of community and healthy competition that benefits everyone. Students anonymize themselves on the scoreboard by giving themselves nicknames. A mix of curiosity and competitiveness drives the stronger students to be at the top of the scoreboard, and all students have a clear idea of what they need for full credit. Autolab also provides gradebooks, rosters, handins/handouts, lab writeups, code annotation, manual grading, late penalties, grace days, cheat checking, meetings, partners, and bulk emails. For more of the rationale behind Autolab, please check out this blog post . Components Autolab consists of two services: (1) the Autolab frontend which is implemented using Ruby on Rails, and (2) Tango, the RESTful Python autograding server. Either service can run independently without the other. But in order to use all features of Autolab, we highly recommend installing both services. While the Autolab frontend supports Autolab's web application framework, the backend Tango is responsible for distributing and completing autograding jobs, which run in virtual machines or containers (we currently support Docker and AWS virtual machines). When Tango is done running a job, it then sends the autograded result back to the frontend. Below is a visualization of the typical workflow of the Autolab system. As you can see on the left, the Autolab frontend receives handin files from the clients through either traditional browser interaction or the command line interface (CLI) . The frontend then sends the files through http requests to Tango. Tango adds them to a job queue, assigns them to available containers/virtual machines for grading through ssh, and shepherds the jobs through the process. On the right, inside the domain box for Docker at CMU, we show 3 VM pools - rhel, rhel122, and rhel411 - each with potentially different software packages. At the bottom right, we show an example of an AWS VM domain with VM pools rhelPKU and ubuntu. Tango assigns jobs only to the virtual machine instances from their corresponding course\u2019s VM pool. For example, jobs with handin files for the course 122 would only go to rhel122\u2019s instances. Once a job is done, the feedback is copied back to Tango through ssh and sent back to the Autolab frontend through http. The frontend then displays the feedback in the browser or through the CLI. It also updates the scoreboard if applicable and stores the feedback into the database, which is displayed at the bottom left. Apart from client usage, both the Autolab frontend and Tango provide application programming interfaces (API) for developers. The specific guides are included in the Reference section. Demonstration Website Installation instructions can be found in our comprehensive installation guide . If this is your first experience with Autolab, we encourage you to try out some key features on Autolab's Demo Site . Login with the following test credentials Email address: admin@demo.bar Password: adminfoobar The demonstration website refreshes daily and it is publicly accessible. Do not use the site to store important information. After you have successfully logged in, try the user flow below 1. Create a new course Click on Manage Autolab (top-right navigation bar) > Create New Course . Fill in the name and semester, and then create to see your course on the homepage. (NOTE: the email doesn't need to be real here) 2. Create an Autograded Lab Assessment. Go into the course you have just created, click on Install Assessment . You can install a simple autograded lab, called hello lab. Download hello.tar and install it using the Import from Tarball option. In the hello lab, students are asked to write a file called hello.c . The autograder checks that the submitted hello.c program compiles and runs with an exit status of zero. If so, the submission gets 100 points. Otherwise it gets 0 points. Try submitting to the autograded hello lab Create and submit a hello.c file. #include <stdio.h> int main () { printf ( \"Hello, World!\" ); return 0 ; } Refresh the submitted entries page to see the autograded score appear Click on a sub score, in this case the 100.0 under the Correctness heading, to see the output from the autograder. For more information on hello lab, or how to create your own lab, go to Guide for Lab Authors ! You can also check out other sample autograders on our Autograders repository . 3. Create a PDF homework assessment Autolab can also handle pdf submissions as well! Click on Install Assessment , then on Assessment Builder . Name your assessment, and give it a category and click Create Assessment !. Because it defaults to accepting .c files, we would like to change it to *.pdf . Click on Edit Assessment > Handin and then change the Handin filename to handin.pdf instead of handin.c and save the changes Try submitting to the pdf homework asssessment. Submit a .pdf file. Look at your submission using the magnifying glass icon 4. Grading submissions Click on View Gradesheet , and then the arrow button to open up student submissions. For details on the relevant features for an Instructor, go to Guide for Instructors .","title":"Overview"},{"location":"#welcome-to-the-autolab-docs","text":"","title":"Welcome to the Autolab Docs"},{"location":"#rationale","text":"Autolab is a course management platform that enables instructors to offer autograded programming assignments to their students. The two key ideas in Autolab are autograding \u2013that is, programs evaluating other programs, and scoreboards that display the latest autograded scores for each student. Autograding : The model for a traditional programming class is that students work on their code, hand it in once, and then get feedback a week or two later, at which point they've already moved on to the next assignment. Autograding, on the other hand, allows students to get immediate feedback on their performance and become more motivated to refine their coursework. Scoreboard : The scoreboard is a fun and powerful motivation for students. When coupled with autograding, it creates a sense of community and healthy competition that benefits everyone. Students anonymize themselves on the scoreboard by giving themselves nicknames. A mix of curiosity and competitiveness drives the stronger students to be at the top of the scoreboard, and all students have a clear idea of what they need for full credit. Autolab also provides gradebooks, rosters, handins/handouts, lab writeups, code annotation, manual grading, late penalties, grace days, cheat checking, meetings, partners, and bulk emails. For more of the rationale behind Autolab, please check out this blog post .","title":"Rationale"},{"location":"#components","text":"Autolab consists of two services: (1) the Autolab frontend which is implemented using Ruby on Rails, and (2) Tango, the RESTful Python autograding server. Either service can run independently without the other. But in order to use all features of Autolab, we highly recommend installing both services. While the Autolab frontend supports Autolab's web application framework, the backend Tango is responsible for distributing and completing autograding jobs, which run in virtual machines or containers (we currently support Docker and AWS virtual machines). When Tango is done running a job, it then sends the autograded result back to the frontend. Below is a visualization of the typical workflow of the Autolab system. As you can see on the left, the Autolab frontend receives handin files from the clients through either traditional browser interaction or the command line interface (CLI) . The frontend then sends the files through http requests to Tango. Tango adds them to a job queue, assigns them to available containers/virtual machines for grading through ssh, and shepherds the jobs through the process. On the right, inside the domain box for Docker at CMU, we show 3 VM pools - rhel, rhel122, and rhel411 - each with potentially different software packages. At the bottom right, we show an example of an AWS VM domain with VM pools rhelPKU and ubuntu. Tango assigns jobs only to the virtual machine instances from their corresponding course\u2019s VM pool. For example, jobs with handin files for the course 122 would only go to rhel122\u2019s instances. Once a job is done, the feedback is copied back to Tango through ssh and sent back to the Autolab frontend through http. The frontend then displays the feedback in the browser or through the CLI. It also updates the scoreboard if applicable and stores the feedback into the database, which is displayed at the bottom left. Apart from client usage, both the Autolab frontend and Tango provide application programming interfaces (API) for developers. The specific guides are included in the Reference section.","title":"Components"},{"location":"#demonstration-website","text":"Installation instructions can be found in our comprehensive installation guide . If this is your first experience with Autolab, we encourage you to try out some key features on Autolab's Demo Site . Login with the following test credentials Email address: admin@demo.bar Password: adminfoobar The demonstration website refreshes daily and it is publicly accessible. Do not use the site to store important information. After you have successfully logged in, try the user flow below","title":"Demonstration Website"},{"location":"#1-create-a-new-course","text":"Click on Manage Autolab (top-right navigation bar) > Create New Course . Fill in the name and semester, and then create to see your course on the homepage. (NOTE: the email doesn't need to be real here)","title":"1. Create a new course"},{"location":"#2-create-an-autograded-lab-assessment","text":"Go into the course you have just created, click on Install Assessment . You can install a simple autograded lab, called hello lab. Download hello.tar and install it using the Import from Tarball option. In the hello lab, students are asked to write a file called hello.c . The autograder checks that the submitted hello.c program compiles and runs with an exit status of zero. If so, the submission gets 100 points. Otherwise it gets 0 points. Try submitting to the autograded hello lab Create and submit a hello.c file. #include <stdio.h> int main () { printf ( \"Hello, World!\" ); return 0 ; } Refresh the submitted entries page to see the autograded score appear Click on a sub score, in this case the 100.0 under the Correctness heading, to see the output from the autograder. For more information on hello lab, or how to create your own lab, go to Guide for Lab Authors ! You can also check out other sample autograders on our Autograders repository .","title":"2. Create an Autograded Lab Assessment."},{"location":"#3-create-a-pdf-homework-assessment","text":"Autolab can also handle pdf submissions as well! Click on Install Assessment , then on Assessment Builder . Name your assessment, and give it a category and click Create Assessment !. Because it defaults to accepting .c files, we would like to change it to *.pdf . Click on Edit Assessment > Handin and then change the Handin filename to handin.pdf instead of handin.c and save the changes Try submitting to the pdf homework asssessment. Submit a .pdf file. Look at your submission using the magnifying glass icon","title":"3. Create a PDF homework assessment"},{"location":"#4-grading-submissions","text":"Click on View Gradesheet , and then the arrow button to open up student submissions. For details on the relevant features for an Instructor, go to Guide for Instructors .","title":"4. Grading submissions"},{"location":"api-interface/","text":"This page details all the endpoints of the Autolab REST API. The client's access token should be included as a parameter to all endpoints. For details on obtaining access tokens, please see the API Overview . Routing For version 1 of the API, all endpoints are under the path /api/v1/ . For example, to get user info, send a request to https://<host>/api/v1/user . Request & Response Format All endpoints expect the HTTP GET method unless otherwise specified. All parameters listed below are required unless denoted [OPTIONAL]. All responses are in JSON format. If the request is completed successfully, the HTTP response code will be 200. The reference below details the keys and their respective value types that the client can expect from each endpoint. If an error occurs, the response code will not be 200. The returned JSON will be an object with the key 'error'. Its value will be a string that explains the error. Notes on return value types All datetime formats are strings in the form of YYYY-MM-DDThh:mm:ss.sTZD , e.g. 2017-10-23T04:17:41.000-04:00 , which means 4:17:41 AM on October 23rd, 2017 US Eastern Time. JSON spec only has a 'number' type, but the spec below distinguishes between integers and floats for ease of use in certain languages. If a field does not exist, the value is generally null. Please be sure to check if a value is null before using it. Interface user Get basic user info. Scope: 'user_info' Endpoint: /user Parameters: [none] Responses: key type description first_name string The user's first name. last_name string The user's last name. email string The user's registered email. school string The school the user belongs to. major string The user's major of study. year string The user's year. courses Get all courses currently taking or taken before. Scope: 'user_courses' Endpoint: /courses Parameters: state [OPTIONAL] filter the courses by the state of the course. Should be one of 'disabled', 'completed', 'current', or 'upcoming'. If no state is provided, all courses are returned. Responses: A list of courses. Each course contains: key type description name string The unique url-safe name. display_name string The full name of the course. semester string The semester this course is being offered. late_slack integer The number of seconds after a deadline that the server will still accept a submission and not count it as late. grace_days integer AKA late days. The total number of days (over the entire semester) a student is allowed to submit an assessment late. auth_level string The user's level of access for this course. One of 'student', 'course_assistant', or 'instructor'. assessments Get all the assessments of a course. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments Parameters: [none] Responses: A list of assessments. If the user is only a student of the course, only released assessments are available. Otherwise, all assessments are available. Each assessment contains: key type description name string The unique url-safe name. display_name string The full name of the assessments. start_at datetime The time this assessment is released to students. due_at datetime Students can submit before this time without being penalized or using grace days. end_at datetime Last possible time that students can submit (except those granted extensions.) category_name string Name of the category this assessment belongs to. assessment details show Show detailed information of an assessment. Scope: 'user_courses' Endpoint: GET /courses/{course_name}/assessments/{assessment_name} Parameters: [none] Response: key type description name string The unique url-safe name. display_name string The full name of the assessments. description string A short description of the assessment. start_at datetime The time this assessment is released to students. due_at datetime Students can submit before this time without being penalized or using grace days. end_at datetime Last possible time that students can submit (except those granted extensions.) updated_at datetime The last time an update was made to the assessment. max_grace_days integer Maximum number of grace days that a student can spend on this assessment. max_submissions integer The maximum number of times a student can submit the assessment. -1 means unlimited submissions. max_unpenalized_submissions integer The maximum number of times the assessment can be submitted without incurring a penalty. -1 means unlimited submissions. disable_handins boolean Are handins disallowed by students? category_name string Name of the category this assessment belongs to. group_size integer The maximum size of groups for this assessment. writeup_format string The format of this assessment's writeup. One of 'none', 'url', or 'file'. handout_format string The format of this assessment's handout. One of 'none', 'url', or 'file'. has_scoreboard boolean Does this assessment have a scoreboard? has_autograder boolean Does this assessment use an autograder? max_total_score float The maximum total score for this assessment max_scores object An object with the problem name as the key, and the maximum score for the problem as the value set group settings set the group size of the assessment. Scope: 'user_courses' Endpoint: POST /courses/{course_name}/assessments/{assessment_name}/set_group_settings Parameters: key type description group_size integer the number of people in a group allow_student_assign_group boolean whether students are allowed to edit and self-assign groups Response: key type description group_size integer the number of people in a group allow_student_assign_group boolean whether students are allowed to edit and self-assign groups groups index List all groups in an assessment Scope: 'instructor_all' Endpoint: GET /courses/{course name}/assessments/{assessment name}/groups Parameters: show_members key type description show_members optional boolean whether to retrieve the members of each group or not Response: A JSON object containing the group_size, a list of groups, and the assessment containing the groups. If show_members is set to true, each group will have a list members of user objects that are members of that group. show Show the details of a group and its members Scope: 'instructor_all' Endpoint: GET /courses/{course name}/assessments/{assessment name}/groups/{id} Parameters: [none] Response: The requested group object. It contains a list members of user objects that are members of that group. create Create groups in the assessment, given the emails of the people in the group, and an optional group name. Scope: 'instructor_all' Endpoint: POST /courses/{course name}/assessments/{assessment name}/groups Parameters: Groups key type description groups required string List of group s to be created. Refer to group object. Group key type description name string Name of the group group_members required list of string List of emails of students in that group Example json object { \"groups\" : [{ \"name\": \"hello\", \"group_members\": [\"user@foo.bar\",\"user1@foo.bar\"] }, { \"name\": \"hello2\", \"group_members\": [\"user2@foo.bar\",\"\"user3@foo.bar\"] } ] } Response: A list of the groups created if successful. Otherwise an error message will be returned. destroy Delete a certain group of an assessment given the id Scope: 'instructor_all' Endpoint: DELETE /courses/{course name}/assessments/{assessment name}/groups/{id} Parameters: [none] Response: Success message if deleted. problems index Get all problems of an assessment. Scope: 'instructor_all' Endpoint GET /courses/{course_name}/assessments/{assessment_name}/problems Parameters: [none] Responses: A list of problems. Each problem contains: key type description name string Full name of the problem. description string Brief description of the problem. max_score float Maximum possible score for this problem. optional boolean Is this problem optional? starred boolean Is this problem starred? create Create a problem for an assessment. Scope: 'instructor_all' Endpoint POST /courses/{course_name}/assessments/{assessment_name}/problems Parameters: key type description name string Full name of the problem. description string Brief description of the problem. max_score float Maximum possible score for this problem. optional boolean Is this problem optional? Responses: The newly created problem. key type description name string Full name of the problem. description string Brief description of the problem. max_score float Maximum possible score for this problem. optional boolean Is this problem optional? scores index Get the submission scores for all users for an assessment. Scope: 'instructor_all' Endpoint GET /courses/{course_name}/assessments/{assessment_name}/scores Parameters: [none] Responses: A dictionary containing the submission data for each student that's made a submission. The keys are students' emails, and the values are a dictionary with keys equal to the submission number, and values equal to the scores for the graded problems. Example json object { \"student1@andrew.cmu.edu\" : { \"1\": { \"problem1\": 100.0, \"problem2\": 10.0 }, \"2\": { \"problem1\": 100.0, \"problem2\": 15.0 } }, \"student2@andrew.cmu.edu\" : { \"1\": {} } } show Get the submission scores for a user for an assessment. Scope: 'instructor_all' Endpoint GET /courses/{course_name}/assessments/{assessment_name}/scores/{email} Parameters: [none] Response: A dictionary containing the submission data for the student. The keys are the submission number, and values equal to the scores for the graded problems. Example json response: { \"1\": { \"Problem 1\": 100.0, \"Problem 2\": 10.0 } } update_latest Update the scores for a student's latest submission. Scope: 'instructor_all' Endpoint PUT /courses/{course_name}/assessments/{assessment_name}/scores/{email}/update_latest/ Parameters: key type description update_group_scores boolean Should the score update be propagated to the students in the student's group? problems json object Keys equal to the name of the problems to update, values equal to the updated score for a problem Responses: If any of the problems in problems does not exist for the assessment key type value error string \"Problem '...' not found in this assessment\" In this case, no score updates will be saved. If all of the problems in problems exist for the assessment The a dictionary with keys equal to the email of the users with updated scores, values equal to the scores for the latest submission. Example json response: { \"student1@andrew.cmu.edu\": { \"Problem 2\": 10.0, \"Problem 1\": 10.0 }, \"student2@andrew.cmu.edu\": { \"Problem 2\": 10.0, \"Problem 1\": 10.0 } } writeup Get the writeup of an assessment. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments/{assessment_name}/writeup Parameters: [none] Responses: If no writeup exists: key type value writeup string \"none\" If writeup is a url: key type description url string The url of the writeup. If writeup is a file: The file is returned. handout Get the handout of an assessment. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments/{assessment_name}/handout Parameters: [none] Responses: [same as writeup ] submit Make a submission to an assessment. Scope: 'user_submit' Endpoint: POST /courses/{course_name}/assessments/{assessment_name}/submit Parameters: submission[file] The file to submit Note: the name should be the string 'submission[file]' Success Response: key type description version integer The version number of the newly submitted submission. filename string The final filename the submitted file is referred to as. Failure Response: A valid submission request may still fail for many reasons, such as file too large, handins disabled by staff, deadline has passed, etc. When a submission fails, the HTTP response code will not be 200. The response body will include a json with the key 'error'. Its contents will be a user-friendly string that the client may display to the user to explain why the submission has failed. The client must not repeat the request without any modifications. The client is not expected to be able to handle the error automatically. submissions Get all submissions the user has made. Scope: 'user_scores' Endpoint: /courses/{course_name}/assessments/{assessment_name}/submissions Parameters: [none] Response: A list of submissions. Each submission includes: key type description version integer The version number of this submission. filename string The final filename the submitted file is referred to as. created_at datetime The time this submission was made. scores object A dictionary containing the scores of each problem. The keys are the names of the problems, and the value is either the score (a float), or the string 'unreleased' if the score for this problem is not yet released. feedback Get the text feedback given to a problem of a submission. For autograded assessments, the feedback will by default be the autograder feedback, and will be identical for all problems. Scope: 'user_scores' Endpoint: /courses/{course_name}/assessments/{assessment_name}/submissions/{submission_version}/feedback Parameters: problem The name of the problem that the feedback is given to. Response: key type description feedback string The full feedback text for this problem. course_user_data (enrollments) Autolab uses the term course_user_data to represent the users affiliated with a course. It includes all students, course assistants, and instructors of the course. A course_user_data object in the response will be formatted in this form: key type description first_name string The user's first name. last_name string The user's last name. email string The user's registered email. school string The school the user belongs to. major string The user's major of study. year string The user's year. lecture string The user's assigned lecture. section string The user's assigned section. grade_policy string The user's grade policy for this course. nickname string The user's nickname for this course. dropped boolean Is the user marked as dropped from this course? auth_level string The user's level of access for this course. One of 'student', 'course_assistant', or 'instructor'. There are five endpoints related to course_user_data: index List all course_user_data of a course. Scope: 'instructor_all' Endpoint: GET /courses/{course_name}/course_user_data Parameters: [none] Response: A list of course_user_data objects. show Show the course_user_data of a particular student in a course. Scope: 'instructor_all' Endpoint: GET /courses/{course_name}/course_user_data/{user_email} Parameters: [none] Response: The requested user's course_user_data object. create Create a new course_user_data for a course. The user's email is used to uniquely identify the user on Autolab. If the user is not yet a user of Autolab, they need to be registered on Autolab before they can be enrolled in any courses. Scope: 'instructor_all' Endpoint: POST /courses/{course_name}/course_user_data Parameters: key type description email required string The email of the user (to uniquely identify the user). lecture required string The lecture to assign the user to. section required string The section to assign the user to. grade_policy string The user's grade policy (opaque to Autolab). dropped boolean Should the user be marked as dropped? nickname string The nickname to give the user. auth_level required string The level of access this user has for this course. One of 'student', 'course_assistant', or 'instructor'. Response: The newly created course_user_data object. update Update an existing course_user_data. Scope: 'instructor_all' Endpoint: PUT /courses/{course_name}/course_user_data/{user_email} Parameters: key type description lecture string The lecture to assign the user to. section string The section to assign the user to. grade_policy string The user's grade policy (opaque to Autolab). dropped boolean Should the user be marked as dropped? nickname string The nickname to give the user. auth_level string The level of access this user has for this course. One of 'student', 'course_assistant', or 'instructor'. Response: The newly updated course_user_data object. destroy Drop a user from a course. Since CUDs are never deleted from the course, this is just a shortcut for updating a user with the dropped attribute set to true. Scope: 'instructor_all' Endpoint: DELETE /courses/{course_name}/course_user_data/{user_email} Parameters: [none] Response: The newly updated course_user_data object.","title":"Endpoints"},{"location":"api-interface/#routing","text":"For version 1 of the API, all endpoints are under the path /api/v1/ . For example, to get user info, send a request to https://<host>/api/v1/user .","title":"Routing"},{"location":"api-interface/#request-response-format","text":"All endpoints expect the HTTP GET method unless otherwise specified. All parameters listed below are required unless denoted [OPTIONAL]. All responses are in JSON format. If the request is completed successfully, the HTTP response code will be 200. The reference below details the keys and their respective value types that the client can expect from each endpoint. If an error occurs, the response code will not be 200. The returned JSON will be an object with the key 'error'. Its value will be a string that explains the error. Notes on return value types All datetime formats are strings in the form of YYYY-MM-DDThh:mm:ss.sTZD , e.g. 2017-10-23T04:17:41.000-04:00 , which means 4:17:41 AM on October 23rd, 2017 US Eastern Time. JSON spec only has a 'number' type, but the spec below distinguishes between integers and floats for ease of use in certain languages. If a field does not exist, the value is generally null. Please be sure to check if a value is null before using it.","title":"Request &amp; Response Format"},{"location":"api-interface/#interface","text":"","title":"Interface"},{"location":"api-interface/#user","text":"Get basic user info. Scope: 'user_info' Endpoint: /user Parameters: [none] Responses: key type description first_name string The user's first name. last_name string The user's last name. email string The user's registered email. school string The school the user belongs to. major string The user's major of study. year string The user's year.","title":"user"},{"location":"api-interface/#courses","text":"Get all courses currently taking or taken before. Scope: 'user_courses' Endpoint: /courses Parameters: state [OPTIONAL] filter the courses by the state of the course. Should be one of 'disabled', 'completed', 'current', or 'upcoming'. If no state is provided, all courses are returned. Responses: A list of courses. Each course contains: key type description name string The unique url-safe name. display_name string The full name of the course. semester string The semester this course is being offered. late_slack integer The number of seconds after a deadline that the server will still accept a submission and not count it as late. grace_days integer AKA late days. The total number of days (over the entire semester) a student is allowed to submit an assessment late. auth_level string The user's level of access for this course. One of 'student', 'course_assistant', or 'instructor'.","title":"courses"},{"location":"api-interface/#assessments","text":"Get all the assessments of a course. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments Parameters: [none] Responses: A list of assessments. If the user is only a student of the course, only released assessments are available. Otherwise, all assessments are available. Each assessment contains: key type description name string The unique url-safe name. display_name string The full name of the assessments. start_at datetime The time this assessment is released to students. due_at datetime Students can submit before this time without being penalized or using grace days. end_at datetime Last possible time that students can submit (except those granted extensions.) category_name string Name of the category this assessment belongs to.","title":"assessments"},{"location":"api-interface/#assessment-details","text":"","title":"assessment details"},{"location":"api-interface/#show","text":"Show detailed information of an assessment. Scope: 'user_courses' Endpoint: GET /courses/{course_name}/assessments/{assessment_name} Parameters: [none] Response: key type description name string The unique url-safe name. display_name string The full name of the assessments. description string A short description of the assessment. start_at datetime The time this assessment is released to students. due_at datetime Students can submit before this time without being penalized or using grace days. end_at datetime Last possible time that students can submit (except those granted extensions.) updated_at datetime The last time an update was made to the assessment. max_grace_days integer Maximum number of grace days that a student can spend on this assessment. max_submissions integer The maximum number of times a student can submit the assessment. -1 means unlimited submissions. max_unpenalized_submissions integer The maximum number of times the assessment can be submitted without incurring a penalty. -1 means unlimited submissions. disable_handins boolean Are handins disallowed by students? category_name string Name of the category this assessment belongs to. group_size integer The maximum size of groups for this assessment. writeup_format string The format of this assessment's writeup. One of 'none', 'url', or 'file'. handout_format string The format of this assessment's handout. One of 'none', 'url', or 'file'. has_scoreboard boolean Does this assessment have a scoreboard? has_autograder boolean Does this assessment use an autograder? max_total_score float The maximum total score for this assessment max_scores object An object with the problem name as the key, and the maximum score for the problem as the value","title":"show"},{"location":"api-interface/#set-group-settings","text":"set the group size of the assessment. Scope: 'user_courses' Endpoint: POST /courses/{course_name}/assessments/{assessment_name}/set_group_settings Parameters: key type description group_size integer the number of people in a group allow_student_assign_group boolean whether students are allowed to edit and self-assign groups Response: key type description group_size integer the number of people in a group allow_student_assign_group boolean whether students are allowed to edit and self-assign groups","title":"set group settings"},{"location":"api-interface/#groups","text":"","title":"groups"},{"location":"api-interface/#index","text":"List all groups in an assessment Scope: 'instructor_all' Endpoint: GET /courses/{course name}/assessments/{assessment name}/groups Parameters: show_members key type description show_members optional boolean whether to retrieve the members of each group or not Response: A JSON object containing the group_size, a list of groups, and the assessment containing the groups. If show_members is set to true, each group will have a list members of user objects that are members of that group.","title":"index"},{"location":"api-interface/#show_1","text":"Show the details of a group and its members Scope: 'instructor_all' Endpoint: GET /courses/{course name}/assessments/{assessment name}/groups/{id} Parameters: [none] Response: The requested group object. It contains a list members of user objects that are members of that group.","title":"show"},{"location":"api-interface/#create","text":"Create groups in the assessment, given the emails of the people in the group, and an optional group name. Scope: 'instructor_all' Endpoint: POST /courses/{course name}/assessments/{assessment name}/groups Parameters: Groups key type description groups required string List of group s to be created. Refer to group object. Group key type description name string Name of the group group_members required list of string List of emails of students in that group Example json object { \"groups\" : [{ \"name\": \"hello\", \"group_members\": [\"user@foo.bar\",\"user1@foo.bar\"] }, { \"name\": \"hello2\", \"group_members\": [\"user2@foo.bar\",\"\"user3@foo.bar\"] } ] } Response: A list of the groups created if successful. Otherwise an error message will be returned.","title":"create"},{"location":"api-interface/#destroy","text":"Delete a certain group of an assessment given the id Scope: 'instructor_all' Endpoint: DELETE /courses/{course name}/assessments/{assessment name}/groups/{id} Parameters: [none] Response: Success message if deleted.","title":"destroy"},{"location":"api-interface/#problems","text":"","title":"problems"},{"location":"api-interface/#index_1","text":"Get all problems of an assessment. Scope: 'instructor_all' Endpoint GET /courses/{course_name}/assessments/{assessment_name}/problems Parameters: [none] Responses: A list of problems. Each problem contains: key type description name string Full name of the problem. description string Brief description of the problem. max_score float Maximum possible score for this problem. optional boolean Is this problem optional? starred boolean Is this problem starred?","title":"index"},{"location":"api-interface/#create_1","text":"Create a problem for an assessment. Scope: 'instructor_all' Endpoint POST /courses/{course_name}/assessments/{assessment_name}/problems Parameters: key type description name string Full name of the problem. description string Brief description of the problem. max_score float Maximum possible score for this problem. optional boolean Is this problem optional? Responses: The newly created problem. key type description name string Full name of the problem. description string Brief description of the problem. max_score float Maximum possible score for this problem. optional boolean Is this problem optional?","title":"create"},{"location":"api-interface/#scores","text":"","title":"scores"},{"location":"api-interface/#index_2","text":"Get the submission scores for all users for an assessment. Scope: 'instructor_all' Endpoint GET /courses/{course_name}/assessments/{assessment_name}/scores Parameters: [none] Responses: A dictionary containing the submission data for each student that's made a submission. The keys are students' emails, and the values are a dictionary with keys equal to the submission number, and values equal to the scores for the graded problems. Example json object { \"student1@andrew.cmu.edu\" : { \"1\": { \"problem1\": 100.0, \"problem2\": 10.0 }, \"2\": { \"problem1\": 100.0, \"problem2\": 15.0 } }, \"student2@andrew.cmu.edu\" : { \"1\": {} } }","title":"index"},{"location":"api-interface/#show_2","text":"Get the submission scores for a user for an assessment. Scope: 'instructor_all' Endpoint GET /courses/{course_name}/assessments/{assessment_name}/scores/{email} Parameters: [none] Response: A dictionary containing the submission data for the student. The keys are the submission number, and values equal to the scores for the graded problems. Example json response: { \"1\": { \"Problem 1\": 100.0, \"Problem 2\": 10.0 } }","title":"show"},{"location":"api-interface/#update_latest","text":"Update the scores for a student's latest submission. Scope: 'instructor_all' Endpoint PUT /courses/{course_name}/assessments/{assessment_name}/scores/{email}/update_latest/ Parameters: key type description update_group_scores boolean Should the score update be propagated to the students in the student's group? problems json object Keys equal to the name of the problems to update, values equal to the updated score for a problem Responses: If any of the problems in problems does not exist for the assessment key type value error string \"Problem '...' not found in this assessment\" In this case, no score updates will be saved. If all of the problems in problems exist for the assessment The a dictionary with keys equal to the email of the users with updated scores, values equal to the scores for the latest submission. Example json response: { \"student1@andrew.cmu.edu\": { \"Problem 2\": 10.0, \"Problem 1\": 10.0 }, \"student2@andrew.cmu.edu\": { \"Problem 2\": 10.0, \"Problem 1\": 10.0 } }","title":"update_latest"},{"location":"api-interface/#writeup","text":"Get the writeup of an assessment. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments/{assessment_name}/writeup Parameters: [none] Responses: If no writeup exists: key type value writeup string \"none\" If writeup is a url: key type description url string The url of the writeup. If writeup is a file: The file is returned.","title":"writeup"},{"location":"api-interface/#handout","text":"Get the handout of an assessment. Scope: 'user_courses' Endpoint: /courses/{course_name}/assessments/{assessment_name}/handout Parameters: [none] Responses: [same as writeup ]","title":"handout"},{"location":"api-interface/#submit","text":"Make a submission to an assessment. Scope: 'user_submit' Endpoint: POST /courses/{course_name}/assessments/{assessment_name}/submit Parameters: submission[file] The file to submit Note: the name should be the string 'submission[file]' Success Response: key type description version integer The version number of the newly submitted submission. filename string The final filename the submitted file is referred to as. Failure Response: A valid submission request may still fail for many reasons, such as file too large, handins disabled by staff, deadline has passed, etc. When a submission fails, the HTTP response code will not be 200. The response body will include a json with the key 'error'. Its contents will be a user-friendly string that the client may display to the user to explain why the submission has failed. The client must not repeat the request without any modifications. The client is not expected to be able to handle the error automatically.","title":"submit"},{"location":"api-interface/#submissions","text":"Get all submissions the user has made. Scope: 'user_scores' Endpoint: /courses/{course_name}/assessments/{assessment_name}/submissions Parameters: [none] Response: A list of submissions. Each submission includes: key type description version integer The version number of this submission. filename string The final filename the submitted file is referred to as. created_at datetime The time this submission was made. scores object A dictionary containing the scores of each problem. The keys are the names of the problems, and the value is either the score (a float), or the string 'unreleased' if the score for this problem is not yet released.","title":"submissions"},{"location":"api-interface/#feedback","text":"Get the text feedback given to a problem of a submission. For autograded assessments, the feedback will by default be the autograder feedback, and will be identical for all problems. Scope: 'user_scores' Endpoint: /courses/{course_name}/assessments/{assessment_name}/submissions/{submission_version}/feedback Parameters: problem The name of the problem that the feedback is given to. Response: key type description feedback string The full feedback text for this problem.","title":"feedback"},{"location":"api-interface/#course_user_data-enrollments","text":"Autolab uses the term course_user_data to represent the users affiliated with a course. It includes all students, course assistants, and instructors of the course. A course_user_data object in the response will be formatted in this form: key type description first_name string The user's first name. last_name string The user's last name. email string The user's registered email. school string The school the user belongs to. major string The user's major of study. year string The user's year. lecture string The user's assigned lecture. section string The user's assigned section. grade_policy string The user's grade policy for this course. nickname string The user's nickname for this course. dropped boolean Is the user marked as dropped from this course? auth_level string The user's level of access for this course. One of 'student', 'course_assistant', or 'instructor'. There are five endpoints related to course_user_data:","title":"course_user_data (enrollments)"},{"location":"api-interface/#index_3","text":"List all course_user_data of a course. Scope: 'instructor_all' Endpoint: GET /courses/{course_name}/course_user_data Parameters: [none] Response: A list of course_user_data objects.","title":"index"},{"location":"api-interface/#show_3","text":"Show the course_user_data of a particular student in a course. Scope: 'instructor_all' Endpoint: GET /courses/{course_name}/course_user_data/{user_email} Parameters: [none] Response: The requested user's course_user_data object.","title":"show"},{"location":"api-interface/#create_2","text":"Create a new course_user_data for a course. The user's email is used to uniquely identify the user on Autolab. If the user is not yet a user of Autolab, they need to be registered on Autolab before they can be enrolled in any courses. Scope: 'instructor_all' Endpoint: POST /courses/{course_name}/course_user_data Parameters: key type description email required string The email of the user (to uniquely identify the user). lecture required string The lecture to assign the user to. section required string The section to assign the user to. grade_policy string The user's grade policy (opaque to Autolab). dropped boolean Should the user be marked as dropped? nickname string The nickname to give the user. auth_level required string The level of access this user has for this course. One of 'student', 'course_assistant', or 'instructor'. Response: The newly created course_user_data object.","title":"create"},{"location":"api-interface/#update","text":"Update an existing course_user_data. Scope: 'instructor_all' Endpoint: PUT /courses/{course_name}/course_user_data/{user_email} Parameters: key type description lecture string The lecture to assign the user to. section string The section to assign the user to. grade_policy string The user's grade policy (opaque to Autolab). dropped boolean Should the user be marked as dropped? nickname string The nickname to give the user. auth_level string The level of access this user has for this course. One of 'student', 'course_assistant', or 'instructor'. Response: The newly updated course_user_data object.","title":"update"},{"location":"api-interface/#destroy_1","text":"Drop a user from a course. Since CUDs are never deleted from the course, this is just a shortcut for updating a user with the dropped attribute set to true. Scope: 'instructor_all' Endpoint: DELETE /courses/{course_name}/course_user_data/{user_email} Parameters: [none] Response: The newly updated course_user_data object.","title":"destroy"},{"location":"api-managing-authorized-apps/","text":"Managing Authorized Apps With the advent of the API, developers can now create new, more versatile and convenient ways of accessing Autolab. What this means for users is that you can now use third-party programs to access Autolab to view assignments, download handouts, and even submit your solutions. Rest assured that all developers and their clients will be manually vetted by our team to ensure quality and safety. However, it is still important that you understand how clients interact with your account. Terminology user: a user of Autolab (student/instructor) client: a program that uses the Autolab api developer: a person that develops clients Granting access As a user of Autolab, when you want to use a client for the first time, you need to grant access to the client so that it can interact with Autolab for you. Easy Activation : Clients that have access to a web browser (e.g. mobile apps, web apps) will redirect the user directly to the Grant Permissions page on Autolab. Manual Activation : Clients that don't have access to a web browser (e.g. command line programs) will present to the user a 6-digit code (case sensitive) that should be entered on the Autolab website. Note : Third-party clients never ask for your Autolab username or password. Never enter them anywhere else except on the Autolab website (always check the page url before entering your credentials). Manual activation page When you enter the code on the website and click \"Activate\", you will be taken to the Grant Permissions page. API Grant Permissions Page This page shows you all the permissions the client requests. Click 'approve' to grant these permissions to this client. Reviewing your authorized clients As a user, you can review all the clients that you've granted access to on the Manage Authorized Clients page. Click on the menu at the upper right corner, then click on 'Account'. At the bottom of the page you'll find the 'Manage Authorized Clients' link. Manage all the clients that currently have access to your account You can view the permissions that each client has (hover over the icon to see a description of each permission). You can also click 'Revoke' at any time to revoke the access of a client immediately.","title":"Managing Authorized Apps"},{"location":"api-managing-authorized-apps/#managing-authorized-apps","text":"With the advent of the API, developers can now create new, more versatile and convenient ways of accessing Autolab. What this means for users is that you can now use third-party programs to access Autolab to view assignments, download handouts, and even submit your solutions. Rest assured that all developers and their clients will be manually vetted by our team to ensure quality and safety. However, it is still important that you understand how clients interact with your account.","title":"Managing Authorized Apps"},{"location":"api-managing-authorized-apps/#terminology","text":"user: a user of Autolab (student/instructor) client: a program that uses the Autolab api developer: a person that develops clients","title":"Terminology"},{"location":"api-managing-authorized-apps/#granting-access","text":"As a user of Autolab, when you want to use a client for the first time, you need to grant access to the client so that it can interact with Autolab for you. Easy Activation : Clients that have access to a web browser (e.g. mobile apps, web apps) will redirect the user directly to the Grant Permissions page on Autolab. Manual Activation : Clients that don't have access to a web browser (e.g. command line programs) will present to the user a 6-digit code (case sensitive) that should be entered on the Autolab website. Note : Third-party clients never ask for your Autolab username or password. Never enter them anywhere else except on the Autolab website (always check the page url before entering your credentials). Manual activation page When you enter the code on the website and click \"Activate\", you will be taken to the Grant Permissions page. API Grant Permissions Page This page shows you all the permissions the client requests. Click 'approve' to grant these permissions to this client.","title":"Granting access"},{"location":"api-managing-authorized-apps/#reviewing-your-authorized-clients","text":"As a user, you can review all the clients that you've granted access to on the Manage Authorized Clients page. Click on the menu at the upper right corner, then click on 'Account'. At the bottom of the page you'll find the 'Manage Authorized Clients' link. Manage all the clients that currently have access to your account You can view the permissions that each client has (hover over the icon to see a description of each permission). You can also click 'Revoke' at any time to revoke the access of a client immediately.","title":"Reviewing your authorized clients"},{"location":"api-overview/","text":"Overview The web interface that has served us well for many years is no longer the only way to use Autolab. With the API, developers will be able to help make Autolab more versatile and convenient: Whether it be with a mobile app, a command line tool, a browser extension, or something we've never even thought of. For students and instructors who only plan to use Autolab, try out the Autolab CLI . The Autolab REST API allows developers to create clients that can access features of Autolab on behalf of Autolab users. V1 of the API allows clients to: Access basic user info View courses and assessments Submit to assessments View scores and feedback Manage course enrollments Authorization All endpoints of the Autolab API requires client authentication in the form of an access token. To obtain this access token, clients must obtain authorization from the user. Autolab API uses the standard OAuth2 Authorization Code Grant for user authorization. For clients with no easy access to web browsers (e.g. console apps), an alternative device flow -based authorization method is provided as well. To register an API application, one needs to have an admin privileges, and then visit Manage Autolab > Manage API application . Refer to Scopes for the available scopes. To understand how to authorize and unauthorize clients as a user, go to Managing Authorized Apps Authorization Code Grant Flow (OAuth2) OAuth Authorization Request Endpoint : /oauth/authorize OAuth Access Token Endpoint : oauth/token The authorization code grant consists of 5 basic steps: Client directs the user to the authorization request endpoint via a web browser. Authorization server (Autolab) authenticates the user. If user grants access to the client, the authorization server provides an \"authorization code\" to the client. Client exchanges the authorization code for an access token from the access token endpoint. Client uses the access token for subsequent requests to the API. Section 4.1 of RFC 6749 details the parameters required and the response clients can expect from these endpoints. Autolab API provides a refresh token with every new access token. Once the access token has expired, the client can use the refresh token to obtain a new access token, refresh token pair. Details are also provided in RFC 6749 here . Device Flow (Alternative) For devices that cannot use a web browser to obtain user authorization, the alternative device flow approach circumvents the first 3 steps in the authorization code grant flow. Instead of directing a user to the authorization page directly, the client obtains a user code that the user can enter on the Autolab website from any device. The website then takes the user through the authorization procedure, and returns the authorization code to the client. The client can then use this code to request an access token from the access token endpoint as usual. Note that this is different from the \"device flow\" described in the Internet Draft linked above. Obtaining User Code Request Endpoint: GET /oauth/device_flow_init Parameters: client_id: the client_id obtained when registering the client Success Response: device_code: the verification code used by the client (should be kept secret from the user). user_code: the verification code that should be displayed to the user. verification_uri: the verification uri that the user should use to authorize the client. By default is /activate The latter two should be displayed to the user. Obtaining Authorization Code After asking the user to enter the user code on the verification site, the client should poll the device_flow_authorize endpoint to find out if the user has completed the authorization step. Request Endpoint: GET /oauth/device_flow_authorize Parameters: client_id: the client_id obtained when registering the client device_code: the device_code obtained from the device_flow_init endpoint Failure Responses: 400 Bad Request: {error: authorization_pending} The user has not yet granted or denied the authorization request. Please try again in a while. 429 Too Many Requests: {error: Retry later} The client is polling too frequently. Please wait for a while before polling again. The default rate limit is once every 5 seconds. Success Response: code: the authorization code that should be used to obtain an access token. The client could then perform steps 4 and 5 of the Authorization Code Grant Flow. Getting Started Autolab requires all client applications to be registered clients. Upon registration, a client_id and client_secret pair will be provided to the developers for use in the app as identification to the server. Please contact the administrators of your specific Autolab deployment for registration. Security Concerns Please make sure to keep the client_secret secret. Leaking this code may allow third-parties to impersonate your app. Scopes The scopes of an API client specifies the permissions it has, and must be specified during client registration (can be modified later). Currently, Autolab offers the following scopes for third-party clients: user_info: Access your basic info (e.g. name, email, school, year). user_courses: Access your courses and assessments. user_scores: Access your submissions, scores, and feedback. user_submit: Submit to assessments on your behalf. instructor_all: Access admin options of courses where you are an instructor. Example usages If your app only wants to use the API for quick user authentication, you only need the 'user_info' scope. If you want to develop a mobile client for Autolab that allows students to view their upcoming assessments, you may ask for 'user_info' and 'user_courses'. If you want to write a full desktop client that users can use to submit to assessments and view their grades, you may ask for all 5 scopes. Of course, these are only examples. We can't wait to see what new usages of the API you may come up with! We just recommend that you only ask for the scopes you need as the users will be shown the required scopes during authorization, and it gives them peace of mind to know that an app doesn't ask for excessive permissions.","title":"Overview"},{"location":"api-overview/#overview","text":"The web interface that has served us well for many years is no longer the only way to use Autolab. With the API, developers will be able to help make Autolab more versatile and convenient: Whether it be with a mobile app, a command line tool, a browser extension, or something we've never even thought of. For students and instructors who only plan to use Autolab, try out the Autolab CLI . The Autolab REST API allows developers to create clients that can access features of Autolab on behalf of Autolab users. V1 of the API allows clients to: Access basic user info View courses and assessments Submit to assessments View scores and feedback Manage course enrollments","title":"Overview"},{"location":"api-overview/#authorization","text":"All endpoints of the Autolab API requires client authentication in the form of an access token. To obtain this access token, clients must obtain authorization from the user. Autolab API uses the standard OAuth2 Authorization Code Grant for user authorization. For clients with no easy access to web browsers (e.g. console apps), an alternative device flow -based authorization method is provided as well. To register an API application, one needs to have an admin privileges, and then visit Manage Autolab > Manage API application . Refer to Scopes for the available scopes. To understand how to authorize and unauthorize clients as a user, go to Managing Authorized Apps","title":"Authorization"},{"location":"api-overview/#authorization-code-grant-flow-oauth2","text":"OAuth Authorization Request Endpoint : /oauth/authorize OAuth Access Token Endpoint : oauth/token The authorization code grant consists of 5 basic steps: Client directs the user to the authorization request endpoint via a web browser. Authorization server (Autolab) authenticates the user. If user grants access to the client, the authorization server provides an \"authorization code\" to the client. Client exchanges the authorization code for an access token from the access token endpoint. Client uses the access token for subsequent requests to the API. Section 4.1 of RFC 6749 details the parameters required and the response clients can expect from these endpoints. Autolab API provides a refresh token with every new access token. Once the access token has expired, the client can use the refresh token to obtain a new access token, refresh token pair. Details are also provided in RFC 6749 here .","title":"Authorization Code Grant Flow (OAuth2)"},{"location":"api-overview/#device-flow-alternative","text":"For devices that cannot use a web browser to obtain user authorization, the alternative device flow approach circumvents the first 3 steps in the authorization code grant flow. Instead of directing a user to the authorization page directly, the client obtains a user code that the user can enter on the Autolab website from any device. The website then takes the user through the authorization procedure, and returns the authorization code to the client. The client can then use this code to request an access token from the access token endpoint as usual. Note that this is different from the \"device flow\" described in the Internet Draft linked above.","title":"Device Flow (Alternative)"},{"location":"api-overview/#obtaining-user-code","text":"Request Endpoint: GET /oauth/device_flow_init Parameters: client_id: the client_id obtained when registering the client Success Response: device_code: the verification code used by the client (should be kept secret from the user). user_code: the verification code that should be displayed to the user. verification_uri: the verification uri that the user should use to authorize the client. By default is /activate The latter two should be displayed to the user.","title":"Obtaining User Code"},{"location":"api-overview/#obtaining-authorization-code","text":"After asking the user to enter the user code on the verification site, the client should poll the device_flow_authorize endpoint to find out if the user has completed the authorization step. Request Endpoint: GET /oauth/device_flow_authorize Parameters: client_id: the client_id obtained when registering the client device_code: the device_code obtained from the device_flow_init endpoint Failure Responses: 400 Bad Request: {error: authorization_pending} The user has not yet granted or denied the authorization request. Please try again in a while. 429 Too Many Requests: {error: Retry later} The client is polling too frequently. Please wait for a while before polling again. The default rate limit is once every 5 seconds. Success Response: code: the authorization code that should be used to obtain an access token. The client could then perform steps 4 and 5 of the Authorization Code Grant Flow.","title":"Obtaining Authorization Code"},{"location":"api-overview/#getting-started","text":"Autolab requires all client applications to be registered clients. Upon registration, a client_id and client_secret pair will be provided to the developers for use in the app as identification to the server. Please contact the administrators of your specific Autolab deployment for registration. Security Concerns Please make sure to keep the client_secret secret. Leaking this code may allow third-parties to impersonate your app.","title":"Getting Started"},{"location":"api-overview/#scopes","text":"The scopes of an API client specifies the permissions it has, and must be specified during client registration (can be modified later). Currently, Autolab offers the following scopes for third-party clients: user_info: Access your basic info (e.g. name, email, school, year). user_courses: Access your courses and assessments. user_scores: Access your submissions, scores, and feedback. user_submit: Submit to assessments on your behalf. instructor_all: Access admin options of courses where you are an instructor. Example usages If your app only wants to use the API for quick user authentication, you only need the 'user_info' scope. If you want to develop a mobile client for Autolab that allows students to view their upcoming assessments, you may ask for 'user_info' and 'user_courses'. If you want to write a full desktop client that users can use to submit to assessments and view their grades, you may ask for all 5 scopes. Of course, these are only examples. We can't wait to see what new usages of the API you may come up with! We just recommend that you only ask for the scopes you need as the users will be shown the required scopes during authorization, and it gives them peace of mind to know that an app doesn't ask for excessive permissions.","title":"Scopes"},{"location":"command-line-interface/","text":"Autolab Command Line Interface To help showcase the capabilities of the API , we developed autolab-cli: A first-party command line client that serves as both a practical tool for users of Autolab, as well as a reference design for developers intending to use the API in their own programs. The cli includes features like downloading and submitting assignments from the terminal, viewing problems, and getting submission feedback. Note to CMU Students: This cli binary has already been installed on the andrew machines as autolab . Obtaining authorization Make sure you have the cli installed by running autolab in your terminal. If you see the usage instructions you're good to go. Otherwise, ask your school admin to install the cli from the Autolab CLI Repository . To setup autolab-cli with your Autolab account, run autolab setup . This will initiate a manual activation. What you'll see when you run autolab setup Once you approve the client on the Autolab website, the client will respond telling you that authorization was successful. You should be able to use the client from now on. If at any point you want to reset the client, run autolab setup -f and you'll be asked to re-authorize the client from a clean state. To deauthorize any client that you've given permission to, look at how to Manage Authorized Apps . Viewing your courses and assessments To view your current courses, run $ autolab courses This will show you a list of ongoing courses in the form unique_name (Display name) . You should use the 'unique_name' of each course when interacting with autolab-cli. To view the assessments of a course, run $ autolab asmts <course_unique_name> This will show you a list of assessments in the same unique_name (Display name) format. Downloading an assessment To start working on an assessment, go to a directory where you usually put your work, and run $ autolab download <course_unique_name>:<asmt_unique_name> This will create a directory with the assessment name in your current directory, and download the handout and writeup in it. This new directory is called an 'assessment directory'. Whenever you're inside an assessment directory, autolab-cli will respond according to the context. For example, when you're inside an assessment directory, you can run $ autolab problems This will show you the problems of this assessment. Submitting solutions To submit to an assessment inside an assessment directory, run $ autolab submit <filename> Yep, it's that easy. Viewing scores To view the scores you got, run $ autolab scores The scores command will only return scores for those submissions that are made via this client. This is a privacy constraint of the Autolab API. To view the feedback you got, run $ autolab feedback Advanced features You can learn more about each sub-command by running $ autolab <sub-command> -h This will reveal other flags you may be able to use with each command. For example, you can call all of the context-dependent commands outside of an assessment directory by providing the <course_unique_name>:<asmt_unique_name> pair. We hope this speeds up your workflow! If you find any problems, please file an issue on the Autolab CLI Repository .","title":"Autolab Frontend CLI"},{"location":"command-line-interface/#autolab-command-line-interface","text":"To help showcase the capabilities of the API , we developed autolab-cli: A first-party command line client that serves as both a practical tool for users of Autolab, as well as a reference design for developers intending to use the API in their own programs. The cli includes features like downloading and submitting assignments from the terminal, viewing problems, and getting submission feedback. Note to CMU Students: This cli binary has already been installed on the andrew machines as autolab .","title":"Autolab Command Line Interface"},{"location":"command-line-interface/#obtaining-authorization","text":"Make sure you have the cli installed by running autolab in your terminal. If you see the usage instructions you're good to go. Otherwise, ask your school admin to install the cli from the Autolab CLI Repository . To setup autolab-cli with your Autolab account, run autolab setup . This will initiate a manual activation. What you'll see when you run autolab setup Once you approve the client on the Autolab website, the client will respond telling you that authorization was successful. You should be able to use the client from now on. If at any point you want to reset the client, run autolab setup -f and you'll be asked to re-authorize the client from a clean state. To deauthorize any client that you've given permission to, look at how to Manage Authorized Apps .","title":"Obtaining authorization"},{"location":"command-line-interface/#viewing-your-courses-and-assessments","text":"To view your current courses, run $ autolab courses This will show you a list of ongoing courses in the form unique_name (Display name) . You should use the 'unique_name' of each course when interacting with autolab-cli. To view the assessments of a course, run $ autolab asmts <course_unique_name> This will show you a list of assessments in the same unique_name (Display name) format.","title":"Viewing your courses and assessments"},{"location":"command-line-interface/#downloading-an-assessment","text":"To start working on an assessment, go to a directory where you usually put your work, and run $ autolab download <course_unique_name>:<asmt_unique_name> This will create a directory with the assessment name in your current directory, and download the handout and writeup in it. This new directory is called an 'assessment directory'. Whenever you're inside an assessment directory, autolab-cli will respond according to the context. For example, when you're inside an assessment directory, you can run $ autolab problems This will show you the problems of this assessment.","title":"Downloading an assessment"},{"location":"command-line-interface/#submitting-solutions","text":"To submit to an assessment inside an assessment directory, run $ autolab submit <filename> Yep, it's that easy.","title":"Submitting solutions"},{"location":"command-line-interface/#viewing-scores","text":"To view the scores you got, run $ autolab scores The scores command will only return scores for those submissions that are made via this client. This is a privacy constraint of the Autolab API. To view the feedback you got, run $ autolab feedback","title":"Viewing scores"},{"location":"command-line-interface/#advanced-features","text":"You can learn more about each sub-command by running $ autolab <sub-command> -h This will reveal other flags you may be able to use with each command. For example, you can call all of the context-dependent commands outside of an assessment directory by providing the <course_unique_name>:<asmt_unique_name> pair. We hope this speeds up your workflow! If you find any problems, please file an issue on the Autolab CLI Repository .","title":"Advanced features"},{"location":"instructors/","text":"Guide for Instructors This document provides instructors with a brief overview of the basic ideas and capabilities of the Autolab system. It's meant to be read from beginning to end the first time. Users Users are either instructors , course assistants , or students . Instructors have full permissions. Course assistants are only allowed to enter grades. Students see only their own work. Each user is uniquely identified by their email address. You can change the permissions for a particular user at any time. Note that some instructors opt to give some or all of their TAs instructor status. Roster The roster holds the list of users. You can add and remove users one at a time, or in bulk by uploading a CSV file in the general Autolab format: Semester,email,last_name,first_name,school,major,year,grading_policy,courseNumber,courseLecture,section or in the format that is exported by the CMU S3 service: \"Semester\",\"Course\",\"Section\",\"Lecture\",\"Mini\",\"Last Name\",\"First Name\",\"MI\",\"AndrewID\",\"Email\",\"College\",\"Department\",... Attention CMU Instructors: S3 lists each student twice: once in a lecture roster, which lists the lecture number (e.g., 1, 2,...) in the section field, and once in a section roster, which lists the section letter (e.g., A, B,...) in the section field. Be careful not to import the lecture roster. Instead, export and upload each section individually. Or you can export everything from S3 with a single action, edit out the roster entries for the lecture(s), and then upload a single file to Autolab with all of the sections. For the bulk upload, you can choose to either: add any new students in the roster file to the Autolab roster, or to update the Autolab roster by marking students missing from roster files as dropped . For a linked course, you can sync the Autolab roster by clicking the refresh button above the table of users on the 'Manage Course Users' page. The behavior of the linked course syncing can be customized by clicking the 'Linked Course Settings' button on the 'Manage Course Users' page. The 'Auto drop students' option when enabled will mark students not enrolled in the linked course as dropped on the Autolab roster. Instructors and course assistants are never marked as dropped. User accounts are never deleted. Students marked as dropped can still see their work, but cannot submit new work and do not appear on the instructor gradebook. Instructors can change the dropped status of a student at any time. Once a student is added to the roster for a course, then that course becomes visible to the student when they visit the Autolab site. A student can be enrolled in an arbitrary number of Autolab courses. Access Codes To allow students to self-enroll in a course, you can generate an access code for the course. The access code is a short string of letters and digits that is unique to the course. To do so, select the \"Allow self-enrollment\" checkbox on the course settings page. The access code will be displayed on the course homepage. Students can then enroll in the course by clicking the \"Join Course\" button in the user dropdown menu and entering the access code. Labs (Assessments) A lab (or assessment ) is broadly defined as a submission set; it is anything that your students make submissions (handins) for. This could be a programming assignment, a typed homework, or even an in-class exam. You can create labs from scratch, or reuse them from previous semesters. See the companion Guide For Lab Authors for info on writing and installing labs. Assessment Categories You can tag each assessment with an arbitrary user-defined category , e.g., \"Lab\", \"Exam\", \"Homework\". Autograders and Scoreboards Labs can be autograded or not, at your discretion. When a student submits to an autograded lab, Autolab runs an instructor-supplied autograder program that assigns scores to one or more problems associated with the lab. Autograded labs can have an optional scoreboard that shows (anonymized) results in real-time. See the companion Guide For Lab Authors for details on writing autograded labs with scoreboards. Important Dates A lab has a start date , due date and end date . The link to a lab becomes visible to students after the start date (it's always visible to instructors). Students can submit until the due date without penalty or consuming grace days. Submissions are disabled after the end date. Handins/Submissions Once an assessment is live (past the start date), students can begin submitting handins, where each handin is a single file, which can be either a text file or an archive file (e.g., mm.c , handin.tar ). Alternatively, instructors can enable GitHub submission for an assessment in its settings and students can directly link their GitHub account and submit from their repo's corresponding branch. Check here for how to set up and try our demo site for a feel of its usage. Groups Instructors can enable groups by setting the group size to be greater than 1. By default, students are allowed to form groups on their own. In that case, students can create their own group, ask to join an unsaturated group, or leave their existing group. When a student is in a group, any one member's submission counts towards the group's submission. Alternatively, when instructors disallow students to self-assign, it's best practice for instructors to assign groups through the Autolab Frontend API . Penalties and Extensions You can set penalties for late handins, set hard limits on the number of handins, or set soft limits that penalize excessive handins on a sliding scale. You can also give a student an extension that extends the due dates and end dates for that student. Grace Days Autolab provides support for a late handin policy based on grace days . Each student has a semester-long budget of grace days that are automatically applied if they handin after the due date. Each late day consumes one of the budgeted grace days. The Autolab system keeps track of the number of grace days that have been used by each student to date. If students run out of grace days and handin late, then there is a fixed late penalty (possibly zero) that can be set by the instructor. Problems Each lab contains at least one problem , defined by the instructor, with some point value. Each problem has a name (e.g., \"Prob1\", \"Style\") that is unique for the lab (although different labs can have the same problem names). Grades Grades come in a number of different forms: Problem scores: These are scalar values (possibly negative) assigned per problem per submission, either manually by a human grader after the end date, or automatically by an autograder after each submission. Problem scores can also be uploaded (imported) in bulk from a CSV file. Assessment raw score: By default, the raw score is the sum of the individual problem scores, before any penalties are applied. You can override the default raw score calculation. See below. Assessment total score: The total score is the raw score, plus any late penalties, plus any instructor tweaks . Category averages: This is the average for a particular student over all assessments in a specific instructor-defined category such as \"Labs, or \"Exams\". By default the category average is the arithmetic mean of all assessment total scores, but it can be overridden. See below. Course Average: By default, the course average is average of all category averages, but can be overridden. See below. Submissions can be classified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No Grade\" submission will show up in the gradebook as NG and a zero will be used when calculating averages. An \"Excused\" submission will show up in the gradebook as EXC and will not be used when calculating averages. Overriding Category and Course Averages The average for a category foo is calculated by a default Ruby function called fooAverage , which you can override in the course.rb file. For example, in our course, we prefer to report the \"average\" as the total number of normalized points (out of 100) that the student has accrued so far. This helps them understand where they stand in the class, e.g., \"Going into the final exam (worth 30 normalized points), I have 60 normalized points, so the only way to get an A is to get 100% on the final.\" Here's the Ruby function for category \"Lab\": # In course.rb file def LabAverage ( user ) pts = ( user [ 'datalab' ]. to_f () / 63 . 0 ) * 6 . 0 + ( user [ 'bomblab' ]. to_f () / 70 . 0 ) * 5 . 0 + ( user [ 'attacklab' ]. to_f () / 100 . 0 ) * 4 . 0 + ( user [ 'cachelab' ]. to_f () / 60 . 0 ) * 7 . 0 + ( user [ 'tshlab' ]. to_f () / 110 . 0 ) * 8 . 0 + ( user [ 'malloclab' ]. to_f () / 120 . 0 ) * 12 . 0 + ( user [ 'proxylab' ]. to_f () / 100 . 0 ) * 8 . 0 return pts . to_f () . round ( 2 ) end In this case, labs are worth a total of 50/100 normalized points. The assessment called datalab is graded out of a total of 63 points and is worth 6/50 normalized points. Here is the Ruby function for category \"Exam\": # In course.rb file def ExamAverage ( user ) pts = (( user [ 'midterm' ]. to_f () / 60 . 0 ) * 20 . 0 ) + (( user [ 'final' ]. to_f () / 80 . 0 ) * 30 . 0 ) return pts . to_f () . round ( 2 ) end In this case, exams are worth 50/100 normalized points. The assessment called midterm is graded out of total of 60 points and is worth 20/50 normalized points. The course average is computed by a default Ruby function called courseAverage , which can be overridden by the course.rb file in the course directory. Here is the function for our running example: # In course.rb file def courseAverage ( user ) pts = user [ 'catLab' ]. to_f () + user [ 'catExam' ]. to_f () return pts . to_f () . round ( 2 ) end In this course, the course average is the sum of the category averages for \"Lab\" and \"Exam\". Note: To make these changes live, you must select \"Reload course config file\" on the \"Manage course\" page. Handin History For each lab, students can view all of their submissions, including any source code, and the problem scores, penalties, and total scores associated with those submissions, via the handin history page. Gradesheet The gradesheet (not to be confused with the gradebook ) is the workhorse grading tool. Each assessment has a separate gradesheet with the following features: Provides an interface for manually entering problem scores (and problem feedback) for the most recent submission from each student. Provides an interface for viewing and annotating the submitted code. Displays the problem scores for the most recent submission for each student, summarizes any late penalties, and computes the total score. Provides a link to each student's handin history. Gradebook The gradebook comes in two forms. The student gradebook displays the grades for a particular student, including total scores for each assessment, category averages, and the course average. The instructor gradebook is a table that displays the grades for the most recent submission of each student, including assessment total scores, category averages and course average. For the gradebook calculations, submissions are classified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No Grade\" submission will show up in the gradebook as NG and a zero will be used when calculating averages. An \"Excused\" submission will show up in the gradebook as EXC and will not be used when calculating averages. To auto-expand a column in the gradebook , double-click the edge of that column header. Releasing Grades Manually assigned grades are by default not released, and therefore not visible to students. You can release grades on an individual basis while grading, or release all available grades in bulk by using the \"Release all grades\" option. You can also reverse this process using the \"Withdraw all grades\" option. (The word \"withdraw\" is perhaps unfortunate. No grades are ever deleted. They are simply withdrawn from the student's view.) File Manager Displays files within a course and allows you to rename, delete, and add files automatically via an interface. You can view all courses that you are an instructor of and all associated files. You can access the File Manager under Admin Course after clicking Manage Course. Note: You cannot create folders or upload files in the root Courses directory. To create a course, you should navigate to Create New Course via the Manage Autolab dropdown. You cannot rename files in the root Courses directory. To rename a course, you should navigate to the course, click Manage Course, and click Course Settings. You cannot upload files that are larger than 1 GB. You cannot create a folder or upload a file with a name that already exists. Deleting a folder will also delete all of its contents. Downloading a folder will create a tar with all of its contents. Download Selected downloads each of the selected files/folders separately. When clicking on a file, it will be automatically downloaded if it is larger than 1GB or a binary file. Otherwise, the file contents will be displayed.","title":"Guide for Instructors"},{"location":"instructors/#guide-for-instructors","text":"This document provides instructors with a brief overview of the basic ideas and capabilities of the Autolab system. It's meant to be read from beginning to end the first time.","title":"Guide for Instructors"},{"location":"instructors/#users","text":"Users are either instructors , course assistants , or students . Instructors have full permissions. Course assistants are only allowed to enter grades. Students see only their own work. Each user is uniquely identified by their email address. You can change the permissions for a particular user at any time. Note that some instructors opt to give some or all of their TAs instructor status.","title":"Users"},{"location":"instructors/#roster","text":"The roster holds the list of users. You can add and remove users one at a time, or in bulk by uploading a CSV file in the general Autolab format: Semester,email,last_name,first_name,school,major,year,grading_policy,courseNumber,courseLecture,section or in the format that is exported by the CMU S3 service: \"Semester\",\"Course\",\"Section\",\"Lecture\",\"Mini\",\"Last Name\",\"First Name\",\"MI\",\"AndrewID\",\"Email\",\"College\",\"Department\",... Attention CMU Instructors: S3 lists each student twice: once in a lecture roster, which lists the lecture number (e.g., 1, 2,...) in the section field, and once in a section roster, which lists the section letter (e.g., A, B,...) in the section field. Be careful not to import the lecture roster. Instead, export and upload each section individually. Or you can export everything from S3 with a single action, edit out the roster entries for the lecture(s), and then upload a single file to Autolab with all of the sections. For the bulk upload, you can choose to either: add any new students in the roster file to the Autolab roster, or to update the Autolab roster by marking students missing from roster files as dropped . For a linked course, you can sync the Autolab roster by clicking the refresh button above the table of users on the 'Manage Course Users' page. The behavior of the linked course syncing can be customized by clicking the 'Linked Course Settings' button on the 'Manage Course Users' page. The 'Auto drop students' option when enabled will mark students not enrolled in the linked course as dropped on the Autolab roster. Instructors and course assistants are never marked as dropped. User accounts are never deleted. Students marked as dropped can still see their work, but cannot submit new work and do not appear on the instructor gradebook. Instructors can change the dropped status of a student at any time. Once a student is added to the roster for a course, then that course becomes visible to the student when they visit the Autolab site. A student can be enrolled in an arbitrary number of Autolab courses.","title":"Roster"},{"location":"instructors/#access-codes","text":"To allow students to self-enroll in a course, you can generate an access code for the course. The access code is a short string of letters and digits that is unique to the course. To do so, select the \"Allow self-enrollment\" checkbox on the course settings page. The access code will be displayed on the course homepage. Students can then enroll in the course by clicking the \"Join Course\" button in the user dropdown menu and entering the access code.","title":"Access Codes"},{"location":"instructors/#labs-assessments","text":"A lab (or assessment ) is broadly defined as a submission set; it is anything that your students make submissions (handins) for. This could be a programming assignment, a typed homework, or even an in-class exam. You can create labs from scratch, or reuse them from previous semesters. See the companion Guide For Lab Authors for info on writing and installing labs.","title":"Labs (Assessments)"},{"location":"instructors/#assessment-categories","text":"You can tag each assessment with an arbitrary user-defined category , e.g., \"Lab\", \"Exam\", \"Homework\".","title":"Assessment Categories"},{"location":"instructors/#autograders-and-scoreboards","text":"Labs can be autograded or not, at your discretion. When a student submits to an autograded lab, Autolab runs an instructor-supplied autograder program that assigns scores to one or more problems associated with the lab. Autograded labs can have an optional scoreboard that shows (anonymized) results in real-time. See the companion Guide For Lab Authors for details on writing autograded labs with scoreboards.","title":"Autograders and Scoreboards"},{"location":"instructors/#important-dates","text":"A lab has a start date , due date and end date . The link to a lab becomes visible to students after the start date (it's always visible to instructors). Students can submit until the due date without penalty or consuming grace days. Submissions are disabled after the end date.","title":"Important Dates"},{"location":"instructors/#handinssubmissions","text":"Once an assessment is live (past the start date), students can begin submitting handins, where each handin is a single file, which can be either a text file or an archive file (e.g., mm.c , handin.tar ). Alternatively, instructors can enable GitHub submission for an assessment in its settings and students can directly link their GitHub account and submit from their repo's corresponding branch. Check here for how to set up and try our demo site for a feel of its usage.","title":"Handins/Submissions"},{"location":"instructors/#groups","text":"Instructors can enable groups by setting the group size to be greater than 1. By default, students are allowed to form groups on their own. In that case, students can create their own group, ask to join an unsaturated group, or leave their existing group. When a student is in a group, any one member's submission counts towards the group's submission. Alternatively, when instructors disallow students to self-assign, it's best practice for instructors to assign groups through the Autolab Frontend API .","title":"Groups"},{"location":"instructors/#penalties-and-extensions","text":"You can set penalties for late handins, set hard limits on the number of handins, or set soft limits that penalize excessive handins on a sliding scale. You can also give a student an extension that extends the due dates and end dates for that student.","title":"Penalties and Extensions"},{"location":"instructors/#grace-days","text":"Autolab provides support for a late handin policy based on grace days . Each student has a semester-long budget of grace days that are automatically applied if they handin after the due date. Each late day consumes one of the budgeted grace days. The Autolab system keeps track of the number of grace days that have been used by each student to date. If students run out of grace days and handin late, then there is a fixed late penalty (possibly zero) that can be set by the instructor.","title":"Grace Days"},{"location":"instructors/#problems","text":"Each lab contains at least one problem , defined by the instructor, with some point value. Each problem has a name (e.g., \"Prob1\", \"Style\") that is unique for the lab (although different labs can have the same problem names).","title":"Problems"},{"location":"instructors/#grades","text":"Grades come in a number of different forms: Problem scores: These are scalar values (possibly negative) assigned per problem per submission, either manually by a human grader after the end date, or automatically by an autograder after each submission. Problem scores can also be uploaded (imported) in bulk from a CSV file. Assessment raw score: By default, the raw score is the sum of the individual problem scores, before any penalties are applied. You can override the default raw score calculation. See below. Assessment total score: The total score is the raw score, plus any late penalties, plus any instructor tweaks . Category averages: This is the average for a particular student over all assessments in a specific instructor-defined category such as \"Labs, or \"Exams\". By default the category average is the arithmetic mean of all assessment total scores, but it can be overridden. See below. Course Average: By default, the course average is average of all category averages, but can be overridden. See below. Submissions can be classified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No Grade\" submission will show up in the gradebook as NG and a zero will be used when calculating averages. An \"Excused\" submission will show up in the gradebook as EXC and will not be used when calculating averages.","title":"Grades"},{"location":"instructors/#overriding-category-and-course-averages","text":"The average for a category foo is calculated by a default Ruby function called fooAverage , which you can override in the course.rb file. For example, in our course, we prefer to report the \"average\" as the total number of normalized points (out of 100) that the student has accrued so far. This helps them understand where they stand in the class, e.g., \"Going into the final exam (worth 30 normalized points), I have 60 normalized points, so the only way to get an A is to get 100% on the final.\" Here's the Ruby function for category \"Lab\": # In course.rb file def LabAverage ( user ) pts = ( user [ 'datalab' ]. to_f () / 63 . 0 ) * 6 . 0 + ( user [ 'bomblab' ]. to_f () / 70 . 0 ) * 5 . 0 + ( user [ 'attacklab' ]. to_f () / 100 . 0 ) * 4 . 0 + ( user [ 'cachelab' ]. to_f () / 60 . 0 ) * 7 . 0 + ( user [ 'tshlab' ]. to_f () / 110 . 0 ) * 8 . 0 + ( user [ 'malloclab' ]. to_f () / 120 . 0 ) * 12 . 0 + ( user [ 'proxylab' ]. to_f () / 100 . 0 ) * 8 . 0 return pts . to_f () . round ( 2 ) end In this case, labs are worth a total of 50/100 normalized points. The assessment called datalab is graded out of a total of 63 points and is worth 6/50 normalized points. Here is the Ruby function for category \"Exam\": # In course.rb file def ExamAverage ( user ) pts = (( user [ 'midterm' ]. to_f () / 60 . 0 ) * 20 . 0 ) + (( user [ 'final' ]. to_f () / 80 . 0 ) * 30 . 0 ) return pts . to_f () . round ( 2 ) end In this case, exams are worth 50/100 normalized points. The assessment called midterm is graded out of total of 60 points and is worth 20/50 normalized points. The course average is computed by a default Ruby function called courseAverage , which can be overridden by the course.rb file in the course directory. Here is the function for our running example: # In course.rb file def courseAverage ( user ) pts = user [ 'catLab' ]. to_f () + user [ 'catExam' ]. to_f () return pts . to_f () . round ( 2 ) end In this course, the course average is the sum of the category averages for \"Lab\" and \"Exam\". Note: To make these changes live, you must select \"Reload course config file\" on the \"Manage course\" page.","title":"Overriding Category and Course Averages"},{"location":"instructors/#handin-history","text":"For each lab, students can view all of their submissions, including any source code, and the problem scores, penalties, and total scores associated with those submissions, via the handin history page.","title":"Handin History"},{"location":"instructors/#gradesheet","text":"The gradesheet (not to be confused with the gradebook ) is the workhorse grading tool. Each assessment has a separate gradesheet with the following features: Provides an interface for manually entering problem scores (and problem feedback) for the most recent submission from each student. Provides an interface for viewing and annotating the submitted code. Displays the problem scores for the most recent submission for each student, summarizes any late penalties, and computes the total score. Provides a link to each student's handin history.","title":"Gradesheet"},{"location":"instructors/#gradebook","text":"The gradebook comes in two forms. The student gradebook displays the grades for a particular student, including total scores for each assessment, category averages, and the course average. The instructor gradebook is a table that displays the grades for the most recent submission of each student, including assessment total scores, category averages and course average. For the gradebook calculations, submissions are classified as one of three types: \"Normal\", \"No Grade\" or \"Excused\". A \"No Grade\" submission will show up in the gradebook as NG and a zero will be used when calculating averages. An \"Excused\" submission will show up in the gradebook as EXC and will not be used when calculating averages. To auto-expand a column in the gradebook , double-click the edge of that column header.","title":"Gradebook"},{"location":"instructors/#releasing-grades","text":"Manually assigned grades are by default not released, and therefore not visible to students. You can release grades on an individual basis while grading, or release all available grades in bulk by using the \"Release all grades\" option. You can also reverse this process using the \"Withdraw all grades\" option. (The word \"withdraw\" is perhaps unfortunate. No grades are ever deleted. They are simply withdrawn from the student's view.)","title":"Releasing Grades"},{"location":"instructors/#file-manager","text":"Displays files within a course and allows you to rename, delete, and add files automatically via an interface. You can view all courses that you are an instructor of and all associated files. You can access the File Manager under Admin Course after clicking Manage Course. Note: You cannot create folders or upload files in the root Courses directory. To create a course, you should navigate to Create New Course via the Manage Autolab dropdown. You cannot rename files in the root Courses directory. To rename a course, you should navigate to the course, click Manage Course, and click Course Settings. You cannot upload files that are larger than 1 GB. You cannot create a folder or upload a file with a name that already exists. Deleting a folder will also delete all of its contents. Downloading a folder will create a tar with all of its contents. Download Selected downloads each of the selected files/folders separately. When clicking on a file, it will be automatically downloaded if it is larger than 1GB or a binary file. Otherwise, the file contents will be displayed.","title":"File Manager"},{"location":"lab-hooks/","text":"Lab Hooks This document provides a summary of all the lab (aka assessment) hooks available to an instructor. Lab hooks are defined in the lab's configuration file, <labname>.rb . The configuration file is located in the lab's directory, at the path <coursename>/<labname>/<labname>.rb . To make changes live, you must select the \"Reload config file\" option on the lab's index page. You can also upload a new config file from the lab's setting page. To debug the hooks, you can make use of the ASSESSMENT_LOGGER.log(<expr>) method to print output into the lab's log.txt file. Function Arity When defining the hooks below, be sure that they take the correct number of arguments. Failure to do so might leave your assessment in a hard-to-recover state. Modify Submission Score Hook: modifySubmissionScores By default, the scores output by the autograder will be directly assigned to the individual problem scores. This hook allows you to override the score calculation for a lab. def assessmentVariables variables = {} variables [ \"previous_submissions_lookback\" ] = 1000 variables [ \"exclude_autograding_in_progress_submissions\" ] = false variables end def modifySubmissionScores ( scores , previous_submissions , problems ) scores [ \"Score1\" ] = - ( previous_submissions . length ) # Get Score1 score for previous submission scores [ \"Score2\" ] = previous_submissions [ 0 ]. scores . find_or_initialize_by ( :problem_id => problems . find_by ( :name => \"Score1\" ) . id ) . score # Get Score2 score for previous submission scores [ \"Score3\" ] = previous_submissions [ 0 ]. scores . find_or_initialize_by ( :problem_id => problems . find_by ( :name => \"Score2\" ) . id ) . score scores end The code snippet above allows you to create a lab that has a score that is a function of the number of previous submissions, and the scores of previous submissions. This particular lab has four problems called \"Autograded Score\", \"Score1\", \"Score2\", \"Score3\". It assigns the score of \"Score1\" to be the negative of the number of previous submissions, and the score of \"Score2\" to be the score of \"Score1\" of the previous submission, and the score of \"Score3\" to be the score of \"Score2\" of the previous submission. There are two settings that you can change in the assessmentVariables function that will affect the behavior of the modifySubmissionScores function: previous_submissions_lookback : The number of previous submissions to look back when calculating the score. By default, it is set to 1000. exclude_autograding_in_progress_submissions : If set to true , the submissions that are currently being autograded will be excluded when passed into the modifySubmissionScores function. By default, it is set to false . The three arguments passed into the modifySubmissionScores function are: scores : A hash that maps the problem name to the score. previous_submissions : A list of previous submissions, sorted by submission time in descending order, it is an ActiveRecord object. problems : A list of problems in the lab, it is an ActiveRecord object. For more information on how to use ActiveRecord, please refer to the ActiveRecord documentation . For the schema of the Submission and Problem models, please refer to the Autolab Schema . Raw Score Calculations Hook: raw_score By default, the raw score for a submission is the sum of the individual problem scores. This hook allows you to override the raw score calculation for a lab. def raw_score ( score ) perfindex = score [ \"Autograded Score\" ]. to_f () heap = score [ \"Heap Checker\" ]. to_f () style = score [ \"Style\" ]. to_f () deduct = score [ \"CorrectnessDeductions\" ]. to_f () perfpoints = perfindex # perfindex below 50 gets autograded score of 0. if perfindex < 50 . 0 then perfpoints = 0 else perfpoints = perfindex end return perfpoints + heap + style + deduct end This particular lab has four problems called \"Autograded Score\", \"Heap Checker\", \"Style\", and \"CorrectnessDeductions\". The code snippet above sets an \"Autograded Score\" of less than 50 to 0 when the raw score is calculated. Submission File MIME Type Check Hook: checkMimeType By default, Autolab does not perform MIME type check for submission files. This hook allows you to define a MIME type check method. For example, to prevent students from submitting a binary file to the assessment, you might add the following checkMimeType function: def checkMimeType ( contentType , fileName ) return contentType != \"application/octet-stream\" end As of now, the only way to provide a more informative message to students is to raise an error: def checkMimeType ( contentType , fileName ) raise \"Do not submit binary files!\" if contentType == \"application/octet-stream\" return true end This results in the following error message being displayed to students when they attempt to submit binary files. Alternatively, you can use the file name to do file type checks. The following snippet prevents students from submitting python files: def checkMimeType ( contentType , fileName ) return fileName . split ( \".\" ) [- 1 ] != \"py\" end Note that this function does not have access to Rails controller attributes such as flash or params . Attempts to access what's beyond the arguments passed to the function will result in an error. Lab Handout Hook: handout By default, the handout provided to students when they click on \"Download handout\" is the file path or URL specified in the lab settings. This hook allows you to run custom code when the button is clicked and then return a path to the handout. This can be useful in creating customized handouts on a per-student basis. Restrictions on Handout Path For security reasons, the handout path returned by the hook must reside within the lab folder. def handout course = @assessment . course . name asmt = @assessment . name file = \"autograde-Makefile\" file_path = \"courses/ #{ course } / #{ asmt } / #{ file } \" filename = \"makefile\" Hash [ \"fullpath\" , file_path , \"filename\" , filename ] end The code snippet above downloads the autograde-Makefile (assuming it resides at the root of the lab directory) as the file makefile . On Autograde Completion Hook: autogradeDone By default, upon autograding completion, the feedback is saved to the feedback file and submission(s) scores are updated, amongst other things. This hook allows you to override this behavior. Unless you know what you're doing, you should probably leave this hook alone. def autogradeDone ( submissions , feedback ) # submissions: all the submissions connected to this feedback (could be multiple if this was a group submission) # feedback: feedback string from the autograder # default behavior - write feedback into feedback file saveAutograde ( submissions , feedback ) # you should probably call this end List Options Hook: listOptions By default, the following options are displayed to students when viewing an assessment: View handin history View writeup (if the assessment has a writeup defined) Download handout (if the assessment has a handout defined) Group options (if the assessment has groups enabled) View scoreboard (if the assessment has a scoreboard) This hook allows you to disable the display of these options and/or display your own options. Only affects the dropdown Even if certain options are hidden from the \"Options\" dropdown through this hook, students can still navigate directly to the corresponding pages if they so wish. def listOptions ( list ) # The default options are: history, writeup, handout, groups, scoreboard # Delete the options that you do not want to show list . delete ( \"history\" ) # hides \"View handin history\" # You can display your own options # list[<key>] = <value> where <key> is the url route and <value> is the text to display for the option # (Non-exhaustive) possible values for <key>: history, writeup, handout list [ \"history\" ] = \"View your official scores\" list [ \"writeup\" ] = \"View the writeup\" list [ \"handout\" ] = \"Download your bomb\" # Avoid setting custom keys to a value of nil, as that is how the code distinguishes default options. return list end The code snippet above hides the default option \"View handin history\" and defines three custom options history , writeup , and handout . In particular, the link to history now has the text \"View your official scores\". Valid keys for options Other than history , writeup , and handout , a valid key could technically be any route associated with assessments. However, many of these routes are not visible to students and it would not make sense to list them. For this reason, the following keys are explicitly ignored (but this is not comprehensive): edit , viewGradesheet , reload Invalid keys will be marked as such. Scoreboard Header Hook: scoreboardHeader By default, the scoreboard header follows the following format: If a custom column specification is provided: Rank , Nickname , Version , Time , followed by the columns defined in the column specification Otherwise: Rank , Nickname , Version , Time , Total , followed by the name of each problem in the assessment This hook allows for even greater flexibility in the definition of the scoreboard header. If defined, it takes precedence over a custom column specification. Restrictions on HTML tags Only th and td tags can be used, all other tags will be stripped. def scoreboardHeader \"<th>Nickname</th><th>Version</th><th>Time</th><th>Total</th><th>Problem 1</th><th>Problem 2</th>\" end The code snippet above defines a scoreboard whose header consists of the fields Rank , Nickname , Version , Time , Total , Problem 1 , Problem 2 . Thus, other than the Rank column, the number of columns and their names can be fully customized. Scoreboard Entries Hook: createScoreboardEntry By default, each scoreboard row, corresponding to a user, follows the following format: Rank , Nickname , Version , Time , Total , followed by the score for each problem in the assessment. This hook allows for greater flexibility in the values displayed for each student. In particular, the values displayed for the columns beyond Rank , Nickname , Version , and Time can be configured. This hook should most likely be used in conjunction with the scoreboardHeader hook or a custom column specification . def createScoreboardEntry ( scores , autoresult ) defused = 0 explosions = 0 scores . each_pair do | name , value | if name == \"explosion\" explosions = value . to_i () else defused += value . to_i () end end totalscore = raw_score ( scores ) [ defused , explosions , totalscore ] end Assuming a suitable raw_score method is defined, the code snippet above displays a student's score, together with statistics such as the value associated with the problem explosion and the sum of values associated with the other problems. Scoreboard Ordering Hook: scoreboardOrderSubmissions By default, scoreboard rows are sorted as follows: If a custom column specification is provided (and an autograder is defined): Sort the columns from left to right in descending/ascending order (depending on the column specification) Otherwise: Sort by decreasing total score, followed by increasing submission time This hook allows for greater flexibility in the sorting logic. # The hash contains the following keys: # {:nickname, :andrewID, :fullName, :problems, :time, :version, :autoresult, :entry} # where :entry is the scoreboard entry array returned by createScoreboardEntry def scoreboardOrderSubmissions ( a , b ) # In this example, assume that each entry has the format [defused, explosions, totalscore] # Entry A ranks higher than entry B if it has more defused phases. rank = - ( a [ :entry ][ 0 ] <=> b [ :entry ][ 0 ] ) if rank != 0 return rank end # If defused phases are equal, entry A ranks higher than entry # B if it has _fewer_ explosions. rank = a [ :entry ][ 1 ] <=> b [ :entry ][ 1 ] if rank != 0 return - rank end # As a final tiebreaker, earlier submissions rank higher. - ( a [ :time ] <=> b [ :time ] ) end The code snippet above sorts by the first column ( defused ) in decreasing order, followed by the second column ( explosions ) in increasing order. As a final tiebreaker, it sorts by time. Autograding Input Files Hook: autogradeInputFiles By default, the following autograding input files are sent to Tango The student's handin file The makefile that runs the process The tarfile with all of the files needed by the autograder This hook allows you to define a custom list of input files to be sent instead. def autogradeInputFiles ( ass_dir , assessment , submission ) local_handin = submission . handin_file_path remote_handin = submission . handin_file_long_filename dest_handin = assessment . handin_filename # localFile: path to file on local machine # remoteFile: name of the file on the Tango machine # - if this file is unique per-submission (e.g. student's code), then the filename should also be unique per-submission # so as to avoid name-collisions # - if undefined, value of localFile will be used instead # destFile: name of the file on the destination machine (e.g. docker container) handin = { \"localFile\" => local_handin , \"remoteFile\" => remote_handin , \"destFile\" => dest_handin } [ handin ] # and any other files required end Autoresult Parsing Hook: parseAutoresult By default, the autoresult string from the autograder (the last non-empty line) is assumed to be encoded in JSON and is parsed as such. If a different format is used for the autoresult string, this hook allows you to define custom parsing logic. # _isOfficial is true except for log submissions # If \"Allow unofficial\" is disabled, don't worry about this. def parseAutoresult ( autoresult , _isOfficial ) # Return a hash of problem name to scores { \"Problem 1\" : 1 , \"Problem 2\" : 2 , \"Problem 3\" : 3 , \"Problem 4\" : 4 , \"Problem 5\" : 5 , \"Problem 6\" : 6 } end","title":"Lab Hooks"},{"location":"lab-hooks/#lab-hooks","text":"This document provides a summary of all the lab (aka assessment) hooks available to an instructor. Lab hooks are defined in the lab's configuration file, <labname>.rb . The configuration file is located in the lab's directory, at the path <coursename>/<labname>/<labname>.rb . To make changes live, you must select the \"Reload config file\" option on the lab's index page. You can also upload a new config file from the lab's setting page. To debug the hooks, you can make use of the ASSESSMENT_LOGGER.log(<expr>) method to print output into the lab's log.txt file. Function Arity When defining the hooks below, be sure that they take the correct number of arguments. Failure to do so might leave your assessment in a hard-to-recover state.","title":"Lab Hooks"},{"location":"lab-hooks/#modify-submission-score","text":"Hook: modifySubmissionScores By default, the scores output by the autograder will be directly assigned to the individual problem scores. This hook allows you to override the score calculation for a lab. def assessmentVariables variables = {} variables [ \"previous_submissions_lookback\" ] = 1000 variables [ \"exclude_autograding_in_progress_submissions\" ] = false variables end def modifySubmissionScores ( scores , previous_submissions , problems ) scores [ \"Score1\" ] = - ( previous_submissions . length ) # Get Score1 score for previous submission scores [ \"Score2\" ] = previous_submissions [ 0 ]. scores . find_or_initialize_by ( :problem_id => problems . find_by ( :name => \"Score1\" ) . id ) . score # Get Score2 score for previous submission scores [ \"Score3\" ] = previous_submissions [ 0 ]. scores . find_or_initialize_by ( :problem_id => problems . find_by ( :name => \"Score2\" ) . id ) . score scores end The code snippet above allows you to create a lab that has a score that is a function of the number of previous submissions, and the scores of previous submissions. This particular lab has four problems called \"Autograded Score\", \"Score1\", \"Score2\", \"Score3\". It assigns the score of \"Score1\" to be the negative of the number of previous submissions, and the score of \"Score2\" to be the score of \"Score1\" of the previous submission, and the score of \"Score3\" to be the score of \"Score2\" of the previous submission. There are two settings that you can change in the assessmentVariables function that will affect the behavior of the modifySubmissionScores function: previous_submissions_lookback : The number of previous submissions to look back when calculating the score. By default, it is set to 1000. exclude_autograding_in_progress_submissions : If set to true , the submissions that are currently being autograded will be excluded when passed into the modifySubmissionScores function. By default, it is set to false . The three arguments passed into the modifySubmissionScores function are: scores : A hash that maps the problem name to the score. previous_submissions : A list of previous submissions, sorted by submission time in descending order, it is an ActiveRecord object. problems : A list of problems in the lab, it is an ActiveRecord object. For more information on how to use ActiveRecord, please refer to the ActiveRecord documentation . For the schema of the Submission and Problem models, please refer to the Autolab Schema .","title":"Modify Submission Score"},{"location":"lab-hooks/#raw-score-calculations","text":"Hook: raw_score By default, the raw score for a submission is the sum of the individual problem scores. This hook allows you to override the raw score calculation for a lab. def raw_score ( score ) perfindex = score [ \"Autograded Score\" ]. to_f () heap = score [ \"Heap Checker\" ]. to_f () style = score [ \"Style\" ]. to_f () deduct = score [ \"CorrectnessDeductions\" ]. to_f () perfpoints = perfindex # perfindex below 50 gets autograded score of 0. if perfindex < 50 . 0 then perfpoints = 0 else perfpoints = perfindex end return perfpoints + heap + style + deduct end This particular lab has four problems called \"Autograded Score\", \"Heap Checker\", \"Style\", and \"CorrectnessDeductions\". The code snippet above sets an \"Autograded Score\" of less than 50 to 0 when the raw score is calculated.","title":"Raw Score Calculations"},{"location":"lab-hooks/#submission-file-mime-type-check","text":"Hook: checkMimeType By default, Autolab does not perform MIME type check for submission files. This hook allows you to define a MIME type check method. For example, to prevent students from submitting a binary file to the assessment, you might add the following checkMimeType function: def checkMimeType ( contentType , fileName ) return contentType != \"application/octet-stream\" end As of now, the only way to provide a more informative message to students is to raise an error: def checkMimeType ( contentType , fileName ) raise \"Do not submit binary files!\" if contentType == \"application/octet-stream\" return true end This results in the following error message being displayed to students when they attempt to submit binary files. Alternatively, you can use the file name to do file type checks. The following snippet prevents students from submitting python files: def checkMimeType ( contentType , fileName ) return fileName . split ( \".\" ) [- 1 ] != \"py\" end Note that this function does not have access to Rails controller attributes such as flash or params . Attempts to access what's beyond the arguments passed to the function will result in an error.","title":"Submission File MIME Type Check"},{"location":"lab-hooks/#lab-handout","text":"Hook: handout By default, the handout provided to students when they click on \"Download handout\" is the file path or URL specified in the lab settings. This hook allows you to run custom code when the button is clicked and then return a path to the handout. This can be useful in creating customized handouts on a per-student basis. Restrictions on Handout Path For security reasons, the handout path returned by the hook must reside within the lab folder. def handout course = @assessment . course . name asmt = @assessment . name file = \"autograde-Makefile\" file_path = \"courses/ #{ course } / #{ asmt } / #{ file } \" filename = \"makefile\" Hash [ \"fullpath\" , file_path , \"filename\" , filename ] end The code snippet above downloads the autograde-Makefile (assuming it resides at the root of the lab directory) as the file makefile .","title":"Lab Handout"},{"location":"lab-hooks/#on-autograde-completion","text":"Hook: autogradeDone By default, upon autograding completion, the feedback is saved to the feedback file and submission(s) scores are updated, amongst other things. This hook allows you to override this behavior. Unless you know what you're doing, you should probably leave this hook alone. def autogradeDone ( submissions , feedback ) # submissions: all the submissions connected to this feedback (could be multiple if this was a group submission) # feedback: feedback string from the autograder # default behavior - write feedback into feedback file saveAutograde ( submissions , feedback ) # you should probably call this end","title":"On Autograde Completion"},{"location":"lab-hooks/#list-options","text":"Hook: listOptions By default, the following options are displayed to students when viewing an assessment: View handin history View writeup (if the assessment has a writeup defined) Download handout (if the assessment has a handout defined) Group options (if the assessment has groups enabled) View scoreboard (if the assessment has a scoreboard) This hook allows you to disable the display of these options and/or display your own options. Only affects the dropdown Even if certain options are hidden from the \"Options\" dropdown through this hook, students can still navigate directly to the corresponding pages if they so wish. def listOptions ( list ) # The default options are: history, writeup, handout, groups, scoreboard # Delete the options that you do not want to show list . delete ( \"history\" ) # hides \"View handin history\" # You can display your own options # list[<key>] = <value> where <key> is the url route and <value> is the text to display for the option # (Non-exhaustive) possible values for <key>: history, writeup, handout list [ \"history\" ] = \"View your official scores\" list [ \"writeup\" ] = \"View the writeup\" list [ \"handout\" ] = \"Download your bomb\" # Avoid setting custom keys to a value of nil, as that is how the code distinguishes default options. return list end The code snippet above hides the default option \"View handin history\" and defines three custom options history , writeup , and handout . In particular, the link to history now has the text \"View your official scores\". Valid keys for options Other than history , writeup , and handout , a valid key could technically be any route associated with assessments. However, many of these routes are not visible to students and it would not make sense to list them. For this reason, the following keys are explicitly ignored (but this is not comprehensive): edit , viewGradesheet , reload Invalid keys will be marked as such.","title":"List Options"},{"location":"lab-hooks/#scoreboard-header","text":"Hook: scoreboardHeader By default, the scoreboard header follows the following format: If a custom column specification is provided: Rank , Nickname , Version , Time , followed by the columns defined in the column specification Otherwise: Rank , Nickname , Version , Time , Total , followed by the name of each problem in the assessment This hook allows for even greater flexibility in the definition of the scoreboard header. If defined, it takes precedence over a custom column specification. Restrictions on HTML tags Only th and td tags can be used, all other tags will be stripped. def scoreboardHeader \"<th>Nickname</th><th>Version</th><th>Time</th><th>Total</th><th>Problem 1</th><th>Problem 2</th>\" end The code snippet above defines a scoreboard whose header consists of the fields Rank , Nickname , Version , Time , Total , Problem 1 , Problem 2 . Thus, other than the Rank column, the number of columns and their names can be fully customized.","title":"Scoreboard Header"},{"location":"lab-hooks/#scoreboard-entries","text":"Hook: createScoreboardEntry By default, each scoreboard row, corresponding to a user, follows the following format: Rank , Nickname , Version , Time , Total , followed by the score for each problem in the assessment. This hook allows for greater flexibility in the values displayed for each student. In particular, the values displayed for the columns beyond Rank , Nickname , Version , and Time can be configured. This hook should most likely be used in conjunction with the scoreboardHeader hook or a custom column specification . def createScoreboardEntry ( scores , autoresult ) defused = 0 explosions = 0 scores . each_pair do | name , value | if name == \"explosion\" explosions = value . to_i () else defused += value . to_i () end end totalscore = raw_score ( scores ) [ defused , explosions , totalscore ] end Assuming a suitable raw_score method is defined, the code snippet above displays a student's score, together with statistics such as the value associated with the problem explosion and the sum of values associated with the other problems.","title":"Scoreboard Entries"},{"location":"lab-hooks/#scoreboard-ordering","text":"Hook: scoreboardOrderSubmissions By default, scoreboard rows are sorted as follows: If a custom column specification is provided (and an autograder is defined): Sort the columns from left to right in descending/ascending order (depending on the column specification) Otherwise: Sort by decreasing total score, followed by increasing submission time This hook allows for greater flexibility in the sorting logic. # The hash contains the following keys: # {:nickname, :andrewID, :fullName, :problems, :time, :version, :autoresult, :entry} # where :entry is the scoreboard entry array returned by createScoreboardEntry def scoreboardOrderSubmissions ( a , b ) # In this example, assume that each entry has the format [defused, explosions, totalscore] # Entry A ranks higher than entry B if it has more defused phases. rank = - ( a [ :entry ][ 0 ] <=> b [ :entry ][ 0 ] ) if rank != 0 return rank end # If defused phases are equal, entry A ranks higher than entry # B if it has _fewer_ explosions. rank = a [ :entry ][ 1 ] <=> b [ :entry ][ 1 ] if rank != 0 return - rank end # As a final tiebreaker, earlier submissions rank higher. - ( a [ :time ] <=> b [ :time ] ) end The code snippet above sorts by the first column ( defused ) in decreasing order, followed by the second column ( explosions ) in increasing order. As a final tiebreaker, it sorts by time.","title":"Scoreboard Ordering"},{"location":"lab-hooks/#autograding-input-files","text":"Hook: autogradeInputFiles By default, the following autograding input files are sent to Tango The student's handin file The makefile that runs the process The tarfile with all of the files needed by the autograder This hook allows you to define a custom list of input files to be sent instead. def autogradeInputFiles ( ass_dir , assessment , submission ) local_handin = submission . handin_file_path remote_handin = submission . handin_file_long_filename dest_handin = assessment . handin_filename # localFile: path to file on local machine # remoteFile: name of the file on the Tango machine # - if this file is unique per-submission (e.g. student's code), then the filename should also be unique per-submission # so as to avoid name-collisions # - if undefined, value of localFile will be used instead # destFile: name of the file on the destination machine (e.g. docker container) handin = { \"localFile\" => local_handin , \"remoteFile\" => remote_handin , \"destFile\" => dest_handin } [ handin ] # and any other files required end","title":"Autograding Input Files"},{"location":"lab-hooks/#autoresult-parsing","text":"Hook: parseAutoresult By default, the autoresult string from the autograder (the last non-empty line) is assumed to be encoded in JSON and is parsed as such. If a different format is used for the autoresult string, this hook allows you to define custom parsing logic. # _isOfficial is true except for log submissions # If \"Allow unofficial\" is disabled, don't worry about this. def parseAutoresult ( autoresult , _isOfficial ) # Return a hash of problem name to scores { \"Problem 1\" : 1 , \"Problem 2\" : 2 , \"Problem 3\" : 3 , \"Problem 4\" : 4 , \"Problem 5\" : 5 , \"Problem 6\" : 6 } end","title":"Autoresult Parsing"},{"location":"lab/","text":"Guide for Lab Authors This guide explains how to create autograded programming assignments (labs) for the Autolab system. While reading through the documentation is recommended , for a quick start, here's a short video that gives a brief introduction of autograders. Writing Autograders An autograder is a program that takes a student's work as input, and generates some quantitative evaluation of that work as output. The student's work consists of one or more source files written in an arbitrary programming language. The autograder processes these files and generates arbitrary text lines on stdout. What's written to stdout will be displayed to the students as autograder feedback . Streaming Output As of Autolab v2.10, any output written into the stdout will be streamed directly for students to see, so students can see the live progress of the autograding. Many programming languages do buffered writes to stdout , so if you want live progress, you would have to guarantee that you are writing to stdout by flushing the buffer accordingly (e.g. Python print 's flush flag , C's fflush , CPP's fflush ) The last text line on stdout must be a JSON string, called an autoresult , that assigns an autograded score to one or more problems, and optionally, generates the scoreboard entries for this submission. The JSON autoresult is a \"scores\" hash that assigns a numerical score to one or more problems, and an optional \"scoreboard\" array that provides the scoreboard entries for this submission. For example, { \"scores\" : { \"Prob1\" : 10 , \"Prob2\" : 5 } } assigns 10 points to \"Prob1\" and 5 points to \"Prob2\" for this submission. The names of the problems must exactly match the names of the problems for this lab on the Autolab website. Not all problems need to be autograded. For example, there might be a problem for this assessment called \"Style\" that you grade manually after the due date. If you used the Autolab website to configure a scoreboard for this lab with three columns called \"Prob1\", \"Prob2\", and \"Total\", then the autoresult might be: { \"scores\" : { \"Prob1\" : 10 , \"Prob2\" : 5 }, \"scoreboard\" : [ 10 , 5 , 15 ] } By convention, an autograder accepts an optional -A command line argument that tells it to emit the JSON autoresult. So if you run the autograder outside of the context of Autolab, you can suppress the autoresult line by calling the autograder without the -A argument. One of the nice properties of Autolab autograders is that they can be written and tested offline, without requiring any interaction with Autolab. Writing autograders is not easy, but the fact that they can be developed offline allows you to develop and test them in your own familiar computing environment. To format your autoresult feedback provided to the students, use the formatted feedback feature . Installing Autograded Labs After you've written and tested the autograder, you then use the Autolab website to create the autograded lab. Autolab supports creating new labs from scratch, or reusing labs from previous semesters. We'll describe each of these in turn. Creating an Autograded Lab from Scratch Step 1: Create the new lab. Create a new lab by clicking the \"Install Assessment\" button and choosing \"Option 1: Create a New Assessment from Scratch.\" For course <course> and lab <lab> , this will create a lab directory in the Autolab file hierarchy called courses/<course>/<lab> . This initial directory contains a couple of config files and a directory called <lab>/handin that will contain all student handin files. In general, you should never modify any of these. Attention CMU Lab Authors At CMU, the lab directory is called /afs/cs/academic/class/<course>/autolab/<lab> . For example: /afs/cs/academic/class/15213-f16/autolab/foo is the lab directory for the lab named foo for the Fall 2016 instance of 15-213. All lab-related files must go in this autolab directory to avoid permissions issues. Step 2: Configure the lab for autograding. Using the \"Edit Assessment\" page, turn on autograding for this lab by selecting \"Add Autograder.\" You will be asked for the name of the image to be used for autograding this lab. The default image distributed with Autolab is an Ubuntu image called autograding_image . If your class needs different software, then you or your facilities staff will need to update the default image or create a new one. Attention CMU Lab Authors The default autograding image at CMU is called rhel.img and is a copy of the software on the CMU Andrew machines ( linux.andrew.cmu.edu ). If you need custom software installed, please send mail to autolab-help@andrew.cmu.edu. If you want a scoreboard, you should select \"Add Scoreboard,\" which will allow you to specify the number of columns and their names. The \"Add Scoreboard\" page contains a tutorial on how to do this. You'll also need to define the names and point values for all the problems in this lab, including the autograded ones. Each student submission is a single file, either a text source file or an archive file containing multiple files and directories. You'll need to specify the base name for the student submission files (e.g., mm.c , handin.tar ). Step 3: Add the required autograding files. For an autograded lab, Autolab expects the following two autograding files in the lab directory: autograde-Makefile : runs the autograder on a student submission. autograde.tar : contains all of the files (except for the student handin file) that are needed for autograding. Each time a student submits their work or an instructor requests a regrade, Autolab copies the student handin file, along with the two autograding files, to an empty directory on an autograding instance , renames the student handin file to base name (e.g., hello.c, handin.tar), renames autograde-Makefile to Makefile , executes the command make on the autograding instance, and finally captures the stdout generated by the autograder, and parses the resulting JSON autoresult to determine the autograded scores. Importing an Autograded Lab from a Previous Semester If you've created a lab for a course in a previous semester and have access to the lab directory (as we do at CMU via AFS), you can import the lab into your current course by copying the lab directory from the previous course to the current course, cleaning out the handin directory, then visiting the \"Install Assessment\" page and selecting \"Option 2: Import an existing assessment from the file system.\" Autolab will give you a list of all of the directories that appear to be uninstalled labs, from which you can select your particular lab. If you don't have access to the lab directory, another option is to import a lab from a tarball that was created by running \"Export assessment\" in an instance of a lab from a previous semester. Visit the \"Install Assessment\" page and select \"Option 3: Import an existing assessment from tarball.\" This will upload the tarball, create a new lab directory by expanding the tarball, and then import the directory. Example: Hello Lab In this section we'll look at the simplest possible autograded lab we could imagine, called, appropriately enough, the Hello Lab (with tarball ), which is stored in a lab directory called hello in the Autolab github repo. While it's trivial, it illustrates all of the aspects of developing an autograded lab, and provides a simple example that you can use for sanity testing on your Autolab installation. In this lab, students are asked to write a version of the K&R \"hello, world\" program, called hello.c . The autograder simply checks that the submitted hello.c program compiles and runs with an exit status of zero. If so, the submission gets 100 points. Otherwise it gets 0 points. Directory Structure Autolab expects to find the autograde-Makefile and autograde.tar files in the hello lab directory, but otherwise places no constraints on the contents and organization of this directory. However, based on our experience, we strongly recommend a directory structure with the following form: hello/README : # Basic files created by the lab author Makefile Builds the lab from src/ README autograde-Makefile Makefile that runs the autograder src/ Contains all src files and solutions test-autograder/ For testing autograder offline writeup/ Lab writeup that students view from Autolab # Files created by running make hello-handout/ The directory that is handed out to students, created using files from src/. hello-handout.tar Archive of hello-handout directory autograde.tar File that is copied to the autograding instance (along with autograde-Makefile and student handin file) # Files created and managed by Autolab handin/ All students handin files hello.rb Config file hello.yml Database properties that persist from semester to semester log.txt Log of autograded submissions The key idea with this directory structure is to place all code for the lab in the src directory, including the autograding code and any starter code handed out to students in the handout directory ( hello-handout.tar in this example). Keeping all hard state in the src directory helps limit inconsistencies. The main makefile creates hello-handout by copying files from src , and then tars it up: hello/Makefile : # # Makefile to manage the example Hello Lab # # Get the name of the lab directory LAB = $( notdir $( PWD )) all : handout handout - tarfile handout : # Rebuild the handout directory that students download ( rm -rf $( LAB ) -handout ; mkdir $( LAB ) -handout ) cp -p src/Makefile-handout $( LAB ) -handout/Makefile cp -p src/README-handout $( LAB ) -handout/README cp -p src/hello.c-handout $( LAB ) -handout/hello.c cp -p src/driver.sh $( LAB ) -handout handout-tarfile : handout # Build *-handout.tar and autograde.tar tar cvf $( LAB ) -handout.tar $( LAB ) -handout cp -p $( LAB ) -handout.tar autograde.tar clean : # Clean the entire lab directory tree. Note that you can run # \"make clean; make\" at any time while the lab is live with no # adverse effects. rm -f *~ *.tar ( cd src ; make clean ) ( cd test-autograder ; make clean ) rm -rf $( LAB ) -handout rm -f autograde.tar # # CAREFULL!!! This will delete all student records in the logfile and # in the handin directory. Don't run this once the lab has started. # Use it to clean the directory when you are starting a new version # of the lab from scratch, or when you are debugging the lab prior # to releasing it to the students. # cleanallfiles : # Reset the lab from scratch. make clean rm -f log.txt rm -rf handin/* Filenames are disambiguated by appending -handout , which is stripped when they are copied to the handout directory. For example, src/hello.c is the instructor's solution file, and src/hello.c-handout is the starter code that is given to the students in hello-handout/hello.c . And src/README is the README for the src directory and src/README-handout is the README that is handed out to students in hello-handout/README . To build the lab, type make clean; make . You can do this as often as you like while the lab is live with no adverse effects. However, be careful to never type make cleanallfiles while the lab is live; this should only be done before the lab goes live; never during or after. Source Directory The hello/src/ directory contains all of the code files for the Hello Lab, including the files that are handed out to students: hello/src/README : # Autograder and solution files Makefile Makefile and ... README ... README for this directory driver.sh* Autograder hello.c Solution hello.c file # Files that are handed out to students Makefile-handout Makefile and ... README-handout ... README handed out to students hello.c-handout Blank hello.c file handed out to students Handout Directory The hello/hello-handout/ directory contains the files that the students will use to work on the lab. It contains no hard state, and is populated entirely with files from hello/src : hello/hello-handout/README : For this lab, you should write a tiny C program, called \"hello.c\", that prints \"hello, world\" to stdout and then indicates success by exiting with a status of zero. To test your work: $ make clean; make; ./hello To run the same autograder that Autolab will use when you submit: $ ./driver.sh Files: README This file Makefile Compiles hello.c driver.sh Autolab autograder hello.c Empty C file that you will edit hello/hello-handout/Makefile contains the rules that compile the student source code: # Student makefile for the Hello Lab all : gcc hello.c -o hello clean : rm -rf *~ hello To compile and run their code, students type: $ make clean ; make $ ./hello Autograder The autograder for the Hello Lab is a trivially simple bash script called driver.sh that compiles and runs hello.c and verifies that it returns with an exit status of zero: hello/src/driver.sh : #!/bin/bash # driver.sh - The simplest autograder we could think of. It checks # that students can write a C program that compiles, and then # executes with an exit status of zero. # Usage: ./driver.sh # Compile the code echo \"Compiling hello.c\" ( make clean ; make ) status = $? if [ ${ status } -ne 0 ] ; then echo \"Failure: Unable to compile hello.c (return status = ${ status } )\" echo \"{\\\"scores\\\": {\\\"Correctness\\\": 0}}\" exit fi # Run the code echo \"Running ./hello\" ./hello status = $? if [ ${ status } -eq 0 ] ; then echo \"Success: ./hello runs with an exit status of 0\" echo \"{\\\"scores\\\": {\\\"Correctness\\\": 100}}\" else echo \"Failure: ./hello fails or returns nonzero exit status of ${ status } \" echo \"{\\\"scores\\\": {\\\"Correctness\\\": 0}}\" fi exit For example: $ ./driver.sh # Compiling hello.c # rm -rf *~ hello # gcc hello.c -o hello # Running ./hello # Hello, world # Success: ./hello runs with an exit status of 0 # {\"scores\": {\"Correctness\": 100}} Notice that the autograder expects the hello lab on the Autolab front-end to have been defined with a problem called \"Correctness\", with a maximum value of 100 points. If you forget to define the problems listed in the JSON autoresult, scores will still be logged, but they won't be posted to the database. Required Autograding Files Autolab requires two autograding files called autograde.tar , which contains all of the code required by the autograder, and autograde-Makefile , which runs the autograder on the autograding image when each submission is graded. For the Hello Lab, autograde.tar is simply a copy of the hello-handout.tar file that is handed out to students. And here is the corresponding hello/autograde-makefile : all : tar xvf autograde.tar cp hello.c hello-handout ( cd hello-handout ; ./driver.sh ) clean : rm -rf *~ hello-handout The makefile expands autograde.tar into hello-handout , copies hello.c (the submission file) into hello-handout , changes directory to hello-handout , builds the autograder, and then runs it. Test Directory For our labs, we like to setup a test directory (called test-autograder in this example), that allows us to test our autograde-Makefile and autograde-tar files by simulating Autolab's behavior on the autograding instance. The test-autograder directory has the following form: $ cd test-autograder $ ls -l # total 3 # lrwxr-xr-x 1 droh users 21 Aug 4 16:43 Makefile -> autograde-Makefile # lrwxr-xr-x 1 droh users 16 Aug 4 16:43 autograde.tar -> autograde.tar # -rw-rw-r-- 1 droh users 113 Aug 4 16:44 hello.c To simulate Autolab's behavior on an autograding instance: $ cd test-autograder && make clean && make # Running ./hello # Hello, world # Success: ./hello runs with an exit status of 0 # {\"scores\": {\"Correctness\": 100}} Writeup directory The hello/writeup contains the detailed lab writeup, either html or pdf file, that students can download from the Autolab front end. Other sample autograders We have a repository for sample autograders written for popular languages, which includes Python, Java, C++, Golang, and Javascript. Troubleshooting Why is Autolab not displaying my stdout output? Autolab always shows the stdout output of running make, even when the program crashed or timed out. However, when it does crash and the expected autoresult json string is not appended to the output, parsing of the last line will fail. If this happens, any stdout output that is longer than 10,000 lines will be discarded (Note that this limit does not apply when the autoresult json is valid). Why is Autolab not able to stream my stdout output? The output only seems to be displayed when autograding is completed. Autolab can only stream stdout. Many programming languages do buffered writes to stdout , so you would have to guarantee that you are writing to stdout by flushing the buffer accordingly (e.g. Python print 's flush flag , C's fflush , CPP's fflush ) Why is my assessment failing to be imported? Import via Tar A valid assessment tar has a single root directory that's named after the assessment, containing an assessment yaml file with the same name as the root directory. See name rules below . An assessment config ruby file can optionally be provided, and the config file name also must have the same name as the root directory. The module name of the config file can be any name. The bare minimum for an assessment with a folder name of \"hello-asmt\" is shown below: hello-asmt/: hello-asmt.yml Import via file system The same restrictions to assessment imports via tar apply to importing via file system. Assessment Naming Rules Assessments on Autolab have two names: a \"name\" and a \"display name.\" The \"name\" is automatically created upon creation of a new assessment, based on the inputted \"display name.\" However, in the case of importing assessments from the file directory, or importing via tarball, the name of the folder containing the assessment files determines the assessment name. In addition, the assessment \"name\" is used to determine the name of the assessment config file. As a result, the assessment name must be a valid Ruby Identifier , but hyphens are also allowed. The assessment name must also be unique within a course. Valid Assessment Names autolabAsmt Autolab-Asmt Autolab_Asmt10 AuTol4b-_4smt Invalid Assessment Names _autolab 4utolab autolabAsmt!! -autolabAsmt .autolab.asmt When a new assessment is created from scratch, a \"name\" will be generated based on the \"display name,\" as long as a valid \"name\" can be generated by stripping out special characters. Otherwise, the server will reject the given \"display name.\" When editing an existing assessment, or by specifying the display_name field in the course yml file, the \"display name\" has no restrictions on special characters.","title":"Guide for Lab Authors"},{"location":"lab/#guide-for-lab-authors","text":"This guide explains how to create autograded programming assignments (labs) for the Autolab system. While reading through the documentation is recommended , for a quick start, here's a short video that gives a brief introduction of autograders.","title":"Guide for Lab Authors"},{"location":"lab/#writing-autograders","text":"An autograder is a program that takes a student's work as input, and generates some quantitative evaluation of that work as output. The student's work consists of one or more source files written in an arbitrary programming language. The autograder processes these files and generates arbitrary text lines on stdout. What's written to stdout will be displayed to the students as autograder feedback . Streaming Output As of Autolab v2.10, any output written into the stdout will be streamed directly for students to see, so students can see the live progress of the autograding. Many programming languages do buffered writes to stdout , so if you want live progress, you would have to guarantee that you are writing to stdout by flushing the buffer accordingly (e.g. Python print 's flush flag , C's fflush , CPP's fflush ) The last text line on stdout must be a JSON string, called an autoresult , that assigns an autograded score to one or more problems, and optionally, generates the scoreboard entries for this submission. The JSON autoresult is a \"scores\" hash that assigns a numerical score to one or more problems, and an optional \"scoreboard\" array that provides the scoreboard entries for this submission. For example, { \"scores\" : { \"Prob1\" : 10 , \"Prob2\" : 5 } } assigns 10 points to \"Prob1\" and 5 points to \"Prob2\" for this submission. The names of the problems must exactly match the names of the problems for this lab on the Autolab website. Not all problems need to be autograded. For example, there might be a problem for this assessment called \"Style\" that you grade manually after the due date. If you used the Autolab website to configure a scoreboard for this lab with three columns called \"Prob1\", \"Prob2\", and \"Total\", then the autoresult might be: { \"scores\" : { \"Prob1\" : 10 , \"Prob2\" : 5 }, \"scoreboard\" : [ 10 , 5 , 15 ] } By convention, an autograder accepts an optional -A command line argument that tells it to emit the JSON autoresult. So if you run the autograder outside of the context of Autolab, you can suppress the autoresult line by calling the autograder without the -A argument. One of the nice properties of Autolab autograders is that they can be written and tested offline, without requiring any interaction with Autolab. Writing autograders is not easy, but the fact that they can be developed offline allows you to develop and test them in your own familiar computing environment. To format your autoresult feedback provided to the students, use the formatted feedback feature .","title":"Writing Autograders"},{"location":"lab/#installing-autograded-labs","text":"After you've written and tested the autograder, you then use the Autolab website to create the autograded lab. Autolab supports creating new labs from scratch, or reusing labs from previous semesters. We'll describe each of these in turn.","title":"Installing Autograded Labs"},{"location":"lab/#creating-an-autograded-lab-from-scratch","text":"","title":"Creating an Autograded Lab from Scratch"},{"location":"lab/#step-1-create-the-new-lab","text":"Create a new lab by clicking the \"Install Assessment\" button and choosing \"Option 1: Create a New Assessment from Scratch.\" For course <course> and lab <lab> , this will create a lab directory in the Autolab file hierarchy called courses/<course>/<lab> . This initial directory contains a couple of config files and a directory called <lab>/handin that will contain all student handin files. In general, you should never modify any of these. Attention CMU Lab Authors At CMU, the lab directory is called /afs/cs/academic/class/<course>/autolab/<lab> . For example: /afs/cs/academic/class/15213-f16/autolab/foo is the lab directory for the lab named foo for the Fall 2016 instance of 15-213. All lab-related files must go in this autolab directory to avoid permissions issues.","title":"Step 1: Create the new lab."},{"location":"lab/#step-2-configure-the-lab-for-autograding","text":"Using the \"Edit Assessment\" page, turn on autograding for this lab by selecting \"Add Autograder.\" You will be asked for the name of the image to be used for autograding this lab. The default image distributed with Autolab is an Ubuntu image called autograding_image . If your class needs different software, then you or your facilities staff will need to update the default image or create a new one. Attention CMU Lab Authors The default autograding image at CMU is called rhel.img and is a copy of the software on the CMU Andrew machines ( linux.andrew.cmu.edu ). If you need custom software installed, please send mail to autolab-help@andrew.cmu.edu. If you want a scoreboard, you should select \"Add Scoreboard,\" which will allow you to specify the number of columns and their names. The \"Add Scoreboard\" page contains a tutorial on how to do this. You'll also need to define the names and point values for all the problems in this lab, including the autograded ones. Each student submission is a single file, either a text source file or an archive file containing multiple files and directories. You'll need to specify the base name for the student submission files (e.g., mm.c , handin.tar ).","title":"Step 2: Configure the lab for autograding."},{"location":"lab/#step-3-add-the-required-autograding-files","text":"For an autograded lab, Autolab expects the following two autograding files in the lab directory: autograde-Makefile : runs the autograder on a student submission. autograde.tar : contains all of the files (except for the student handin file) that are needed for autograding. Each time a student submits their work or an instructor requests a regrade, Autolab copies the student handin file, along with the two autograding files, to an empty directory on an autograding instance , renames the student handin file to base name (e.g., hello.c, handin.tar), renames autograde-Makefile to Makefile , executes the command make on the autograding instance, and finally captures the stdout generated by the autograder, and parses the resulting JSON autoresult to determine the autograded scores.","title":"Step 3: Add the required autograding files."},{"location":"lab/#importing-an-autograded-lab-from-a-previous-semester","text":"If you've created a lab for a course in a previous semester and have access to the lab directory (as we do at CMU via AFS), you can import the lab into your current course by copying the lab directory from the previous course to the current course, cleaning out the handin directory, then visiting the \"Install Assessment\" page and selecting \"Option 2: Import an existing assessment from the file system.\" Autolab will give you a list of all of the directories that appear to be uninstalled labs, from which you can select your particular lab. If you don't have access to the lab directory, another option is to import a lab from a tarball that was created by running \"Export assessment\" in an instance of a lab from a previous semester. Visit the \"Install Assessment\" page and select \"Option 3: Import an existing assessment from tarball.\" This will upload the tarball, create a new lab directory by expanding the tarball, and then import the directory.","title":"Importing an Autograded Lab from a Previous Semester"},{"location":"lab/#example-hello-lab","text":"In this section we'll look at the simplest possible autograded lab we could imagine, called, appropriately enough, the Hello Lab (with tarball ), which is stored in a lab directory called hello in the Autolab github repo. While it's trivial, it illustrates all of the aspects of developing an autograded lab, and provides a simple example that you can use for sanity testing on your Autolab installation. In this lab, students are asked to write a version of the K&R \"hello, world\" program, called hello.c . The autograder simply checks that the submitted hello.c program compiles and runs with an exit status of zero. If so, the submission gets 100 points. Otherwise it gets 0 points.","title":"Example: Hello Lab"},{"location":"lab/#directory-structure","text":"Autolab expects to find the autograde-Makefile and autograde.tar files in the hello lab directory, but otherwise places no constraints on the contents and organization of this directory. However, based on our experience, we strongly recommend a directory structure with the following form: hello/README : # Basic files created by the lab author Makefile Builds the lab from src/ README autograde-Makefile Makefile that runs the autograder src/ Contains all src files and solutions test-autograder/ For testing autograder offline writeup/ Lab writeup that students view from Autolab # Files created by running make hello-handout/ The directory that is handed out to students, created using files from src/. hello-handout.tar Archive of hello-handout directory autograde.tar File that is copied to the autograding instance (along with autograde-Makefile and student handin file) # Files created and managed by Autolab handin/ All students handin files hello.rb Config file hello.yml Database properties that persist from semester to semester log.txt Log of autograded submissions The key idea with this directory structure is to place all code for the lab in the src directory, including the autograding code and any starter code handed out to students in the handout directory ( hello-handout.tar in this example). Keeping all hard state in the src directory helps limit inconsistencies. The main makefile creates hello-handout by copying files from src , and then tars it up: hello/Makefile : # # Makefile to manage the example Hello Lab # # Get the name of the lab directory LAB = $( notdir $( PWD )) all : handout handout - tarfile handout : # Rebuild the handout directory that students download ( rm -rf $( LAB ) -handout ; mkdir $( LAB ) -handout ) cp -p src/Makefile-handout $( LAB ) -handout/Makefile cp -p src/README-handout $( LAB ) -handout/README cp -p src/hello.c-handout $( LAB ) -handout/hello.c cp -p src/driver.sh $( LAB ) -handout handout-tarfile : handout # Build *-handout.tar and autograde.tar tar cvf $( LAB ) -handout.tar $( LAB ) -handout cp -p $( LAB ) -handout.tar autograde.tar clean : # Clean the entire lab directory tree. Note that you can run # \"make clean; make\" at any time while the lab is live with no # adverse effects. rm -f *~ *.tar ( cd src ; make clean ) ( cd test-autograder ; make clean ) rm -rf $( LAB ) -handout rm -f autograde.tar # # CAREFULL!!! This will delete all student records in the logfile and # in the handin directory. Don't run this once the lab has started. # Use it to clean the directory when you are starting a new version # of the lab from scratch, or when you are debugging the lab prior # to releasing it to the students. # cleanallfiles : # Reset the lab from scratch. make clean rm -f log.txt rm -rf handin/* Filenames are disambiguated by appending -handout , which is stripped when they are copied to the handout directory. For example, src/hello.c is the instructor's solution file, and src/hello.c-handout is the starter code that is given to the students in hello-handout/hello.c . And src/README is the README for the src directory and src/README-handout is the README that is handed out to students in hello-handout/README . To build the lab, type make clean; make . You can do this as often as you like while the lab is live with no adverse effects. However, be careful to never type make cleanallfiles while the lab is live; this should only be done before the lab goes live; never during or after.","title":"Directory Structure"},{"location":"lab/#source-directory","text":"The hello/src/ directory contains all of the code files for the Hello Lab, including the files that are handed out to students: hello/src/README : # Autograder and solution files Makefile Makefile and ... README ... README for this directory driver.sh* Autograder hello.c Solution hello.c file # Files that are handed out to students Makefile-handout Makefile and ... README-handout ... README handed out to students hello.c-handout Blank hello.c file handed out to students","title":"Source Directory"},{"location":"lab/#handout-directory","text":"The hello/hello-handout/ directory contains the files that the students will use to work on the lab. It contains no hard state, and is populated entirely with files from hello/src : hello/hello-handout/README : For this lab, you should write a tiny C program, called \"hello.c\", that prints \"hello, world\" to stdout and then indicates success by exiting with a status of zero. To test your work: $ make clean; make; ./hello To run the same autograder that Autolab will use when you submit: $ ./driver.sh Files: README This file Makefile Compiles hello.c driver.sh Autolab autograder hello.c Empty C file that you will edit hello/hello-handout/Makefile contains the rules that compile the student source code: # Student makefile for the Hello Lab all : gcc hello.c -o hello clean : rm -rf *~ hello To compile and run their code, students type: $ make clean ; make $ ./hello","title":"Handout Directory"},{"location":"lab/#autograder","text":"The autograder for the Hello Lab is a trivially simple bash script called driver.sh that compiles and runs hello.c and verifies that it returns with an exit status of zero: hello/src/driver.sh : #!/bin/bash # driver.sh - The simplest autograder we could think of. It checks # that students can write a C program that compiles, and then # executes with an exit status of zero. # Usage: ./driver.sh # Compile the code echo \"Compiling hello.c\" ( make clean ; make ) status = $? if [ ${ status } -ne 0 ] ; then echo \"Failure: Unable to compile hello.c (return status = ${ status } )\" echo \"{\\\"scores\\\": {\\\"Correctness\\\": 0}}\" exit fi # Run the code echo \"Running ./hello\" ./hello status = $? if [ ${ status } -eq 0 ] ; then echo \"Success: ./hello runs with an exit status of 0\" echo \"{\\\"scores\\\": {\\\"Correctness\\\": 100}}\" else echo \"Failure: ./hello fails or returns nonzero exit status of ${ status } \" echo \"{\\\"scores\\\": {\\\"Correctness\\\": 0}}\" fi exit For example: $ ./driver.sh # Compiling hello.c # rm -rf *~ hello # gcc hello.c -o hello # Running ./hello # Hello, world # Success: ./hello runs with an exit status of 0 # {\"scores\": {\"Correctness\": 100}} Notice that the autograder expects the hello lab on the Autolab front-end to have been defined with a problem called \"Correctness\", with a maximum value of 100 points. If you forget to define the problems listed in the JSON autoresult, scores will still be logged, but they won't be posted to the database.","title":"Autograder"},{"location":"lab/#required-autograding-files","text":"Autolab requires two autograding files called autograde.tar , which contains all of the code required by the autograder, and autograde-Makefile , which runs the autograder on the autograding image when each submission is graded. For the Hello Lab, autograde.tar is simply a copy of the hello-handout.tar file that is handed out to students. And here is the corresponding hello/autograde-makefile : all : tar xvf autograde.tar cp hello.c hello-handout ( cd hello-handout ; ./driver.sh ) clean : rm -rf *~ hello-handout The makefile expands autograde.tar into hello-handout , copies hello.c (the submission file) into hello-handout , changes directory to hello-handout , builds the autograder, and then runs it.","title":"Required Autograding Files"},{"location":"lab/#test-directory","text":"For our labs, we like to setup a test directory (called test-autograder in this example), that allows us to test our autograde-Makefile and autograde-tar files by simulating Autolab's behavior on the autograding instance. The test-autograder directory has the following form: $ cd test-autograder $ ls -l # total 3 # lrwxr-xr-x 1 droh users 21 Aug 4 16:43 Makefile -> autograde-Makefile # lrwxr-xr-x 1 droh users 16 Aug 4 16:43 autograde.tar -> autograde.tar # -rw-rw-r-- 1 droh users 113 Aug 4 16:44 hello.c To simulate Autolab's behavior on an autograding instance: $ cd test-autograder && make clean && make # Running ./hello # Hello, world # Success: ./hello runs with an exit status of 0 # {\"scores\": {\"Correctness\": 100}}","title":"Test Directory"},{"location":"lab/#writeup-directory","text":"The hello/writeup contains the detailed lab writeup, either html or pdf file, that students can download from the Autolab front end.","title":"Writeup directory"},{"location":"lab/#other-sample-autograders","text":"We have a repository for sample autograders written for popular languages, which includes Python, Java, C++, Golang, and Javascript.","title":"Other sample autograders"},{"location":"lab/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"lab/#why-is-autolab-not-displaying-my-stdout-output","text":"Autolab always shows the stdout output of running make, even when the program crashed or timed out. However, when it does crash and the expected autoresult json string is not appended to the output, parsing of the last line will fail. If this happens, any stdout output that is longer than 10,000 lines will be discarded (Note that this limit does not apply when the autoresult json is valid).","title":"Why is Autolab not displaying my stdout output?"},{"location":"lab/#why-is-autolab-not-able-to-stream-my-stdout-output-the-output-only-seems-to-be-displayed-when-autograding-is-completed","text":"Autolab can only stream stdout. Many programming languages do buffered writes to stdout , so you would have to guarantee that you are writing to stdout by flushing the buffer accordingly (e.g. Python print 's flush flag , C's fflush , CPP's fflush )","title":"Why is Autolab not able to stream my stdout output? The output only seems to be displayed when autograding is completed."},{"location":"lab/#why-is-my-assessment-failing-to-be-imported","text":"","title":"Why is my assessment failing to be imported?"},{"location":"lab/#import-via-tar","text":"A valid assessment tar has a single root directory that's named after the assessment, containing an assessment yaml file with the same name as the root directory. See name rules below . An assessment config ruby file can optionally be provided, and the config file name also must have the same name as the root directory. The module name of the config file can be any name. The bare minimum for an assessment with a folder name of \"hello-asmt\" is shown below: hello-asmt/: hello-asmt.yml","title":"Import via Tar"},{"location":"lab/#import-via-file-system","text":"The same restrictions to assessment imports via tar apply to importing via file system.","title":"Import via file system"},{"location":"lab/#assessment-naming-rules","text":"Assessments on Autolab have two names: a \"name\" and a \"display name.\" The \"name\" is automatically created upon creation of a new assessment, based on the inputted \"display name.\" However, in the case of importing assessments from the file directory, or importing via tarball, the name of the folder containing the assessment files determines the assessment name. In addition, the assessment \"name\" is used to determine the name of the assessment config file. As a result, the assessment name must be a valid Ruby Identifier , but hyphens are also allowed. The assessment name must also be unique within a course.","title":"Assessment Naming Rules"},{"location":"lab/#valid-assessment-names","text":"autolabAsmt Autolab-Asmt Autolab_Asmt10 AuTol4b-_4smt","title":"Valid Assessment Names"},{"location":"lab/#invalid-assessment-names","text":"_autolab 4utolab autolabAsmt!! -autolabAsmt .autolab.asmt When a new assessment is created from scratch, a \"name\" will be generated based on the \"display name,\" as long as a valid \"name\" can be generated by stripping out special characters. Otherwise, the server will reject the given \"display name.\" When editing an existing assessment, or by specifying the display_name field in the course yml file, the \"display name\" has no restrictions on special characters.","title":"Invalid Assessment Names"},{"location":"reference/","text":"Reference Overview This section includes all the references for Autolab's Application Programming Interfaces (APIs) and is primarily intended for developers. Reference Description Autolab Frontend API API that allows developers to create clients that connect to Autolab's web server Tango API API that is used to manage jobs on Tango Tango VMMS API API for creating new tango interfaces with VMMS","title":"Reference Overview"},{"location":"reference/#reference-overview","text":"This section includes all the references for Autolab's Application Programming Interfaces (APIs) and is primarily intended for developers. Reference Description Autolab Frontend API API that allows developers to create clients that connect to Autolab's web server Tango API API that is used to manage jobs on Tango Tango VMMS API API for creating new tango interfaces with VMMS","title":"Reference Overview"},{"location":"tango-cli/","text":"Tango Command Line Client This is a guide to use the command-line client ( clients/tango-cli.py ) to test and collect other valuable information from Tango. Please setup Tango before moving forward. This guide assumes an instance of Tango is already up and running. Running a Sample Job The CLI supports two ways to run a sample job, individual steps or in a single all-in-one command . The first option is better for debugging each individual API call, whereas the second option is best for quickly running a job. Other Tango CLI commands are also discussed below . The Tango directory contains various different jobs in the clients/ directory; clients/README.md discusses the function of each job. Find out more information about the Tango REST API here . Single Command The --runJob command simply runs a job from a directory of files by uploading all the files in the directory. You can use this to submit an autograding job by running $ python clients/tango-cli.py -P 3000 -k test -l assessment1 --runJob clients/job1/ --image autograding_image The args are -P <port>, -k <key>, -l <unique_job_name> --runJob <job_files_path> --image <autograde_image> Individual Steps Open a courselab on Tango. This will create a directory for tango to store the files for the job. $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> --open Upload files necessary for the job. $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --upload --filename <clients/job1/hello.sh> $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --upload --filename <clients/job1/autograde-Makefile> Add the job to the queue. Note: localFile is the name of the file that was uploaded and destFile is the name of the file that will be on the VM. One of the destFile attributes must be Makefile . Furthermore, image references the name of the VM image you want the job to be run on. For Docker it is autograding_image . $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --addJob --infiles \\ '{\"localFile\" : \"hello.sh\", \"destFile\" : \"hello.sh\"}' \\ '{\"localFile\" : \"autograde-Makefile\", \"destFile\" : \"Makefile\"}' \\ --image <image> --outputFile <outputFileName> \\ --jobname <jobname> --maxsize <maxOutputSize> --timeout <jobTimeout> Get the job output. $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --poll --outputFile <outputFileName> The output file will have the following header: Autograder [ <date-time> ] : Received job <jobname>:<jobid> Autograder [ <date-time> ] : Success: Autodriver returned normally Autograder [ <date-time> ] : Here is the output from the autograder: Miscellaneous Commands The CLI also implements a list of commands to invoke the Tango REST API , including --info , --prealloc , and --jobs . For a full list of commands, run: python clients/tango-cli.py --help The general form for each command is as follows: python clients/tango-cli.py -P <port> -k <key> <command>","title":"Tango CLI"},{"location":"tango-cli/#tango-command-line-client","text":"This is a guide to use the command-line client ( clients/tango-cli.py ) to test and collect other valuable information from Tango. Please setup Tango before moving forward. This guide assumes an instance of Tango is already up and running.","title":"Tango Command Line Client"},{"location":"tango-cli/#running-a-sample-job","text":"The CLI supports two ways to run a sample job, individual steps or in a single all-in-one command . The first option is better for debugging each individual API call, whereas the second option is best for quickly running a job. Other Tango CLI commands are also discussed below . The Tango directory contains various different jobs in the clients/ directory; clients/README.md discusses the function of each job. Find out more information about the Tango REST API here .","title":"Running a Sample Job"},{"location":"tango-cli/#single-command","text":"The --runJob command simply runs a job from a directory of files by uploading all the files in the directory. You can use this to submit an autograding job by running $ python clients/tango-cli.py -P 3000 -k test -l assessment1 --runJob clients/job1/ --image autograding_image The args are -P <port>, -k <key>, -l <unique_job_name> --runJob <job_files_path> --image <autograde_image>","title":"Single Command"},{"location":"tango-cli/#individual-steps","text":"Open a courselab on Tango. This will create a directory for tango to store the files for the job. $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> --open Upload files necessary for the job. $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --upload --filename <clients/job1/hello.sh> $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --upload --filename <clients/job1/autograde-Makefile> Add the job to the queue. Note: localFile is the name of the file that was uploaded and destFile is the name of the file that will be on the VM. One of the destFile attributes must be Makefile . Furthermore, image references the name of the VM image you want the job to be run on. For Docker it is autograding_image . $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --addJob --infiles \\ '{\"localFile\" : \"hello.sh\", \"destFile\" : \"hello.sh\"}' \\ '{\"localFile\" : \"autograde-Makefile\", \"destFile\" : \"Makefile\"}' \\ --image <image> --outputFile <outputFileName> \\ --jobname <jobname> --maxsize <maxOutputSize> --timeout <jobTimeout> Get the job output. $ python clients/tango-cli.py -P <port> -k <key> -l <courselab> \\ --poll --outputFile <outputFileName> The output file will have the following header: Autograder [ <date-time> ] : Received job <jobname>:<jobid> Autograder [ <date-time> ] : Success: Autodriver returned normally Autograder [ <date-time> ] : Here is the output from the autograder:","title":"Individual Steps"},{"location":"tango-cli/#miscellaneous-commands","text":"The CLI also implements a list of commands to invoke the Tango REST API , including --info , --prealloc , and --jobs . For a full list of commands, run: python clients/tango-cli.py --help The general form for each command is as follows: python clients/tango-cli.py -P <port> -k <key> <command>","title":"Miscellaneous Commands"},{"location":"tango-rest/","text":"This page documents the REST API for submitting jobs to Tango. Authentication In order to have access to the REST interface of the Tango server, clients will first have to obtain a key from the Tango server. This key is a unique identifier of the client and it must be supplied with every HTTP request to the Tango server. If the Tango server fails to recognize the key, it does not entertain the request and returns an error message as part of the response body. Job Requests Here is a description of the requests that clients use to submit jobs: open A request to open consists of the client's key and an identifier for every lab, which is likely to be a combination of the course name and the lab name (i.e. courselab for autograding jobs). open checks if a directory for courselab exists. If a directory for courselab exists, a dict of MD5 hashes corresponding to every file in that directory is returned. If the directory does not exist, it is created and a folder for output files is also created within the courselab directory. Since no files exist in the newly created directory, an empty dict of MD5 hashes is returned. Request header: GET /open/key/courselab/ Request body: empty Response body: { \"statusMsg\" : <s tr i n g> , \"statusId\" : <i nt > , \"files\" : { < f ileName 1 > : <md 5 hash 1 > , < f ileName 2 > : <md 5 hash 2 > ... }, } upload After receiving a list of MD5 hashes of files that exist on the Tango server, the client can choose to upload files that are different from the ones on the Tango server via successive upload commands. For each upload, the client must supply a filename header that gives the name of the file (on the local machine) to be uploaded to Tango. One of these files must be a Makefile, which needs to contain a rule called autograde (command to drive the autograding process). Request header: POST /upload/key/courselab/ Request body: <file> Response body: { \"statusMsg\" : <s tr i n g> , \"statusId\" : <i nt > } addJob After uploading the appropriate files, the client uses this command to run the job for the files specified as files in the courselab and on an instance of a particular VM image . Each file has localFile and destFile attributes which specify what the file is called on the Tango server and what it should be called when copied over to a VM (for autograding) respectively. Exactly one of the specified files should have the destFile attribute set to Makefile , and the Makefile must contain a rule called autograde . Clients can also specify an optional timeout value ( timeout ) and maximum output file size ( max_kb ). This command is non-blocking and returns immediately with a status message. Additionally, the command accepts an optional parameter, callback_url . If the callback_url is specified, then the Tango server sends a POST request to the callback_url with the output file once the job is terminated. If the callback_url is not specified, the client can then send a poll request for the output_file to check the status of that job and retrieve the output file from the Tango server if autograding is complete. Request header: POST /addJob/key/courselab/ Request body: { \"image\" : <s tr i n g> , # required VM image (e.g. \"rhel.img\" ) \"files\" : [ { \"localFile\" : <s tr i n g> , \"destFile\" : <s tr i n g> }, ... ], # required lis t o f f iles t o be used f or au t ogradi n g \"jobName\" : <s tr i n g> , # required na me o f job \"output_file\" : <s tr i n g> , # required na me o f ou t pu t f ile \"timeout\" : <i nt > , # op t io nal t imeou t value (secs) \"max_kb\" : <i nt > , # op t io nal max ou t pu t f ile size (KB) \"callback_url\" : <s tr i n g> # op t io nal URL f or POST callback fr om server t o clie nt } Response body: { \"statusMsg\" : <s tr i n g> , \"statusId\" : <i nt > , \"jobId\" : <i nt > } poll Check if the job for outputFile has completed. If not, return 404: Not Found and a status message, otherwise return the file in the response body, and free all resources held by the job. Request header: GET /poll/key/courselab/outputFile/ Request body: { <emp t y> } Response body: <autograder output file> if autograding successful otherwise: { \"statusMsg\" : <s tr i n g> , \"statusId\" : <i nt > } Administrative Requests Here are the requests that administrators use to manage the Tango service, typically from a command line client. /info This is the \"hello, world\" request for the service. It returns a JSON object with some basic stats about the service, such as uptime, number of jobs, etc. Request header: GET /info/<KEY>/ Request body: { <emp t y> } Response body: { \"info\" : { \"num_threads\" : <i nt > , \"job_requests\" : <i nt > , \"waitvm_timeouts\" : <i nt > , \"runjob_timeouts\" : <i nt > , \"elapsed_secs\" : < fl oa t > , \"runjob_errors\" : <i nt > , \"job_retries\" : <i nt > , \"copyin_errors\" : <i nt > , \"copyout_errors\" : <i nt > }, \"statusMsg\" : \"Found info successfully\" , \"statusId\" : 0 } /jobs Return a list of jobs. If deadjobs is set to 1, then return a list of recently completed jobs. Otherwise, return the list of currently running jobs. Note: This isn't strictly an admin request, since clients might find it useful to display jobs status, as we do in the Autolab front end. Request header: POST autograde.me/jobs/key/deadjobs/ Request body: empty Response body: JSON jobs object pool Returns a JSON object that provides info about the current state of a pool of instances spawned from some image . The response gives the total number of instances in the pool, and the number of free instances not currently allocated to any job. Request header: GET /pool/key/image/ Response body: JSON pool object prealloc Creates a pool of num identical instances spawned from image (e.g. \"rhel.img). Request header: POST /prealloc/key/image/num/ Request body: { \"vmms\" : <s tr i n g> , # vmms t o use (e.g. \"localSSH\" ) \"cores\" : <i nt > , # nu mber o f cores per VM \"memory\" : <i nt > , # amou nt o f memory per VM } Response body: { \"status\": <string> } Implementation Notes Tango will maintain a directory for each of the labs in a course, which is created by open . All output files are stored within a specified output folder in this directory. Besides the runtime job queue, no other state is necessary. At job execution time, Tango will copy files specified by the files parameter in addJob to the VM. When the VM finishes, it will copy the output file back to the lab directory.","title":"Tango API"},{"location":"tango-rest/#authentication","text":"In order to have access to the REST interface of the Tango server, clients will first have to obtain a key from the Tango server. This key is a unique identifier of the client and it must be supplied with every HTTP request to the Tango server. If the Tango server fails to recognize the key, it does not entertain the request and returns an error message as part of the response body.","title":"Authentication"},{"location":"tango-rest/#job-requests","text":"Here is a description of the requests that clients use to submit jobs:","title":"Job Requests"},{"location":"tango-rest/#open","text":"A request to open consists of the client's key and an identifier for every lab, which is likely to be a combination of the course name and the lab name (i.e. courselab for autograding jobs). open checks if a directory for courselab exists. If a directory for courselab exists, a dict of MD5 hashes corresponding to every file in that directory is returned. If the directory does not exist, it is created and a folder for output files is also created within the courselab directory. Since no files exist in the newly created directory, an empty dict of MD5 hashes is returned. Request header: GET /open/key/courselab/ Request body: empty Response body: { \"statusMsg\" : <s tr i n g> , \"statusId\" : <i nt > , \"files\" : { < f ileName 1 > : <md 5 hash 1 > , < f ileName 2 > : <md 5 hash 2 > ... }, }","title":"open"},{"location":"tango-rest/#upload","text":"After receiving a list of MD5 hashes of files that exist on the Tango server, the client can choose to upload files that are different from the ones on the Tango server via successive upload commands. For each upload, the client must supply a filename header that gives the name of the file (on the local machine) to be uploaded to Tango. One of these files must be a Makefile, which needs to contain a rule called autograde (command to drive the autograding process). Request header: POST /upload/key/courselab/ Request body: <file> Response body: { \"statusMsg\" : <s tr i n g> , \"statusId\" : <i nt > }","title":"upload"},{"location":"tango-rest/#addjob","text":"After uploading the appropriate files, the client uses this command to run the job for the files specified as files in the courselab and on an instance of a particular VM image . Each file has localFile and destFile attributes which specify what the file is called on the Tango server and what it should be called when copied over to a VM (for autograding) respectively. Exactly one of the specified files should have the destFile attribute set to Makefile , and the Makefile must contain a rule called autograde . Clients can also specify an optional timeout value ( timeout ) and maximum output file size ( max_kb ). This command is non-blocking and returns immediately with a status message. Additionally, the command accepts an optional parameter, callback_url . If the callback_url is specified, then the Tango server sends a POST request to the callback_url with the output file once the job is terminated. If the callback_url is not specified, the client can then send a poll request for the output_file to check the status of that job and retrieve the output file from the Tango server if autograding is complete. Request header: POST /addJob/key/courselab/ Request body: { \"image\" : <s tr i n g> , # required VM image (e.g. \"rhel.img\" ) \"files\" : [ { \"localFile\" : <s tr i n g> , \"destFile\" : <s tr i n g> }, ... ], # required lis t o f f iles t o be used f or au t ogradi n g \"jobName\" : <s tr i n g> , # required na me o f job \"output_file\" : <s tr i n g> , # required na me o f ou t pu t f ile \"timeout\" : <i nt > , # op t io nal t imeou t value (secs) \"max_kb\" : <i nt > , # op t io nal max ou t pu t f ile size (KB) \"callback_url\" : <s tr i n g> # op t io nal URL f or POST callback fr om server t o clie nt } Response body: { \"statusMsg\" : <s tr i n g> , \"statusId\" : <i nt > , \"jobId\" : <i nt > }","title":"addJob"},{"location":"tango-rest/#poll","text":"Check if the job for outputFile has completed. If not, return 404: Not Found and a status message, otherwise return the file in the response body, and free all resources held by the job. Request header: GET /poll/key/courselab/outputFile/ Request body: { <emp t y> } Response body: <autograder output file> if autograding successful otherwise: { \"statusMsg\" : <s tr i n g> , \"statusId\" : <i nt > }","title":"poll"},{"location":"tango-rest/#administrative-requests","text":"Here are the requests that administrators use to manage the Tango service, typically from a command line client.","title":"Administrative Requests"},{"location":"tango-rest/#info","text":"This is the \"hello, world\" request for the service. It returns a JSON object with some basic stats about the service, such as uptime, number of jobs, etc. Request header: GET /info/<KEY>/ Request body: { <emp t y> } Response body: { \"info\" : { \"num_threads\" : <i nt > , \"job_requests\" : <i nt > , \"waitvm_timeouts\" : <i nt > , \"runjob_timeouts\" : <i nt > , \"elapsed_secs\" : < fl oa t > , \"runjob_errors\" : <i nt > , \"job_retries\" : <i nt > , \"copyin_errors\" : <i nt > , \"copyout_errors\" : <i nt > }, \"statusMsg\" : \"Found info successfully\" , \"statusId\" : 0 }","title":"/info"},{"location":"tango-rest/#jobs","text":"Return a list of jobs. If deadjobs is set to 1, then return a list of recently completed jobs. Otherwise, return the list of currently running jobs. Note: This isn't strictly an admin request, since clients might find it useful to display jobs status, as we do in the Autolab front end. Request header: POST autograde.me/jobs/key/deadjobs/ Request body: empty Response body: JSON jobs object","title":"/jobs"},{"location":"tango-rest/#pool","text":"Returns a JSON object that provides info about the current state of a pool of instances spawned from some image . The response gives the total number of instances in the pool, and the number of free instances not currently allocated to any job. Request header: GET /pool/key/image/ Response body: JSON pool object","title":"pool"},{"location":"tango-rest/#prealloc","text":"Creates a pool of num identical instances spawned from image (e.g. \"rhel.img). Request header: POST /prealloc/key/image/num/ Request body: { \"vmms\" : <s tr i n g> , # vmms t o use (e.g. \"localSSH\" ) \"cores\" : <i nt > , # nu mber o f cores per VM \"memory\" : <i nt > , # amou nt o f memory per VM } Response body: { \"status\": <string> }","title":"prealloc"},{"location":"tango-rest/#implementation-notes","text":"Tango will maintain a directory for each of the labs in a course, which is created by open . All output files are stored within a specified output folder in this directory. Besides the runtime job queue, no other state is necessary. At job execution time, Tango will copy files specified by the files parameter in addJob to the VM. When the VM finishes, it will copy the output file back to the lab directory.","title":"Implementation Notes"},{"location":"tango-vmms/","text":"This page documents the interface for Tango's Virtual Machine Management Systems' (VMMSs) API and instructions for setting up VMMSs. See the vmms directory in Tango for example implementations. We currently provide Docker VMMS and the Amazon EC2 VMMS interfaces. The following API is for developers who intend to use a different kind of VMMS for Tango's autograding purposes. API The functions necessary to implement the API are documented here. Note that for certain implementations, some of these methods will be no-ops since the VMMS doesn't require any particular instructions to perform the specified actions. Furthermore, throughout this document, we use the term \"VM\" liberally to represent any container-like object on which Tango jobs may be run. initializeVM initializeVM ( self , vm ) Creates a new VM instance for the VMMS based on the fields of vm , which is a TangoMachine object defined in tangoObjects.py . waitVM waitVM ( self , vm , max_secs ) Waits at most max_secs for a VM to be ready to run jobs. Returns an error if the VM is not ready after max_secs . copyIn copyIn ( self , vm , inputFiles ) Copies the input files for a job into the VM. inputFiles is a list of InputFile objects defined in tangoObjects.py . For each InputFile object, file.localFile is the name of the file on the Tango host machine and file.destFile is what the name of the file should be on the VM. runJob runJob ( self , vm , runTimeout , maxOutputFileSize ) Runs the autodriver binary on the VM. The autodriver runs make on the VM (which in turn runs the job via the Makefile that was provided as a part of the input files for the job). The output from the autodriver most likely should be redirected to some feedback file to be used in the next method of the API. copyOut copyOut ( self , vm , destFile ) Copies the output file for the job out of the VM into destFile on the Tango host machine. destroyVM destroyVM ( self , vm ) Removes a VM from the Tango system. safeDestroyVM safeDestroyVM ( self , vm ) Removes a VM from the Tango system and makes sure that it has been removed. getVMs getVMs ( self ) Returns a complete list of VMs associated with this Tango system.","title":"Tango VMMS API"},{"location":"tango-vmms/#api","text":"The functions necessary to implement the API are documented here. Note that for certain implementations, some of these methods will be no-ops since the VMMS doesn't require any particular instructions to perform the specified actions. Furthermore, throughout this document, we use the term \"VM\" liberally to represent any container-like object on which Tango jobs may be run.","title":"API"},{"location":"tango-vmms/#initializevm","text":"initializeVM ( self , vm ) Creates a new VM instance for the VMMS based on the fields of vm , which is a TangoMachine object defined in tangoObjects.py .","title":"initializeVM"},{"location":"tango-vmms/#waitvm","text":"waitVM ( self , vm , max_secs ) Waits at most max_secs for a VM to be ready to run jobs. Returns an error if the VM is not ready after max_secs .","title":"waitVM"},{"location":"tango-vmms/#copyin","text":"copyIn ( self , vm , inputFiles ) Copies the input files for a job into the VM. inputFiles is a list of InputFile objects defined in tangoObjects.py . For each InputFile object, file.localFile is the name of the file on the Tango host machine and file.destFile is what the name of the file should be on the VM.","title":"copyIn"},{"location":"tango-vmms/#runjob","text":"runJob ( self , vm , runTimeout , maxOutputFileSize ) Runs the autodriver binary on the VM. The autodriver runs make on the VM (which in turn runs the job via the Makefile that was provided as a part of the input files for the job). The output from the autodriver most likely should be redirected to some feedback file to be used in the next method of the API.","title":"runJob"},{"location":"tango-vmms/#copyout","text":"copyOut ( self , vm , destFile ) Copies the output file for the job out of the VM into destFile on the Tango host machine.","title":"copyOut"},{"location":"tango-vmms/#destroyvm","text":"destroyVM ( self , vm ) Removes a VM from the Tango system.","title":"destroyVM"},{"location":"tango-vmms/#safedestroyvm","text":"safeDestroyVM ( self , vm ) Removes a VM from the Tango system and makes sure that it has been removed.","title":"safeDestroyVM"},{"location":"tango-vmms/#getvms","text":"getVMs ( self ) Returns a complete list of VMs associated with this Tango system.","title":"getVMs"},{"location":"features/admin-features/","text":"Admin Features Features that only admins have access to. Reset User's Password An admin is able to trigger the reset of the student's password by selecting Manage Users from the Manage Autolab dropdown if a student does not have access to a registered email address and forgets their password. The admin can click on a student from the generated Users List and click on Change User's Password to reset the student's password. The admin can either choose to change the student's password on behalf of the student by clicking here in the flash that appeared or sending the link that was generated in the flash to the student so that the student can change their password.","title":"Admin Features"},{"location":"features/admin-features/#admin-features","text":"Features that only admins have access to.","title":"Admin Features"},{"location":"features/admin-features/#reset-users-password","text":"An admin is able to trigger the reset of the student's password by selecting Manage Users from the Manage Autolab dropdown if a student does not have access to a registered email address and forgets their password. The admin can click on a student from the generated Users List and click on Change User's Password to reset the student's password. The admin can either choose to change the student's password on behalf of the student by clicking here in the flash that appeared or sending the link that was generated in the flash to the student so that the student can change their password.","title":"Reset User's Password"},{"location":"features/annotations/","text":"Annotations Annotations is a feature introduced as part of the Speedgrader update to Autolab. It allows instructors and TAs to quickly leave comments and grade code at the same time. Non-Autograded Problems Only Note that annotations can only be added to non-autograded problems. Specifically, a problem is non-autograded if there is no assigned score for that problem in the json outputted by the autograder Annotation types Line-based annotations Hover over any line of the code and click on the green arrow, and the annotation form will appear. Add the comment, adjust the score, and select the targeted problem. When the \"Add to shared comment pool\" option is checked, the comment will be saved to a shared comment pool. When creating new annotations, shared comments matching what you have typed will be available via a dropdown list. Only the 50 most recently added shared comments will be displayed. Note that the shared comment pool operates on a per-problem basis, so a shared comment for one problem will not be available when creating an annotation for another problem within the same assessment. Additionally, the same shared comment pool is used by all instructors and course-assistants, so your shared comments will be available to other instructors and course-assistants, and vice-versa. Deleting annotations Note that shared comments are tied to the original annotation. If the original annotation is deleted, the comment will not persist in the shared comment pool. Global annotations You can can also add a global annotation to a problem that is not tied to a specific line of code. To do so, click on the \"+\" header button corresponding to the problem. Scoring Behavior There are two intended ways for course instructors to use the add annotation features. Deductions from maximum (\"negative grading\"), or additions to zero (\"positive grading\"). The default setting is negative grading, but positive grading can be enabled under Edit Assessment > Problems . Negative Grading Set a max_score either programmatically, or under Edit Assessment > Problems for the particular non-autograded question. Then when the grader is viewing the code, add a negative score, such as -5 into the score field, to deduct from the maximum. This use case is preferred when grading based on a rubric, and the score is deducted for each mistake. The maximum score can be 0 if the deductions are meant to be penalties, such as for poor code style or violation of library interfaces. Positive Grading Set a max_score either programmatically, or under Edit Assessment > Problems for the particular non-autograded question. Then when the grader is viewing the code, add a positive score, such as 5 to the score field, to add to the score. This use case is preferred when giving out bonus points. Interaction with Gradesheet We have kept the ability the edit the scores in the gradesheet, as we understand that there are instances in which editing the gradesheet directly is much more efficient and/or needed. However, this leads to an unintended interaction with the annotations. In particular, modifications on the gradesheet itself will override all changes made to a problem by annotations, but the annotations made will still remain. A example would be, if the max_score of a problem is 10 . A grader adds an annotation with -5 score to that problem (so the score is now 10-5=5 ). Then if the same/another grader changes the score to 8 on the gradesheet, the final score would be 8 . Recommendation It is much preferred to grade using annotations whenever possible, as it provides a better experience for the students who will be able to identify the exact line at which the mistake is made. Gradesheet should be used in situations where the modification is non-code related.","title":"Annotations"},{"location":"features/annotations/#annotations","text":"Annotations is a feature introduced as part of the Speedgrader update to Autolab. It allows instructors and TAs to quickly leave comments and grade code at the same time. Non-Autograded Problems Only Note that annotations can only be added to non-autograded problems. Specifically, a problem is non-autograded if there is no assigned score for that problem in the json outputted by the autograder","title":"Annotations"},{"location":"features/annotations/#annotation-types","text":"","title":"Annotation types"},{"location":"features/annotations/#line-based-annotations","text":"Hover over any line of the code and click on the green arrow, and the annotation form will appear. Add the comment, adjust the score, and select the targeted problem. When the \"Add to shared comment pool\" option is checked, the comment will be saved to a shared comment pool. When creating new annotations, shared comments matching what you have typed will be available via a dropdown list. Only the 50 most recently added shared comments will be displayed. Note that the shared comment pool operates on a per-problem basis, so a shared comment for one problem will not be available when creating an annotation for another problem within the same assessment. Additionally, the same shared comment pool is used by all instructors and course-assistants, so your shared comments will be available to other instructors and course-assistants, and vice-versa. Deleting annotations Note that shared comments are tied to the original annotation. If the original annotation is deleted, the comment will not persist in the shared comment pool.","title":"Line-based annotations"},{"location":"features/annotations/#global-annotations","text":"You can can also add a global annotation to a problem that is not tied to a specific line of code. To do so, click on the \"+\" header button corresponding to the problem.","title":"Global annotations"},{"location":"features/annotations/#scoring-behavior","text":"There are two intended ways for course instructors to use the add annotation features. Deductions from maximum (\"negative grading\"), or additions to zero (\"positive grading\"). The default setting is negative grading, but positive grading can be enabled under Edit Assessment > Problems .","title":"Scoring Behavior"},{"location":"features/annotations/#negative-grading","text":"Set a max_score either programmatically, or under Edit Assessment > Problems for the particular non-autograded question. Then when the grader is viewing the code, add a negative score, such as -5 into the score field, to deduct from the maximum. This use case is preferred when grading based on a rubric, and the score is deducted for each mistake. The maximum score can be 0 if the deductions are meant to be penalties, such as for poor code style or violation of library interfaces.","title":"Negative Grading"},{"location":"features/annotations/#positive-grading","text":"Set a max_score either programmatically, or under Edit Assessment > Problems for the particular non-autograded question. Then when the grader is viewing the code, add a positive score, such as 5 to the score field, to add to the score. This use case is preferred when giving out bonus points.","title":"Positive Grading"},{"location":"features/annotations/#interaction-with-gradesheet","text":"We have kept the ability the edit the scores in the gradesheet, as we understand that there are instances in which editing the gradesheet directly is much more efficient and/or needed. However, this leads to an unintended interaction with the annotations. In particular, modifications on the gradesheet itself will override all changes made to a problem by annotations, but the annotations made will still remain. A example would be, if the max_score of a problem is 10 . A grader adds an annotation with -5 score to that problem (so the score is now 10-5=5 ). Then if the same/another grader changes the score to 8 on the gradesheet, the final score would be 8 . Recommendation It is much preferred to grade using annotations whenever possible, as it provides a better experience for the students who will be able to identify the exact line at which the mistake is made. Gradesheet should be used in situations where the modification is non-code related.","title":"Interaction with Gradesheet"},{"location":"features/embedded-forms/","text":"Embedded Forms This feature allows an instructor to create an assessment which does not require a file submission on the part of the student. Instead, when an assessment is created, the hand-in page for that assessment will display an HTML form of the instructor\u2019s design. When the student submits the form, the information is sent directly in JSON format to the Tango grading server for evaluation. Tango Required Tango is needed to use this feature. Please install Tango and connect it to Autolab before proceeding. Creating an Embedded Form Create an HTML file with a combination of the following elements. The HTML file need only include form elements, because it will automatically be wrapped in a <form></form> block when it is rendered on the page. In order for the JSON string (the information passed to the grader) to be constructed properly, your form elements must follow the following conventions: A unique name attribute A value attribute which corresponds to the correct answer to the question (unless it is a text field or text area) HTML Form Reference: Text Field (For short responses) < input type = \"text\" name = \"question-1\" /> Text Area (For coding questions) < textarea name = \"question-2\" style = \"width:100%\" /> Radio Button (For multiple choice) < div class = \"row\" > < label > < input name = \"question-3\" type = \"radio\" value = \"object\" id = \"q3-1\" /> < span > Object </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"boolean\" id = \"q3-2\" /> < span > Boolean </ span > </ label > </ div > Dropdown (For multiple choice or select all that apply) < select multiple name = \"question-4\" > < option value = \"1\" > Option 1 </ option > < option value = \"2\" > Option 2 </ option > < option value = \"3\" > Option 3 </ option > </ select > Example Form (shown in screenshot above) < div > < h6 > What's your name? </ h6 > < input type = \"text\" name = \"question-1\" id = \"q1\" /> </ div > < div > < h6 > Which year are you? </ h6 > < div class = \"row\" > < label > < input name = \"question-2\" type = \"radio\" value = \"freshman\" id = \"q3-1\" /> < span > Freshman </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"sophomore\" id = \"q3-2\" /> < span > Sophomore </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"junior\" id = \"q3-3\" /> < span > Junior </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"senior\" id = \"q3-4\" /> < span > Senior </ span > </ label > </ div > </ div > < div > < h6 > What's your favorite language? </ h6 > < select name = \"question-3\" id = \"q4\" > < option value = \"C\" > C </ option > < option value = \"Python\" > Python </ option > < option value = \"Java\" > Java </ option > </ select > </ div > Navigate to the Basic section of editing an assessment ( /courses/<course>/assessments/<assessment>/edit ), check the check box, and upload the HTML file. Ensure you submit the form by clicking Save at the bottom of the page. Grading an Embedded Form When a student submits a form, the form data is sent to Tango in the form of a JSON string in the file out.txt. In your grading script, parse the contents of out.txt as a JSON object. The JSON object will be a key-value pair data structure, so you can access the students response string ( value ) by its unique key (the name attribute). For the example form shown above, the JSON object will be as follows: { \"utf8\" : \"\u2713\" , \"authenticity_token\" : \"LONGAUTHTOKEN\" , \"submission[embedded_quiz_form_answer]\" : \"\" , \"question-1\" : \"John Smith\" , \"question-2\" : \"junior\" , \"question-3\" : \"Python\" , \"integrity_checkbox\" : \"1\" } Use this information to do any processing you need in Tango.If you find any problems, please file an issue on the Autolab Github .","title":"Embedded Forms"},{"location":"features/embedded-forms/#embedded-forms","text":"This feature allows an instructor to create an assessment which does not require a file submission on the part of the student. Instead, when an assessment is created, the hand-in page for that assessment will display an HTML form of the instructor\u2019s design. When the student submits the form, the information is sent directly in JSON format to the Tango grading server for evaluation. Tango Required Tango is needed to use this feature. Please install Tango and connect it to Autolab before proceeding.","title":"Embedded Forms"},{"location":"features/embedded-forms/#creating-an-embedded-form","text":"Create an HTML file with a combination of the following elements. The HTML file need only include form elements, because it will automatically be wrapped in a <form></form> block when it is rendered on the page. In order for the JSON string (the information passed to the grader) to be constructed properly, your form elements must follow the following conventions: A unique name attribute A value attribute which corresponds to the correct answer to the question (unless it is a text field or text area) HTML Form Reference: Text Field (For short responses) < input type = \"text\" name = \"question-1\" /> Text Area (For coding questions) < textarea name = \"question-2\" style = \"width:100%\" /> Radio Button (For multiple choice) < div class = \"row\" > < label > < input name = \"question-3\" type = \"radio\" value = \"object\" id = \"q3-1\" /> < span > Object </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"boolean\" id = \"q3-2\" /> < span > Boolean </ span > </ label > </ div > Dropdown (For multiple choice or select all that apply) < select multiple name = \"question-4\" > < option value = \"1\" > Option 1 </ option > < option value = \"2\" > Option 2 </ option > < option value = \"3\" > Option 3 </ option > </ select > Example Form (shown in screenshot above) < div > < h6 > What's your name? </ h6 > < input type = \"text\" name = \"question-1\" id = \"q1\" /> </ div > < div > < h6 > Which year are you? </ h6 > < div class = \"row\" > < label > < input name = \"question-2\" type = \"radio\" value = \"freshman\" id = \"q3-1\" /> < span > Freshman </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"sophomore\" id = \"q3-2\" /> < span > Sophomore </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"junior\" id = \"q3-3\" /> < span > Junior </ span > </ label > < label > < input name = \"question-2\" type = \"radio\" value = \"senior\" id = \"q3-4\" /> < span > Senior </ span > </ label > </ div > </ div > < div > < h6 > What's your favorite language? </ h6 > < select name = \"question-3\" id = \"q4\" > < option value = \"C\" > C </ option > < option value = \"Python\" > Python </ option > < option value = \"Java\" > Java </ option > </ select > </ div > Navigate to the Basic section of editing an assessment ( /courses/<course>/assessments/<assessment>/edit ), check the check box, and upload the HTML file. Ensure you submit the form by clicking Save at the bottom of the page.","title":"Creating an Embedded Form"},{"location":"features/embedded-forms/#grading-an-embedded-form","text":"When a student submits a form, the form data is sent to Tango in the form of a JSON string in the file out.txt. In your grading script, parse the contents of out.txt as a JSON object. The JSON object will be a key-value pair data structure, so you can access the students response string ( value ) by its unique key (the name attribute). For the example form shown above, the JSON object will be as follows: { \"utf8\" : \"\u2713\" , \"authenticity_token\" : \"LONGAUTHTOKEN\" , \"submission[embedded_quiz_form_answer]\" : \"\" , \"question-1\" : \"John Smith\" , \"question-2\" : \"junior\" , \"question-3\" : \"Python\" , \"integrity_checkbox\" : \"1\" } Use this information to do any processing you need in Tango.If you find any problems, please file an issue on the Autolab Github .","title":"Grading an Embedded Form"},{"location":"features/formatted-feedback/","text":"Formatted Feedback Autograding feedback plays an important role in a student's Autolab experience. Good feedback provided by autograders can really enhance a student's learning. As of Summer 2020, Autolab includes the formatted feedback feature by Jala Alamin . The feature was originally introduced in Washington State University Vancouver's version of Autolab. Using formatted feedback requires a prior understanding of how Autolab's autograders work, as per the Guide for Lab Authors . The formatted feedback feature is an optional extension of the default feedback. It comes in a staged fashion, allowing differing levels of adoption. The next few sections are meant to be read in order, with each following section introducing a more complex usage of the formatted feedback feature than the previous. Experimenting with the hellocat example code is another way to familiarize with the formatted feedback. Default Feedback By only outputting the autoresult ( autoresult is the JSON string that needs to be outputted on the last line of stdout, as mentioned in the Guide for Lab Authors ), the default feedback format will automatically be used. { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 } } Autolab will simply display the raw output as produced by the autograder Semantic Feedback (Minimal) By adding an additional JSON string before the autoresult , as follows { \"_presentation\" : \"semantic\" } { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 } } we can invoke the semantic layout, which will display the both the raw output and a formatted table of scores. Semantic Feedback with Test Cases By further describing the additional JSON string, we can introduce test stages to the formatted feedback, which we can use to indicate to the student the test cases that have passed and/or failed. Actual JSON to be outputted { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Test Results\" ], \"Test Results\" : { \"Build\" : { \"passed\" : true }, \"Run\" : { \"passed\" : true }}} { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} Prettified JSON (for reference only) { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Test Results\" ], \"Test Results\" : { \"Build\" : { \"passed\" : true }, \"Run\" : { \"passed\" : true } } } { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} We would add [\"Test Results\"] to the stages key. Then we would add to the corresponding Test Results key an object containing all the test case results. In this case Build and Run were used, but you can use other names for the test cases as well. Semantic Feedback (Multi-Stage) Using the same manner in which we add a Test Stage in the previous section, we can adapt it to create as many stages as we want. The following example has three different stages, namely Build , Test and Timing , but you can use other names for the stages as well. Actual JSON to be outputted { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Build\" , \"Test\" , \"Timing\" ], \"Test\" : { \"Add Things\" : { \"passed\" : true }, \"Return Values\" : { \"passed\" : false , \"hint\" : \"You need to return 1\" }}, \"Build\" : { \"compile\" : { \"passed\" : true }, \"link\" : { \"passed\" : true }}, \"Timing\" : { \"Stage 1 (ms)\" : 10 , \"Stage 2 (ms)\" : 20 }} { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} Prettified JSON (for reference only) { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Build\" , \"Test\" , \"Timing\" ], \"Test\" : { \"Add Things\" : { \"passed\" : true }, \"Return Values\" : { \"passed\" : false , \"hint\" : \"You need to return 1\" } }, \"Build\" : { \"compile\" : { \"passed\" : true }, \"link\" : { \"passed\" : true } }, \"Timing\" : { \"Stage 1 (ms)\" : 10 , \"Stage 2 (ms)\" : 20 } } { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} We would add the stages we want into the stages array. Then we would add those corresponding stages as a separate key, with each of them holding their own set of test case results. We are also able to provide hints if the student gets the particular test case wrong by adding a hint key to the test case.","title":"Formatted Feedback"},{"location":"features/formatted-feedback/#formatted-feedback","text":"Autograding feedback plays an important role in a student's Autolab experience. Good feedback provided by autograders can really enhance a student's learning. As of Summer 2020, Autolab includes the formatted feedback feature by Jala Alamin . The feature was originally introduced in Washington State University Vancouver's version of Autolab. Using formatted feedback requires a prior understanding of how Autolab's autograders work, as per the Guide for Lab Authors . The formatted feedback feature is an optional extension of the default feedback. It comes in a staged fashion, allowing differing levels of adoption. The next few sections are meant to be read in order, with each following section introducing a more complex usage of the formatted feedback feature than the previous. Experimenting with the hellocat example code is another way to familiarize with the formatted feedback.","title":"Formatted Feedback"},{"location":"features/formatted-feedback/#default-feedback","text":"By only outputting the autoresult ( autoresult is the JSON string that needs to be outputted on the last line of stdout, as mentioned in the Guide for Lab Authors ), the default feedback format will automatically be used. { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 } } Autolab will simply display the raw output as produced by the autograder","title":"Default Feedback"},{"location":"features/formatted-feedback/#semantic-feedback-minimal","text":"By adding an additional JSON string before the autoresult , as follows { \"_presentation\" : \"semantic\" } { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 } } we can invoke the semantic layout, which will display the both the raw output and a formatted table of scores.","title":"Semantic Feedback (Minimal)"},{"location":"features/formatted-feedback/#semantic-feedback-with-test-cases","text":"By further describing the additional JSON string, we can introduce test stages to the formatted feedback, which we can use to indicate to the student the test cases that have passed and/or failed. Actual JSON to be outputted { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Test Results\" ], \"Test Results\" : { \"Build\" : { \"passed\" : true }, \"Run\" : { \"passed\" : true }}} { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} Prettified JSON (for reference only) { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Test Results\" ], \"Test Results\" : { \"Build\" : { \"passed\" : true }, \"Run\" : { \"passed\" : true } } } { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} We would add [\"Test Results\"] to the stages key. Then we would add to the corresponding Test Results key an object containing all the test case results. In this case Build and Run were used, but you can use other names for the test cases as well.","title":"Semantic Feedback with Test Cases"},{"location":"features/formatted-feedback/#semantic-feedback-multi-stage","text":"Using the same manner in which we add a Test Stage in the previous section, we can adapt it to create as many stages as we want. The following example has three different stages, namely Build , Test and Timing , but you can use other names for the stages as well. Actual JSON to be outputted { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Build\" , \"Test\" , \"Timing\" ], \"Test\" : { \"Add Things\" : { \"passed\" : true }, \"Return Values\" : { \"passed\" : false , \"hint\" : \"You need to return 1\" }}, \"Build\" : { \"compile\" : { \"passed\" : true }, \"link\" : { \"passed\" : true }}, \"Timing\" : { \"Stage 1 (ms)\" : 10 , \"Stage 2 (ms)\" : 20 }} { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} Prettified JSON (for reference only) { \"_presentation\" : \"semantic\" , \"stages\" : [ \"Build\" , \"Test\" , \"Timing\" ], \"Test\" : { \"Add Things\" : { \"passed\" : true }, \"Return Values\" : { \"passed\" : false , \"hint\" : \"You need to return 1\" } }, \"Build\" : { \"compile\" : { \"passed\" : true }, \"link\" : { \"passed\" : true } }, \"Timing\" : { \"Stage 1 (ms)\" : 10 , \"Stage 2 (ms)\" : 20 } } { \"scores\" : { \"Correctness\" : 20 , \"TA/Design/Readability\" : 40 }} We would add the stages we want into the stages array. Then we would add those corresponding stages as a separate key, with each of them holding their own set of test case results. We are also able to provide hints if the student gets the particular test case wrong by adding a hint key to the test case.","title":"Semantic Feedback (Multi-Stage)"},{"location":"features/git-submission/","text":"Git Submission Git Submission is a feature that allows students to submit their code via Github instead of manual file uploads. This has pedagogical benefits such as encouraging the use of version control among students, and also makes the process of submitting code much easier for students, as they no longer have to create submission tarfiles. Autograders written for assessments that allow Git Submission must expect a .tgz handin file. See the Handin Format section for more information. Installation Follow these steps on the installation page in order to configure your Autolab deployment to support Github submission. Enabling Git Submission Git Submission can be enabled in the Handin tab of the Edit Assessment page. It can be toggled via a checkbox: How Git Submission Works Git Submission works by having students performing OAuth with your Github application in order to be granted access to access their private repositories. Only the minimum set of permissions to achieve this is requested. This allows your Autolab deployment to be able to query and clone their selected private repositories on the submission page. Autolab will create a compressed .tgz archive from the cloned repository, which ignores the .git directory (in case the student accidentally committed large files in the past), which is then saved and sent to Tango for autograding, if necessary. The cloned folder will be deleted. Handin Format Due to how Github Submission works , autograders on Git Submission enabled assessments must expect a .tgz archive. Depending on the design of your autograder, it may also be necessary for your autograder to only copy out relevant handin files from the uncompressed archive for use in autograding. Other than this requirement, using Github submission does not change any other part of the autograding/submission process. API Limits By default, each token is entitled to 5000 API requests per hour. Github counts all API requests (other than querying for your current API quota) against the token limits. This count is performed against the token that was used, and therefore it means that every student is able to make 5000 requests/hr, and not that the entire Autolab deployment can only make 5000 API requests/hr, which should be more than sufficient. Revoking Github Tokens Students can revoke their Github tokens on their profile page via the button Revoke Github Token . This will both destroy the token with Github, and also remove the token from the database. In order to use Github Submission again, the student will need to perform the OAuth workflow again. Best Practices and Common Issues Github Classroom Consider using Github Classroom to initialize the repositories containing starter code for assessments for students, with a comprehensive .gitignore file to prevent students from checking in unnecessary files. Handin Sizes The default handin size is quite small (2MB), and it is possible that starter code sizes could easily exceed that, so consider raising the limit based on what is reasonable. However, it is also a common mistake for students to accidentally check-in unnecessary logfiles or core dumps that could significantly inflate the size of their submission, and therefore the limit should not be too high to avoid a high rate of disk space usage.","title":"Git Submission"},{"location":"features/git-submission/#git-submission","text":"Git Submission is a feature that allows students to submit their code via Github instead of manual file uploads. This has pedagogical benefits such as encouraging the use of version control among students, and also makes the process of submitting code much easier for students, as they no longer have to create submission tarfiles. Autograders written for assessments that allow Git Submission must expect a .tgz handin file. See the Handin Format section for more information.","title":"Git Submission"},{"location":"features/git-submission/#installation","text":"Follow these steps on the installation page in order to configure your Autolab deployment to support Github submission.","title":"Installation"},{"location":"features/git-submission/#enabling-git-submission","text":"Git Submission can be enabled in the Handin tab of the Edit Assessment page. It can be toggled via a checkbox:","title":"Enabling Git Submission"},{"location":"features/git-submission/#how-git-submission-works","text":"Git Submission works by having students performing OAuth with your Github application in order to be granted access to access their private repositories. Only the minimum set of permissions to achieve this is requested. This allows your Autolab deployment to be able to query and clone their selected private repositories on the submission page. Autolab will create a compressed .tgz archive from the cloned repository, which ignores the .git directory (in case the student accidentally committed large files in the past), which is then saved and sent to Tango for autograding, if necessary. The cloned folder will be deleted.","title":"How Git Submission Works"},{"location":"features/git-submission/#handin-format","text":"Due to how Github Submission works , autograders on Git Submission enabled assessments must expect a .tgz archive. Depending on the design of your autograder, it may also be necessary for your autograder to only copy out relevant handin files from the uncompressed archive for use in autograding. Other than this requirement, using Github submission does not change any other part of the autograding/submission process.","title":"Handin Format"},{"location":"features/git-submission/#api-limits","text":"By default, each token is entitled to 5000 API requests per hour. Github counts all API requests (other than querying for your current API quota) against the token limits. This count is performed against the token that was used, and therefore it means that every student is able to make 5000 requests/hr, and not that the entire Autolab deployment can only make 5000 API requests/hr, which should be more than sufficient.","title":"API Limits"},{"location":"features/git-submission/#revoking-github-tokens","text":"Students can revoke their Github tokens on their profile page via the button Revoke Github Token . This will both destroy the token with Github, and also remove the token from the database. In order to use Github Submission again, the student will need to perform the OAuth workflow again.","title":"Revoking Github Tokens"},{"location":"features/git-submission/#best-practices-and-common-issues","text":"","title":"Best Practices and Common Issues"},{"location":"features/git-submission/#github-classroom","text":"Consider using Github Classroom to initialize the repositories containing starter code for assessments for students, with a comprehensive .gitignore file to prevent students from checking in unnecessary files.","title":"Github Classroom"},{"location":"features/git-submission/#handin-sizes","text":"The default handin size is quite small (2MB), and it is possible that starter code sizes could easily exceed that, so consider raising the limit based on what is reasonable. However, it is also a common mistake for students to accidentally check-in unnecessary logfiles or core dumps that could significantly inflate the size of their submission, and therefore the limit should not be too high to avoid a high rate of disk space usage.","title":"Handin Sizes"},{"location":"features/lti-course-integration/","text":"LTI Course Integration LTI Course Integration is a feature that allows instructors to link a course on their University's chosen LTI platform such as Canvas to their Autolab course. Currently, only course roster synchronization is supported. Installation Follow these steps on the installation page in order to configure your Autolab deployment to support LTI Course Integration. You must be an Autolab Administrator in order to configure LTI linking. Instructors can ignore these steps. Enabling LTI Course Linking Log on to Autolab. Then on your LTI platform, Install the LTI Platform's Autolab App to your corresponding course. The app should be set up by the administrators of your institution's LTI platform instance. Launch the Autolab App from your LTI platform. For example, on Canvas, after Autolab app is installed on your course, click on the \"Autolab\" link in the course navigation tab. Once you are redirected to Autolab, choose which Autolab course to link with the LTI platform. LTI Course Synchronization Once the LTI Launch flow is completed, instructors should be able to synchronize their course roster on Autolab with the LTI platform. Autolab will use the LTI platform as the source of truth when synchronizing its roster. Course synchronization is currently only done manually . In order to synchronize Autolab's course roster: Navigate to the \"Manage Course Users\" page. The last time at which the roster was synchronized should be displayed next to the refresh button. Click the refresh button at the top right of the course roster. You will then be redirected to a confirmation screen. Confirm the roster changes displayed in the roster table are correct, and then click \"confirm\" to apply the changes to your course roster. LTI Course Synchronization Settings Click on \"Linked Course Settings\" in the \"Manage Course\" page. Click \"Auto drop students not enrolled in linked course\" to drop any students in the Autolab course that are not found in the LTI platform's course. Click \"Unlink Course\" to unlink the LTI platform course from the Autolab course.","title":"LTI Course Linking"},{"location":"features/lti-course-integration/#lti-course-integration","text":"LTI Course Integration is a feature that allows instructors to link a course on their University's chosen LTI platform such as Canvas to their Autolab course. Currently, only course roster synchronization is supported.","title":"LTI Course Integration"},{"location":"features/lti-course-integration/#installation","text":"Follow these steps on the installation page in order to configure your Autolab deployment to support LTI Course Integration. You must be an Autolab Administrator in order to configure LTI linking. Instructors can ignore these steps.","title":"Installation"},{"location":"features/lti-course-integration/#enabling-lti-course-linking","text":"Log on to Autolab. Then on your LTI platform, Install the LTI Platform's Autolab App to your corresponding course. The app should be set up by the administrators of your institution's LTI platform instance. Launch the Autolab App from your LTI platform. For example, on Canvas, after Autolab app is installed on your course, click on the \"Autolab\" link in the course navigation tab. Once you are redirected to Autolab, choose which Autolab course to link with the LTI platform.","title":"Enabling LTI Course Linking"},{"location":"features/lti-course-integration/#lti-course-synchronization","text":"Once the LTI Launch flow is completed, instructors should be able to synchronize their course roster on Autolab with the LTI platform. Autolab will use the LTI platform as the source of truth when synchronizing its roster. Course synchronization is currently only done manually . In order to synchronize Autolab's course roster: Navigate to the \"Manage Course Users\" page. The last time at which the roster was synchronized should be displayed next to the refresh button. Click the refresh button at the top right of the course roster. You will then be redirected to a confirmation screen. Confirm the roster changes displayed in the roster table are correct, and then click \"confirm\" to apply the changes to your course roster.","title":"LTI Course Synchronization"},{"location":"features/lti-course-integration/#lti-course-synchronization-settings","text":"Click on \"Linked Course Settings\" in the \"Manage Course\" page. Click \"Auto drop students not enrolled in linked course\" to drop any students in the Autolab course that are not found in the LTI platform's course. Click \"Unlink Course\" to unlink the LTI platform course from the Autolab course.","title":"LTI Course Synchronization Settings"},{"location":"features/metrics/","text":"Metrics Traditional approaches of identifying students who are struggling with class is reactive; course staff wait for students to come to them to provide help, which can often be too late. The metrics feature seeks to be a proactive approach by actively identifying students who might be struggling in class through tracking of metrics that signify possible risks. Identifying students in need of attention early in the course would provide a better chance of getting them back on track in the course. Usage Flow We envision the feature to be used by the instructors in this order. They would: Set up their course and assignments as per usual Set up Student Metrics at the start of the course Be notified of pending students in need of attention in their Watchlist on the course page Visit the Watchlist , contact students if necessary, using it as a work list at the same time Refine the Student Metrics as the course progresses Student Metrics From our interviews with instructors, we understand that different courses have different measures of whether a student is in need of attention. As such, a set of conditions together will define the course's student metrics. We intend to add more conditions to the metrics in the future. Feel free to suggest them via our GitHub Issues page. Student Metrics Condition Rationale The conditions are designed to capture different characteristics of a possible student in need of attention. In the sections that follow we attempt to explain the rationale behind each condition to aid with selecting the conditions. Students who have used number grace days by date . For courses that provides grace days, students who use many grace days early in the course tend to have issues managing the workload and/or their time. A good rule of thumb is that a student should not have used all their grace days before the middle of the course. Students whose grades have dropped by number percent within number consecutive assignments within a category Identify students who have been slipping in their grades. Below are the underlying properties windowed based on the number consecutive decrease must be consecutive skips over no-submissions For example, given 4 assignments and we are looking for 20 percent grade drop over 3 consecutive assignments Assignment 1 2 3 4 Grade Dropping? Student A 80 80 80 80 No. Constant score Student B1 80 70 60 80 Yes. Slipping from assignment 1,2,3 Student B2 80 80 70 60 Yes. Slipping from assignment 2,3,4 Student C 80 90 60 70 No. Although there was a drop, it was not 3 consecutive Student D 90 80 no submit 80 No. It skips over no submission Students who did not submit number assignments Identify students who have not been submitting assignments. We made this a flexible number because we expect some courses to have ability to drop some assignments Students with number submitted assignments below a percentage of number Identify weaker students. We expect this condition to be useful earlier in the course, as it looks at all submitted assignments. It does not consider students who have not submitted an assignment. Watchlist Once instructors have set up student metrics for their course, students that are identified as in need of attention based on these metrics will appear in the watchlist. Watchlist Instance Every row in the watchlist represents a particular instance of a student who meets one or more of the metrics conditions. A single student can appear in multiple watchlist instances if they are identified for new metrics conditions on separate occasions of loading the watchlist. For example, let's look at Jane Doe in the image above. Upon loading the watchlist, Jane appears in a watchlist instance for using 3 grace days before the instructor-specified date and for having 2 low scores below the instructor-specified threshold. If Jane later receives another score below the threshold, a new instance will appear for Jane when the instructor reloads the watchlist. Jane now appears twice in the watchlist, once in an instance with 3 grace days and 2 low scores , and once in an instance with 3 grace days and 3 low scores . Actions The instructor can act on a watchlist instance by either contacting the student or resolving the student. Clicking the contact button on the watchlist instance directs the instructor to a mailto link and moves the instance into the contacted tab. Clicking the resolve button moves the instance into the resolved tab. The contacted and resolved tabs are discussed in the next section. To perform a \"resolve\" or \"contact\" in bulk, an instructor can click on multiple checkboxes and use the buttons located above the watchlist, or the instructor can select all by using the checkbox located above the watchlist. An instructor can also hover over the condition tags to view the specific submissions and/or scores that led to the student being identified. Tabs There are four categories that watchlist instances can fall into: pending, contacted, resolved, and archived. Pending The pending tab contains identified students who have not yet been contacted or resolved. The number of pending instances will appear in a notification badge on the main course page, as shown below. Contacted The contacted tab contains all instances for which the instructor has contacted the student. Note: this does not mean that the student has been contacted for all associated watchlist instances. Resolved The resolved tab contains all instances that the instructor has marked as resolved. Note: this does not mean that the student has been marked as resolved for all associated watchlist instances. Archived When an instructor adjusts the student metrics for a course, all instances that were in contacted or resolved for the outdated student metrics are placed into archived . All pending instances for the outdated student metrics are dropped. As such, all instances in pending , contacted , and resolved are consistent with the most up-to-date student metrics. Considered Categories To account for the fact that some courses may have optional assignments that shouldn't be used to determine which students are struggling, we have created an \"allowlist\" of considered categories. This feature allows instructors to toggle which categories of assignments should be included or excluded in metrics calculations. Configuring Considered Categories The allowlist is located under the \"Configurations\" tab of the student metrics page. To toggle an assignment between \"Included\" and \"Excluded,\" a user can press the arrow icon between the two lists. After pressing save, the new considered category settings will be updated and taken into consideration the next time you refresh the watchlist.","title":"Metrics"},{"location":"features/metrics/#metrics","text":"Traditional approaches of identifying students who are struggling with class is reactive; course staff wait for students to come to them to provide help, which can often be too late. The metrics feature seeks to be a proactive approach by actively identifying students who might be struggling in class through tracking of metrics that signify possible risks. Identifying students in need of attention early in the course would provide a better chance of getting them back on track in the course.","title":"Metrics"},{"location":"features/metrics/#usage-flow","text":"We envision the feature to be used by the instructors in this order. They would: Set up their course and assignments as per usual Set up Student Metrics at the start of the course Be notified of pending students in need of attention in their Watchlist on the course page Visit the Watchlist , contact students if necessary, using it as a work list at the same time Refine the Student Metrics as the course progresses","title":"Usage Flow"},{"location":"features/metrics/#student-metrics","text":"From our interviews with instructors, we understand that different courses have different measures of whether a student is in need of attention. As such, a set of conditions together will define the course's student metrics. We intend to add more conditions to the metrics in the future. Feel free to suggest them via our GitHub Issues page.","title":"Student Metrics"},{"location":"features/metrics/#student-metrics-condition-rationale","text":"The conditions are designed to capture different characteristics of a possible student in need of attention. In the sections that follow we attempt to explain the rationale behind each condition to aid with selecting the conditions.","title":"Student Metrics Condition Rationale"},{"location":"features/metrics/#students-who-have-used-number-grace-days-by-date","text":"For courses that provides grace days, students who use many grace days early in the course tend to have issues managing the workload and/or their time. A good rule of thumb is that a student should not have used all their grace days before the middle of the course.","title":"Students who have used number grace days by date."},{"location":"features/metrics/#students-whose-grades-have-dropped-by-number-percent-within-number-consecutive-assignments-within-a-category","text":"Identify students who have been slipping in their grades. Below are the underlying properties windowed based on the number consecutive decrease must be consecutive skips over no-submissions For example, given 4 assignments and we are looking for 20 percent grade drop over 3 consecutive assignments Assignment 1 2 3 4 Grade Dropping? Student A 80 80 80 80 No. Constant score Student B1 80 70 60 80 Yes. Slipping from assignment 1,2,3 Student B2 80 80 70 60 Yes. Slipping from assignment 2,3,4 Student C 80 90 60 70 No. Although there was a drop, it was not 3 consecutive Student D 90 80 no submit 80 No. It skips over no submission","title":"Students whose grades have dropped by number percent within number consecutive assignments within a category"},{"location":"features/metrics/#students-who-did-not-submit-number-assignments","text":"Identify students who have not been submitting assignments. We made this a flexible number because we expect some courses to have ability to drop some assignments","title":"Students who did not submit number assignments"},{"location":"features/metrics/#students-with-number-submitted-assignments-below-a-percentage-of-number","text":"Identify weaker students. We expect this condition to be useful earlier in the course, as it looks at all submitted assignments. It does not consider students who have not submitted an assignment.","title":"Students with number submitted assignments below a percentage of number"},{"location":"features/metrics/#watchlist","text":"Once instructors have set up student metrics for their course, students that are identified as in need of attention based on these metrics will appear in the watchlist.","title":"Watchlist"},{"location":"features/metrics/#watchlist-instance","text":"Every row in the watchlist represents a particular instance of a student who meets one or more of the metrics conditions. A single student can appear in multiple watchlist instances if they are identified for new metrics conditions on separate occasions of loading the watchlist. For example, let's look at Jane Doe in the image above. Upon loading the watchlist, Jane appears in a watchlist instance for using 3 grace days before the instructor-specified date and for having 2 low scores below the instructor-specified threshold. If Jane later receives another score below the threshold, a new instance will appear for Jane when the instructor reloads the watchlist. Jane now appears twice in the watchlist, once in an instance with 3 grace days and 2 low scores , and once in an instance with 3 grace days and 3 low scores .","title":"Watchlist Instance"},{"location":"features/metrics/#actions","text":"The instructor can act on a watchlist instance by either contacting the student or resolving the student. Clicking the contact button on the watchlist instance directs the instructor to a mailto link and moves the instance into the contacted tab. Clicking the resolve button moves the instance into the resolved tab. The contacted and resolved tabs are discussed in the next section. To perform a \"resolve\" or \"contact\" in bulk, an instructor can click on multiple checkboxes and use the buttons located above the watchlist, or the instructor can select all by using the checkbox located above the watchlist. An instructor can also hover over the condition tags to view the specific submissions and/or scores that led to the student being identified.","title":"Actions"},{"location":"features/metrics/#tabs","text":"There are four categories that watchlist instances can fall into: pending, contacted, resolved, and archived.","title":"Tabs"},{"location":"features/metrics/#pending","text":"The pending tab contains identified students who have not yet been contacted or resolved. The number of pending instances will appear in a notification badge on the main course page, as shown below.","title":"Pending"},{"location":"features/metrics/#contacted","text":"The contacted tab contains all instances for which the instructor has contacted the student. Note: this does not mean that the student has been contacted for all associated watchlist instances.","title":"Contacted"},{"location":"features/metrics/#resolved","text":"The resolved tab contains all instances that the instructor has marked as resolved. Note: this does not mean that the student has been marked as resolved for all associated watchlist instances.","title":"Resolved"},{"location":"features/metrics/#archived","text":"When an instructor adjusts the student metrics for a course, all instances that were in contacted or resolved for the outdated student metrics are placed into archived . All pending instances for the outdated student metrics are dropped. As such, all instances in pending , contacted , and resolved are consistent with the most up-to-date student metrics.","title":"Archived"},{"location":"features/metrics/#considered-categories","text":"To account for the fact that some courses may have optional assignments that shouldn't be used to determine which students are struggling, we have created an \"allowlist\" of considered categories. This feature allows instructors to toggle which categories of assignments should be included or excluded in metrics calculations.","title":"Considered Categories"},{"location":"features/metrics/#configuring-considered-categories","text":"The allowlist is located under the \"Configurations\" tab of the student metrics page. To toggle an assignment between \"Included\" and \"Excluded,\" a user can press the arrow icon between the two lists. After pressing save, the new considered category settings will be updated and taken into consideration the next time you refresh the watchlist.","title":"Configuring Considered Categories"},{"location":"features/moss/","text":"MOSS Plagiarism Detection Installation MOSS (Measure Of Software Similarity) is a system for checking for plagiarism. MOSS can be setup on Autolab as follows: Obtain the script for MOSS based on the instructions given here . Create a directory called vendor at the root of your Autolab installation, i.e cd <autolab_root> mkdir -p vendor Copy the moss script into the vendor directory and name it mossnet mv <path_to_moss_script> vendor/mossnet","title":"MOSS Plagiarism Detection"},{"location":"features/moss/#moss-plagiarism-detection-installation","text":"MOSS (Measure Of Software Similarity) is a system for checking for plagiarism. MOSS can be setup on Autolab as follows: Obtain the script for MOSS based on the instructions given here . Create a directory called vendor at the root of your Autolab installation, i.e cd <autolab_root> mkdir -p vendor Copy the moss script into the vendor directory and name it mossnet mv <path_to_moss_script> vendor/mossnet","title":"MOSS Plagiarism Detection Installation"},{"location":"features/schedulers/","text":"Schedulers Schedulers are instructor scripts that run periodically. They can be managed via Manage Course > Manage schedulers . Interval Guarantees Schedulers only run when a page load occurs. Thus, the interval parameter only guarantees a minimum time between runs. Scheduler structure Schedulers must define a update method within a class or module. Changes made to the scheduler file take effect immediately. Using a module module Updater def self . update ( course ) # code goes here end end Using a class class Updater def self . update ( course ) # code goes here end end Visual Run You can run a scheduler manually by clicking the Run button. This is useful for ensuring the code's correctness. To assist in debugging, you can return a string from the update method, which will be displayed as output in the browser. You should return nil to represent no output. Output String If you do not explicitly return a value, this might lead to unexpected outputs due to Ruby's implicit return values. The return value will be converted to a string if possible, else it will be treated as no output. Example file module Updater def self . update ( course ) out = \"\" out << \"my output \\n \" out end end Visual Run Output","title":"Schedulers"},{"location":"features/schedulers/#schedulers","text":"Schedulers are instructor scripts that run periodically. They can be managed via Manage Course > Manage schedulers . Interval Guarantees Schedulers only run when a page load occurs. Thus, the interval parameter only guarantees a minimum time between runs.","title":"Schedulers"},{"location":"features/schedulers/#scheduler-structure","text":"Schedulers must define a update method within a class or module. Changes made to the scheduler file take effect immediately. Using a module module Updater def self . update ( course ) # code goes here end end Using a class class Updater def self . update ( course ) # code goes here end end","title":"Scheduler structure"},{"location":"features/schedulers/#visual-run","text":"You can run a scheduler manually by clicking the Run button. This is useful for ensuring the code's correctness. To assist in debugging, you can return a string from the update method, which will be displayed as output in the browser. You should return nil to represent no output. Output String If you do not explicitly return a value, this might lead to unexpected outputs due to Ruby's implicit return values. The return value will be converted to a string if possible, else it will be treated as no output. Example file module Updater def self . update ( course ) out = \"\" out << \"my output \\n \" out end end Visual Run Output","title":"Visual Run"},{"location":"features/scoreboards/","text":"Scoreboards Scoreboards are created by the output of Autograders . They anonymously rank students submitted assignments inspiring health competition and desire to improve. They are simple and highly customizable. Scoreboard's can be added/edited on the edit assessment screen ( /courses/<course>/assessments/<assessment>/edit ). In general, scoreboards are configured using a JSON string. Default Scoreboard The default scoreboard displays the total problem scores, followed by each individual problem score, sorted in descending order by the total score. Custom Scoreboards Autograded assignments have the option of creating custom scoreboards. You can specify your own custom scoreboard using a JSON column specification. The column spec consists of a \"scoreboard\" object, which is an array of JSON objects, where each object describes a column. Example: a scoreboard with one column, called Score . { \"scoreboard\" : [{ \"hdr\" : \"Score\" }] } A custom scoreboard sorts the columns from left to right in descending order, and tiebreaks by submission time. You can change the default sort order for a particular column by adding an optional \"asc:1\" element to its hash. Example: Scoreboard with two columns, \"Score\" and \"Ops\", with \"Score\" sorted descending, and then \"Ops\" ascending: { \"scoreboard\" : [{ \"hdr\" : \"Score\" }, { \"hdr\" : \"Ops\" , \"asc\" : 1 }] } Scoreboard Entries The values for each row in a custom scoreboard come directly from a scoreboard array object in the autoresult string produced by the Tango, the autograder. Example: Autoresult returning the score (97) for a single autograded problem called autograded , and a scoreboard entry with two columns: the autograded score ( Score ) and the number of operations ( Ops ): { \"scores\" : { \"autograded\" : 97 }, \"scoreboard\" : [ 97 , 128 ] } For more information on how to use Autograders and Scoreboards together, visit the Guide for Lab Authors .","title":"Scoreboards"},{"location":"features/scoreboards/#scoreboards","text":"Scoreboards are created by the output of Autograders . They anonymously rank students submitted assignments inspiring health competition and desire to improve. They are simple and highly customizable. Scoreboard's can be added/edited on the edit assessment screen ( /courses/<course>/assessments/<assessment>/edit ). In general, scoreboards are configured using a JSON string.","title":"Scoreboards"},{"location":"features/scoreboards/#default-scoreboard","text":"The default scoreboard displays the total problem scores, followed by each individual problem score, sorted in descending order by the total score.","title":"Default Scoreboard"},{"location":"features/scoreboards/#custom-scoreboards","text":"Autograded assignments have the option of creating custom scoreboards. You can specify your own custom scoreboard using a JSON column specification. The column spec consists of a \"scoreboard\" object, which is an array of JSON objects, where each object describes a column. Example: a scoreboard with one column, called Score . { \"scoreboard\" : [{ \"hdr\" : \"Score\" }] } A custom scoreboard sorts the columns from left to right in descending order, and tiebreaks by submission time. You can change the default sort order for a particular column by adding an optional \"asc:1\" element to its hash. Example: Scoreboard with two columns, \"Score\" and \"Ops\", with \"Score\" sorted descending, and then \"Ops\" ascending: { \"scoreboard\" : [{ \"hdr\" : \"Score\" }, { \"hdr\" : \"Ops\" , \"asc\" : 1 }] }","title":"Custom Scoreboards"},{"location":"features/scoreboards/#scoreboard-entries","text":"The values for each row in a custom scoreboard come directly from a scoreboard array object in the autoresult string produced by the Tango, the autograder. Example: Autoresult returning the score (97) for a single autograded problem called autograded , and a scoreboard entry with two columns: the autograded score ( Score ) and the number of operations ( Ops ): { \"scores\" : { \"autograded\" : 97 }, \"scoreboard\" : [ 97 , 128 ] } For more information on how to use Autograders and Scoreboards together, visit the Guide for Lab Authors .","title":"Scoreboard Entries"},{"location":"installation/docker-compose/","text":"Autolab + Tango Docker Compose Installation The Autolab Docker Compose installation is a fast and easy production-ready installation and deployment method. It uses a MySQL database for the Autolab deployment, and comes with TLS/SSL support. This is now the preferred way of installing Autolab due to its ease of use. If you are stuck or find issues with the installation process you can either file an issue on our Github repository, or join our Slack here and let us know and we will try our best to help. Also see the debugging section for tips on how to diagnose problems and check out the troubleshooting section if you run into any issues. Installation First ensure that you have Docker and Docker Compose installed on your machine. See the official Docker docs for the installation steps. Clone this repository and its Autolab and Tango submodules: git clone --recurse-submodules -j8 https://github.com/autolab/docker.git autolab-docker Enter the project directory: cd autolab-docker Update our Autolab and Tango submodules to ensure that you are getting the latest versions: make update You may need to install make using the appropriate command for your system, such as apt install make . Create initial default configs: make Build the Dockerfiles for both Autolab and Tango: docker compose build Run the Docker containers: docker compose up -d Note at this point Nginx will still be crash-looping in the Autolab container because TLS/SSL has not been configured/disabled yet. Ensure that the newly created config files have the right permissions, as it may have been modified during the building process: make set-perms Perform database migrations for Autolab, which will initialize your database schema: make db-migrate Create administrative user for Autolab: make create-user This user has full permissions on Autolab and will be able to create other users and designate other admins. Change DOCKER_TANGO_HOST_VOLUME_PATH in .env to be the absolute path to the Tango volumes directory, i.e /<path-to-docker-compose-installation>/Tango/volumes . This is so that Tango knows where to put the output files of its autograded jobs. # in .env # Modify the below to be the path to volumes on your host machine DOCKER_TANGO_HOST_VOLUME_PATH = /home/your-user/autolab-docker/Tango/volumes Stop all containers, as we are going to setup/disable TLS: docker compose stop Update the Nginx config. Update all occurrences of REPLACE_WITH_YOUR_DOMAIN in nginx/app.conf and nginx/no-ssl-app.conf to your real domain name. The configs are used when TLS is enabled and disabled respectively. Double-check that ALL occurrences are replaced as otherwise you will have trouble accessing your deployment. Continue with TLS setup as outlined in the next section Build the autograding image(s) that you want to use in Tango (see the docs for more information). For this setup we will stick to the default Ubuntu 18.04 autograding image: docker build -t autograding_image Tango/vmms/ Note that we can just run this directly on the host because we are mapping the Docker socket to the Tango container (i.e they are using the same Docker server). Start up everything: docker compose up -d Autolab should now be accessible on port 80 (and 443 if you configured TLS)! You can now go on to configure mailing, follow the instructions for setting up mailing with the only difference being that the paths mentioned are relative to the directory Autolab/ . Now you are all set to start using Autolab! Please fill out this form to join our registry so that we can provide you with news about the latest features, bug-fixes, and security updates. For more info, visit the Guide for Instructors and Guide for Lab Authors . Configuring TLS/SSL Having TLS/SSL configured is important as it helps to ensure that sensitive information like user credentials and submission information are encrypted instead of being sent over in plaintext across the network when users are using Autolab. We have made setting up TLS as easy and pain-free as possible. Using TLS is strongly recommended if you are using Autolab in a production environment with real students and instructors. There are three options for TLS: using Let's Encrypt (for free TLS certificates), using your own certificate, and not using TLS (suitable for local testing/development, but not recommended for production deployment). Option 1: Let's Encrypt Ensure that your DNS record points towards the IP address of your server Ensure that port 443 is exposed on your server (i.e checking your firewall, AWS security group settings, etc) In ssl/init-letsencrypt.sh , change domains=(example.com) to the list of domains that your host is associated with, and change email to be your email address so that Let's Encrypt will be able to email you when your certificate is about to expire If necessary, change staging=0 to staging=1 to avoid being rate-limited by Let's Encrypt since there is a limit of 20 certificates/week. Setting this is helpful if you have an experimental setup. Run your modified script: sudo ./ssl/init-letsencrypt.sh Option 2: Using your own TLS certificate Copy your private key to ssl/privkey.pem Copy your certificate to ssl/fullchain.pem Generate your dhparams: openssl dhparam -out ssl/ssl-dhparams.pem 4096 Uncomment the following lines in docker-compose.yml : # - ./ssl/fullchain.pem:/etc/letsencrypt/live/test.autolab.io/fullchain.pem; # - ./ssl/privkey.pem:/etc/letsencrypt/live/test.autolab.io/privkey.pem; # - ./ssl/ssl-dhparams.pem:/etc/letsencrypt/ssl-dhparams.pem Option 3: No TLS (not recommended, only for local development/testing) In docker-compose.yml , comment out the following: # Comment the below out to disable SSL (not recommended) - ./nginx/app.conf:/etc/nginx/sites-enabled/webapp.conf In docker-compose.yml , also uncomment the following: # Uncomment the below to disable SSL (not recommended) # - ./nginx/no-ssl-app.conf:/etc/nginx/sites-enabled/webapp.conf Lastly, in .env file, set DOCKER_SSL=false : # set to false for no SSL (not recommended) DOCKER_SSL = false Updating Your Docker Compose Deployment Stop your running instances: docker compose stop Update your Autolab and Tango repositories: make update Rebuild the images with the latest code: docker compose build Re-deploy your containers: docker compose up Debugging your Deployment In the (very likely) event that you run into problems during setup, hopefully these steps will help you to help identify and diagnose the issue. If you continue to face difficulties or believe you discovered issues with the setup process please join our Slack here and let us know and we will try our best to help. Better logging output for Docker Compose By default, docker compose up -d runs in detached state and it is not easy to immediately see errors: $ docker compose up -d Starting certbot ... done Starting redis ... done Starting mysql ... done Starting tango ... done Recreating autolab ... done Use docker compose up instead to get output from all the containers in real time: $ docker compose up Starting certbot ... done Starting mysql ... done Starting redis ... done Starting tango ... done Starting autolab ... done Attaching to redis, mysql, certbot, tango, autolab mysql | [ Entrypoint ] MySQL Docker Image 8 .0.22-1.1.18 tango | 2020 -11-11 04 :33:19,533 CRIT Supervisor running as root ( no user in config file ) redis | 1 :C 11 Nov 2020 04 :33:19.032 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo redis | 1 :C 11 Nov 2020 04 :33:19.032 # Redis version=6.0.9, bits=64, commit=00000000, modified=0, pid=1, just started redis | 1 :C 11 Nov 2020 04 :33:19.032 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf mysql | [ Entrypoint ] Starting MySQL 8 .0.22-1.1.18 redis | 1 :M 11 Nov 2020 04 :33:19.033 * Running mode = standalone, port = 6379 . redis | 1 :M 11 Nov 2020 04 :33:19.033 # Server initialized tango | 2020 -11-11 04 :33:19,539 INFO RPC interface 'supervisor' initialized tango | 2020 -11-11 04 :33:19,539 CRIT Server 'unix_http_server' running without any HTTP authentication checking mysql | 2020 -11-11T04:33:19.476749Z 0 [ System ] [ MY-010116 ] [ Server ] /usr/sbin/mysqld ( mysqld 8 .0.22 ) starting as process 22 --- output truncated --- Checking Autolab logs If the Autolab instance is not working properly, taking a look at both the application logs as well as the Nginx logs in the container will be helpful. First, find the name of the container. This should be just autolab by default: $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 765d35962f52 autolab-docker_autolab \"/sbin/my_init\" 31 minutes ago Up 22 minutes 0 .0.0.0:80->80/tcp, 0 .0.0.0:443->443/tcp autolab a5b77b5267b1 autolab-docker_tango \"/usr/bin/supervisor\u2026\" 7 days ago Up 22 minutes 0 .0.0.0:3000->3000/tcp tango 438d8e9f73e2 redis:latest \"docker-entrypoint.s\u2026\" 7 days ago Up 22 minutes 6379 /tcp redis da86acc5a4c3 mysql/mysql-server:latest \"/entrypoint.sh mysq\u2026\" 7 days ago Up 22 minutes ( healthy ) 3306 /tcp, 33060 -33061/tcp mysql 88032e85d669 a2eb12050715 \"/bin/bash\" 9 days ago Up 2 days compiler Next get a shell inside the container: $ docker exec -it autolab bash root@be56be775428:/home/app/webapp# By default we are in the project directory. Navigate to the logs directory and cat or tail production.log . This contains logs from the Autolab application itself. root@be56be775428:/home/app/webapp# cd log root@be56be775428:/home/app/webapp/log# tail -f -n +1 production.log We can also check out our Nginx logs in /var/log/nginx/ : root@be56be775428:/home/app/webapp/log# cd /var/log/nginx/ root@be56be775428:/var/log/nginx# ls access.log error.log Accessing the Rails console Obtain a shell in the autolab container as described previously , and do RAILS_ENV=production bundle exec rails c : root@be56be775428:/home/app/webapp# RAILS_ENV = production bundle exec rails c Loading production environment ( Rails 5 .2.0 ) 2 .6.6 :001 > User.all.count = > 1 In the example above, if you performed make create-user you should have at least one user in your database. If there are errors connecting to a database here it is likely that the database was misconfigured. Checking Tango Logs Get a shell in the Tango instance, similar to the instructions mentioned previously . The logs are stored in the parent folder ( /opt/TangoService ) of the project directory: $ docker exec -it tango bash root@a5b77b5267b1:/opt/TangoService/Tango# cd .. root@a5b77b5267b1:/opt/TangoService# ls Tango tango_job_manager_log.log tango_log.log root@a5b77b5267b1:/opt/TangoService# tail -f -n +1 tango_job_manager_log.log tango_log.log Troubleshooting Autolab/Tango Connection In the Autolab container, try to curl Tango: root@be56be775428:/home/app/webapp# curl tango:3000 Hello, world! RESTful Tango here! In the Tango container, try to curl Autolab: root@a5b77b5267b1:/opt/TangoService/Tango# curl autolab <html> <head><title>301 Moved Permanently</title></head> <body bgcolor = \"white\" > <center><h1>301 Moved Permanently</h1></center> <hr><center>nginx/1.14.0 ( Ubuntu ) </center> </body> </html> Permission issues in Autolab Run the following again: make set-perms Restarting Autolab Passenger Server This is useful when you might want to test out some code change within the Autolab container without having to rebuild everything again. These changes can be applied by just restarting the Passenger service that is serving Autolab. Run passenger-config restart-app : root@8b56488b3fb6:/home/app/webapp# passenger-config restart-app Please select the application to restart. Tip: re-run this command with --help to learn how to automate it. If the menu doesn 't display correctly, press ' ! ' \u2023 /home/app/webapp ( production ) Cancel Restarting /home/app/webapp ( production ) Troubleshooting error: unable to unlink old 'db/schema.rb': Permission denied If you obtain the following error when attempting to perform make update : error: unable to unlink old 'db/schema.rb' : Permission denied fatal: Could not reset index file to revision 'HEAD' . This is due to the fact that db/schema.rb is updated whenever migrations are performed. db/schema.rb documents the database schema, which depends on the database that you are using, its version, and when the migrations were run. It is likely that your db/schema.rb will diverge from the one generated by the devs. You can resolve this by changing the owner of the files to be your current user, and then running make set-perms afterwards when you start the containers again. Autograder logs not appearing This happens when you are accessing Autolab via localhost, as Tango will attempt to send the autograder logs to its own localhost instead. To remedy this, add 127.0.0.1 autolab to /etc/hosts and access Autolab via http://autolab instead of http://localhost . If you are accessing Autolab on a different host, add <your public_ip> <your fqdn> autolab to your /etc/hosts file and access Autolab via http://autolab .","title":"Docker Compose Install"},{"location":"installation/docker-compose/#autolab-tango-docker-compose-installation","text":"The Autolab Docker Compose installation is a fast and easy production-ready installation and deployment method. It uses a MySQL database for the Autolab deployment, and comes with TLS/SSL support. This is now the preferred way of installing Autolab due to its ease of use. If you are stuck or find issues with the installation process you can either file an issue on our Github repository, or join our Slack here and let us know and we will try our best to help. Also see the debugging section for tips on how to diagnose problems and check out the troubleshooting section if you run into any issues.","title":"Autolab + Tango Docker Compose Installation"},{"location":"installation/docker-compose/#installation","text":"First ensure that you have Docker and Docker Compose installed on your machine. See the official Docker docs for the installation steps. Clone this repository and its Autolab and Tango submodules: git clone --recurse-submodules -j8 https://github.com/autolab/docker.git autolab-docker Enter the project directory: cd autolab-docker Update our Autolab and Tango submodules to ensure that you are getting the latest versions: make update You may need to install make using the appropriate command for your system, such as apt install make . Create initial default configs: make Build the Dockerfiles for both Autolab and Tango: docker compose build Run the Docker containers: docker compose up -d Note at this point Nginx will still be crash-looping in the Autolab container because TLS/SSL has not been configured/disabled yet. Ensure that the newly created config files have the right permissions, as it may have been modified during the building process: make set-perms Perform database migrations for Autolab, which will initialize your database schema: make db-migrate Create administrative user for Autolab: make create-user This user has full permissions on Autolab and will be able to create other users and designate other admins. Change DOCKER_TANGO_HOST_VOLUME_PATH in .env to be the absolute path to the Tango volumes directory, i.e /<path-to-docker-compose-installation>/Tango/volumes . This is so that Tango knows where to put the output files of its autograded jobs. # in .env # Modify the below to be the path to volumes on your host machine DOCKER_TANGO_HOST_VOLUME_PATH = /home/your-user/autolab-docker/Tango/volumes Stop all containers, as we are going to setup/disable TLS: docker compose stop Update the Nginx config. Update all occurrences of REPLACE_WITH_YOUR_DOMAIN in nginx/app.conf and nginx/no-ssl-app.conf to your real domain name. The configs are used when TLS is enabled and disabled respectively. Double-check that ALL occurrences are replaced as otherwise you will have trouble accessing your deployment. Continue with TLS setup as outlined in the next section Build the autograding image(s) that you want to use in Tango (see the docs for more information). For this setup we will stick to the default Ubuntu 18.04 autograding image: docker build -t autograding_image Tango/vmms/ Note that we can just run this directly on the host because we are mapping the Docker socket to the Tango container (i.e they are using the same Docker server). Start up everything: docker compose up -d Autolab should now be accessible on port 80 (and 443 if you configured TLS)! You can now go on to configure mailing, follow the instructions for setting up mailing with the only difference being that the paths mentioned are relative to the directory Autolab/ . Now you are all set to start using Autolab! Please fill out this form to join our registry so that we can provide you with news about the latest features, bug-fixes, and security updates. For more info, visit the Guide for Instructors and Guide for Lab Authors .","title":"Installation"},{"location":"installation/docker-compose/#configuring-tlsssl","text":"Having TLS/SSL configured is important as it helps to ensure that sensitive information like user credentials and submission information are encrypted instead of being sent over in plaintext across the network when users are using Autolab. We have made setting up TLS as easy and pain-free as possible. Using TLS is strongly recommended if you are using Autolab in a production environment with real students and instructors. There are three options for TLS: using Let's Encrypt (for free TLS certificates), using your own certificate, and not using TLS (suitable for local testing/development, but not recommended for production deployment).","title":"Configuring TLS/SSL"},{"location":"installation/docker-compose/#option-1-lets-encrypt","text":"Ensure that your DNS record points towards the IP address of your server Ensure that port 443 is exposed on your server (i.e checking your firewall, AWS security group settings, etc) In ssl/init-letsencrypt.sh , change domains=(example.com) to the list of domains that your host is associated with, and change email to be your email address so that Let's Encrypt will be able to email you when your certificate is about to expire If necessary, change staging=0 to staging=1 to avoid being rate-limited by Let's Encrypt since there is a limit of 20 certificates/week. Setting this is helpful if you have an experimental setup. Run your modified script: sudo ./ssl/init-letsencrypt.sh","title":"Option 1: Let's Encrypt"},{"location":"installation/docker-compose/#option-2-using-your-own-tls-certificate","text":"Copy your private key to ssl/privkey.pem Copy your certificate to ssl/fullchain.pem Generate your dhparams: openssl dhparam -out ssl/ssl-dhparams.pem 4096 Uncomment the following lines in docker-compose.yml : # - ./ssl/fullchain.pem:/etc/letsencrypt/live/test.autolab.io/fullchain.pem; # - ./ssl/privkey.pem:/etc/letsencrypt/live/test.autolab.io/privkey.pem; # - ./ssl/ssl-dhparams.pem:/etc/letsencrypt/ssl-dhparams.pem","title":"Option 2: Using your own TLS certificate"},{"location":"installation/docker-compose/#option-3-no-tls-not-recommended-only-for-local-developmenttesting","text":"In docker-compose.yml , comment out the following: # Comment the below out to disable SSL (not recommended) - ./nginx/app.conf:/etc/nginx/sites-enabled/webapp.conf In docker-compose.yml , also uncomment the following: # Uncomment the below to disable SSL (not recommended) # - ./nginx/no-ssl-app.conf:/etc/nginx/sites-enabled/webapp.conf Lastly, in .env file, set DOCKER_SSL=false : # set to false for no SSL (not recommended) DOCKER_SSL = false","title":"Option 3: No TLS (not recommended, only for local development/testing)"},{"location":"installation/docker-compose/#updating-your-docker-compose-deployment","text":"Stop your running instances: docker compose stop Update your Autolab and Tango repositories: make update Rebuild the images with the latest code: docker compose build Re-deploy your containers: docker compose up","title":"Updating Your Docker Compose Deployment"},{"location":"installation/docker-compose/#debugging-your-deployment","text":"In the (very likely) event that you run into problems during setup, hopefully these steps will help you to help identify and diagnose the issue. If you continue to face difficulties or believe you discovered issues with the setup process please join our Slack here and let us know and we will try our best to help.","title":"Debugging your Deployment"},{"location":"installation/docker-compose/#better-logging-output-for-docker-compose","text":"By default, docker compose up -d runs in detached state and it is not easy to immediately see errors: $ docker compose up -d Starting certbot ... done Starting redis ... done Starting mysql ... done Starting tango ... done Recreating autolab ... done Use docker compose up instead to get output from all the containers in real time: $ docker compose up Starting certbot ... done Starting mysql ... done Starting redis ... done Starting tango ... done Starting autolab ... done Attaching to redis, mysql, certbot, tango, autolab mysql | [ Entrypoint ] MySQL Docker Image 8 .0.22-1.1.18 tango | 2020 -11-11 04 :33:19,533 CRIT Supervisor running as root ( no user in config file ) redis | 1 :C 11 Nov 2020 04 :33:19.032 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo redis | 1 :C 11 Nov 2020 04 :33:19.032 # Redis version=6.0.9, bits=64, commit=00000000, modified=0, pid=1, just started redis | 1 :C 11 Nov 2020 04 :33:19.032 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf mysql | [ Entrypoint ] Starting MySQL 8 .0.22-1.1.18 redis | 1 :M 11 Nov 2020 04 :33:19.033 * Running mode = standalone, port = 6379 . redis | 1 :M 11 Nov 2020 04 :33:19.033 # Server initialized tango | 2020 -11-11 04 :33:19,539 INFO RPC interface 'supervisor' initialized tango | 2020 -11-11 04 :33:19,539 CRIT Server 'unix_http_server' running without any HTTP authentication checking mysql | 2020 -11-11T04:33:19.476749Z 0 [ System ] [ MY-010116 ] [ Server ] /usr/sbin/mysqld ( mysqld 8 .0.22 ) starting as process 22 --- output truncated ---","title":"Better logging output for Docker Compose"},{"location":"installation/docker-compose/#checking-autolab-logs","text":"If the Autolab instance is not working properly, taking a look at both the application logs as well as the Nginx logs in the container will be helpful. First, find the name of the container. This should be just autolab by default: $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 765d35962f52 autolab-docker_autolab \"/sbin/my_init\" 31 minutes ago Up 22 minutes 0 .0.0.0:80->80/tcp, 0 .0.0.0:443->443/tcp autolab a5b77b5267b1 autolab-docker_tango \"/usr/bin/supervisor\u2026\" 7 days ago Up 22 minutes 0 .0.0.0:3000->3000/tcp tango 438d8e9f73e2 redis:latest \"docker-entrypoint.s\u2026\" 7 days ago Up 22 minutes 6379 /tcp redis da86acc5a4c3 mysql/mysql-server:latest \"/entrypoint.sh mysq\u2026\" 7 days ago Up 22 minutes ( healthy ) 3306 /tcp, 33060 -33061/tcp mysql 88032e85d669 a2eb12050715 \"/bin/bash\" 9 days ago Up 2 days compiler Next get a shell inside the container: $ docker exec -it autolab bash root@be56be775428:/home/app/webapp# By default we are in the project directory. Navigate to the logs directory and cat or tail production.log . This contains logs from the Autolab application itself. root@be56be775428:/home/app/webapp# cd log root@be56be775428:/home/app/webapp/log# tail -f -n +1 production.log We can also check out our Nginx logs in /var/log/nginx/ : root@be56be775428:/home/app/webapp/log# cd /var/log/nginx/ root@be56be775428:/var/log/nginx# ls access.log error.log","title":"Checking Autolab logs"},{"location":"installation/docker-compose/#accessing-the-rails-console","text":"Obtain a shell in the autolab container as described previously , and do RAILS_ENV=production bundle exec rails c : root@be56be775428:/home/app/webapp# RAILS_ENV = production bundle exec rails c Loading production environment ( Rails 5 .2.0 ) 2 .6.6 :001 > User.all.count = > 1 In the example above, if you performed make create-user you should have at least one user in your database. If there are errors connecting to a database here it is likely that the database was misconfigured.","title":"Accessing the Rails console"},{"location":"installation/docker-compose/#checking-tango-logs","text":"Get a shell in the Tango instance, similar to the instructions mentioned previously . The logs are stored in the parent folder ( /opt/TangoService ) of the project directory: $ docker exec -it tango bash root@a5b77b5267b1:/opt/TangoService/Tango# cd .. root@a5b77b5267b1:/opt/TangoService# ls Tango tango_job_manager_log.log tango_log.log root@a5b77b5267b1:/opt/TangoService# tail -f -n +1 tango_job_manager_log.log tango_log.log","title":"Checking Tango Logs"},{"location":"installation/docker-compose/#troubleshooting-autolabtango-connection","text":"In the Autolab container, try to curl Tango: root@be56be775428:/home/app/webapp# curl tango:3000 Hello, world! RESTful Tango here! In the Tango container, try to curl Autolab: root@a5b77b5267b1:/opt/TangoService/Tango# curl autolab <html> <head><title>301 Moved Permanently</title></head> <body bgcolor = \"white\" > <center><h1>301 Moved Permanently</h1></center> <hr><center>nginx/1.14.0 ( Ubuntu ) </center> </body> </html>","title":"Troubleshooting Autolab/Tango Connection"},{"location":"installation/docker-compose/#permission-issues-in-autolab","text":"Run the following again: make set-perms","title":"Permission issues in Autolab"},{"location":"installation/docker-compose/#restarting-autolab-passenger-server","text":"This is useful when you might want to test out some code change within the Autolab container without having to rebuild everything again. These changes can be applied by just restarting the Passenger service that is serving Autolab. Run passenger-config restart-app : root@8b56488b3fb6:/home/app/webapp# passenger-config restart-app Please select the application to restart. Tip: re-run this command with --help to learn how to automate it. If the menu doesn 't display correctly, press ' ! ' \u2023 /home/app/webapp ( production ) Cancel Restarting /home/app/webapp ( production )","title":"Restarting Autolab Passenger Server"},{"location":"installation/docker-compose/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"installation/docker-compose/#error-unable-to-unlink-old-dbschemarb-permission-denied","text":"If you obtain the following error when attempting to perform make update : error: unable to unlink old 'db/schema.rb' : Permission denied fatal: Could not reset index file to revision 'HEAD' . This is due to the fact that db/schema.rb is updated whenever migrations are performed. db/schema.rb documents the database schema, which depends on the database that you are using, its version, and when the migrations were run. It is likely that your db/schema.rb will diverge from the one generated by the devs. You can resolve this by changing the owner of the files to be your current user, and then running make set-perms afterwards when you start the containers again.","title":"error: unable to unlink old 'db/schema.rb': Permission denied"},{"location":"installation/docker-compose/#autograder-logs-not-appearing","text":"This happens when you are accessing Autolab via localhost, as Tango will attempt to send the autograder logs to its own localhost instead. To remedy this, add 127.0.0.1 autolab to /etc/hosts and access Autolab via http://autolab instead of http://localhost . If you are accessing Autolab on a different host, add <your public_ip> <your fqdn> autolab to your /etc/hosts file and access Autolab via http://autolab .","title":"Autograder logs not appearing"},{"location":"installation/github_integration/","text":"Github Integration Setup In order to setup Github submission, you will first need to create a Github Application and get its corresponding Client ID and Client Secrets. After that, you only need to update the .env file with the information. The full steps are given in the following sections. Creating your Github Application Navigate to the Github developer settings in order to create a new OAuth app Fill in the required form fields: Homepage URL should be the URL that Autolab will be served on (i.e https://my-autolab-deployment.com ). Authorization callback URL is the URL that Github will make a callback to after authentication, which must be the homepage URL appended with /users/github_oauth_callback (i.e [https://my-autolab-deployment.com/users/github_oauth_callback) Application name and description should be something helpful to allow students to trust the OAuth application (i.e CMU Autolab ) An example: After registering the application, you will now have a Client ID for your application. Create a Client Secret for the Client ID, you should see something like the following: Configuring Github Integration for Autolab Ensure that you already have the Github application credentials set up based on the previous section On Autolab, navigate to Manage Autolab > Configure Autolab > Github Integration Enter your Github Client ID and Client Secret into the corresponding fields and click Save Under Github Integration Status , you should now see Autolab is connected to Github!","title":"Github Integration Setup"},{"location":"installation/github_integration/#github-integration-setup","text":"In order to setup Github submission, you will first need to create a Github Application and get its corresponding Client ID and Client Secrets. After that, you only need to update the .env file with the information. The full steps are given in the following sections.","title":"Github Integration Setup"},{"location":"installation/github_integration/#creating-your-github-application","text":"Navigate to the Github developer settings in order to create a new OAuth app Fill in the required form fields: Homepage URL should be the URL that Autolab will be served on (i.e https://my-autolab-deployment.com ). Authorization callback URL is the URL that Github will make a callback to after authentication, which must be the homepage URL appended with /users/github_oauth_callback (i.e [https://my-autolab-deployment.com/users/github_oauth_callback) Application name and description should be something helpful to allow students to trust the OAuth application (i.e CMU Autolab ) An example: After registering the application, you will now have a Client ID for your application. Create a Client Secret for the Client ID, you should see something like the following:","title":"Creating your Github Application"},{"location":"installation/github_integration/#configuring-github-integration-for-autolab","text":"Ensure that you already have the Github application credentials set up based on the previous section On Autolab, navigate to Manage Autolab > Configure Autolab > Github Integration Enter your Github Client ID and Client Secret into the corresponding fields and click Save Under Github Integration Status , you should now see Autolab is connected to Github!","title":"Configuring Github Integration for Autolab"},{"location":"installation/lti_integration/","text":"LTI Integration Setup Autolab supports a limited integration with Learning Tools Interoperability , or LTI. Specifically, Autolab supports LTI Advantage, with support for linking courses on an LTI platform to Autolab in order to pull roster information and sync the member lists of Autolab with the LTI platform. The LTI Advantage integration features have been built to mainly work with Canvas, but it should be possible to integrate with other platforms such as Moodle, though this has not been tested. Please contact us if you run into any issues with the integration process as this feature is still a work-in-progress. Definitions For the purposes of LTI Integration, a \"platform\" is an LMS such as Canvas or Moodle, while Autolab falls under the \"tool\" category. Generate a JWK for your Autolab instance JSON Web Keys (JWKs) make up the backbone of LTI Advantage's security specification. As part of the LTI Advantage Specification, Autolab requires a public-private keypair of RSA JWKs in order to sign and verify any signatures originating from Autolab towards other LTI platforms. To generate a JWK keypair, you may use a website , or a library such as ruby-jwt . Make sure that you specify that the key use is for signatures, using the RS256 algorithm, and include a key-id. An example JWK keypair is below. Save the keypair for later configuration steps. Public-Private Keypair: { \"p\" : \"_VqrjPDQHEce2zwxnzlBVu_6gZwfwBftu8yX8Q1np7nw9StC16e4oi8vKtTHc6hy-byOU-JyKV0Dj9LZXF_r5_HZlCEVCg9J4oopsINAwi1ekWRWj3pGCJaJ6M9QdWTd3Q0zzWVowdeDmwfWGQKesoM7O4JkxzFRV1w-8GqQYyM\" , \"kty\" : \"RSA\" , \"q\" : \"jzrFTIhNP80RUzqX4lYESsBjBhkyXY7nSsOJXLtUmie0LG_-aqJ0nSRi5Q4hhDsou3VjMWj-7QvwrgYw4GF5ktlDdEN874TuIRY2LeuWa1WlsYg7QN31G89tPFb7IxlxY9D0KG0Tg3NkXkZ1t2OgQZZeY_uTsY5yJ9e1Zb0Lpec\" , \"d\" : \"BSZb18GcM7lo72-sB84aOpUrkfhgmECsWBMfXF9fbIv8jsYt7mc5leduANHSf7aFTS39XQtGZUoZ6DLT2b-DhF2_VJCjm0r2P-YnzHV8QJ9iQKu9tEeclRFR7xeFh0HE39f28n_DKSsvUfkMjPY79Jyw7ctYezLoUJFEP1UI9W1zVbJgRn8GHv438qKGzwJWKWfNVMbNuW9eLLkR5ejPZMts_0cLF3e6JLOZCL2x6yrrqwE8bXehUHDIsVnE-YT0yHWiuNge5YZaicHhQDRqEQZ80KYUzLCecstEPCCyB5UPVP-dhuZhkM0xfadwI61oGFaF5HSNjsW-gjxsE7RIHQ\" , \"e\" : \"AQAB\" , \"use\" : \"sig\" , \"kid\" : \"Nm4-PaiSvgIwk7OlFn0wK_64bt402rsHmu5zz8fTOyE\" , \"qi\" : \"PFp5xgEbWmzjgtxHBGvsBxm8YXKUf3xhm3fpBiXYIZo7LMf7PERQzIWb4amFqq0CBfShYgm9tFwdt2ldIQXvhqY2n3bp-Jp1M5P_YD89FR5-YJLk9PnFfEOEJnD1fqRILMWXUGJxL1YxLMIXXkbbhozrh2-2C-Rku7MFdIMA4UI\" , \"dp\" : \"10SkQPQYj_5qHHPP4e9fMkRjwJ7sZ2atjAVyENKIAX6_W0Gv4NdUzRF0bVswQXiegM4SzPm6cKTkst6_63phUsovSmq1mr-U0tT8SUusyZCNKtJrriuBuChY86S7Q5Q542olWt9QnGvHGgIVi1iriRhySsUnZgzkhWsO225gUqs\" , \"alg\" : \"RS256\" , \"dq\" : \"LKWoTPQMCIBXDzerks03Z0nTVFpUXd-m76JGVla6x5bqzhNXHkxnx3hJL5eZEQwT5WNxOTy-gov_SW_6mmcoK4N4SGylLFCmnj-7QdM3P1wiW1XZCp5lwnaFWZLPlCBhPTksctGVjJtSxSR3m2P915QU9lu8rVN2-D6AeCdW0y8\" , \"n\" : \"jb_PoRxF0KaCMASsZwQyZFtoF6X19geXJJsx8QbAcsQEKt8RX34eAueVXn1K49UaGDkK-G9UCDmLDYTKMgjz1mtFKuV2J6CwowplkBq9rE_fUgkSY0XfLC3pCRSaQ6kjwwUjbFjF7tWVQHFhTgjqQ85HA5Pd3ix1yHnMPaNZ08CwucAx1st_WLauEmqdkmfXNIA65S5CO8EXxo94CVJ-DIZ3X7HDJq0m28SRKMR7sPM1q8A3a3z_n7DzIytjRyQkLcCWQq9oLT5dTuvAHz3Hasb1hqGqy9uS3RCFvjXk3GW3JMonVfhJ7310gUCAojEqsQ06vtoLp0g0QsjTUbADlQ\" } Public key: { \"kty\" : \"RSA\" , \"e\" : \"AQAB\" , \"use\" : \"sig\" , \"kid\" : \"Nm4-PaiSvgIwk7OlFn0wK_64bt402rsHmu5zz8fTOyE\" , \"alg\" : \"RS256\" , \"n\" : \"jb_PoRxF0KaCMASsZwQyZFtoF6X19geXJJsx8QbAcsQEKt8RX34eAueVXn1K49UaGDkK-G9UCDmLDYTKMgjz1mtFKuV2J6CwowplkBq9rE_fUgkSY0XfLC3pCRSaQ6kjwwUjbFjF7tWVQHFhTgjqQ85HA5Pd3ix1yHnMPaNZ08CwucAx1st_WLauEmqdkmfXNIA65S5CO8EXxo94CVJ-DIZ3X7HDJq0m28SRKMR7sPM1q8A3a3z_n7DzIytjRyQkLcCWQq9oLT5dTuvAHz3Hasb1hqGqy9uS3RCFvjXk3GW3JMonVfhJ7310gUCAojEqsQ06vtoLp0g0QsjTUbADlQ\" } Set up Autolab as a tool on the LTI platform In order for Autolab to interface with LTI platforms, it will need to have a client_id generated for it by the platform. Canvas sometimes calls their client_id s as \"developer keys\". Generating a client_id on Canvas To generate a developer key, follow the guide provided by Canvas and the instructions below. NOTE: You must be a Canvas Administrator in order to generate a developer key. During the process, fill in the JSON below with your Autolab domain, and also include your public JWK generated in the previous step. Then, in the step \"Select Configuration Method\" use the \"Paste JSON\" method and paste in this JSON object. Save your configuration, and then save the client_id generated by Canvas. This will be used to fill in the configuration file on Autolab's end as well. { \"title\" : \"Autolab Integration\" , \"description\" : \"Autolab is an open-source autograding service developed by students, for students\" , \"oidc_initiation_url\" : \"https://<your-autolab-domain>/lti_launch/oidc_login\" , \"target_link_uri\" : \"https://<your-autolab-domain>/lti_launch/launch\" , \"scopes\" : [ \"https://purl.imsglobal.org/spec/lti-nrps/scope/contextmembership.readonly\" ], \"extensions\" :[ { \"tool_id\" : \"autolab-integration\" , \"platform\" : \"canvas.instructure.com\" , \"domain\" : \"<your-autolab-domain>\" , \"privacy_level\" : \"public\" , \"settings\" : { \"text\" : \"Launch Autolab Integration\" , \"icon_url\" : \"<your-icon>\" , \"placements\" : [ { \"text\" : \"Autolab\" , \"enabled\" : true , \"placement\" : \"course_navigation\" , \"message_type\" : \"LtiResourceLinkRequest\" , \"target_link_uri\" : \"https://<your-autolab-domain>/lti_launch/launch\" , \"icon_url\" : \"<your-icon>\" , \"windowTarget\" : \"_blank\" } ] } } ], \"public_jwk\" :{ \"kty\" : \"RSA\" , \"e\" : \"AQAB\" , \"use\" : \"sig\" , \"kid\" : \"vLkAvsYBQk5KF3lRxYaUgOWodo_-PL5WrzKLNMcwke0\" , \"alg\" : \"RS256\" , \"n\" : \"iHN9QvbOTCTlyQwiyUKCAF5iLyAMGPZSnS46okSH5EZv0k3B65k0DdQr8b454RfwOABp7FgXKOEG4oMG62GiFoWebf1nKVBF5O80QOHZquTZLXYPMBKW9FVB0oDol-pzzNmqX0iDPBnCsoII3S8_sDn5V4ur3LUKM2j7oBBphhAPiin8Oh64gnAPS5nlnJmaV8VIbOdpQgzLLHPH4jIfjFhvIKzwRf1kqQGZsUaGYhrGZTusPOLJ0nBHlNh5cEEjbfp0oEvsNJoMzF0COZaMt2d89G7-oaVE64vcEc4rRbW4g1nL4NbeO8xh1Vkhp4rsqL8Zw__DHNue-8kJQt2LUw\" } } Add Autolab to your Course on Canvas Once Autolab has been configured and a client_id has been generated, Instructors are then able to add Autolab via the client_id . Go to \"Settings\", and click on the \"Apps\" tab, then click on \"+ App\". Select Configuration Type \"By Client ID\", and paste in the client_id generated in the previous step. You should receive a confirmation pop-up similar to \"Tool \"Autolab Integration Tool\" found for client ID . Would you like to install it?\" Click \"Install\". Refresh your Course page and Autolab should appear on the Course Navigation Panel on the left-hand side. Set up Autolab's LTI configuration settings Setting up Autolab to function as an LTI Advantage Tool requires setting up the LTI Tool Configuration specified in the \"LTI Configuration Settings\" Page. Go to \"LTI Configuration Settings\" via the \"Manage Autolab\" Dropdown or by navigating to /lti_config/index The \"LTI Configuration Settings\" should look like the following image. Here, Admins are able to specify a single LTI platform which they can link with. Fill in the text-fields with your platform-specific information. For example, you should paste in the client_id created by your platform into the \"Developer Key\" field. To find the right values for \"iss URL\", \"LTI Launch Auth URL\", \"LTI platform public JWK URL\", and \"Platform OAuth2 Endpoint\", please consult your respective platforms' guides on LTI integration. For example, for Canvas please consult \"Configuring Canvas in the Tool\" on this page . A summary of some of the values needed for configuring a Canvas integration in Autolab are provided below: iss URL: https://canvas.instructure.com LTI Launch Auth URL: https://<your-canvas-domain>/api/lti/authorize_redirect Platform OAuth2 Endpoint: https://<your-canvas-domain>/login/oauth2/token LTI platform public JWK URL: https://<your-canvas-domain>/api/lti/security/jwks There is a choice of using the \"LTI platform public JWK URL\" field or uploading a public JWK as a JSON file field, depending on which is defined in the Configuration. However, it is highly recommended to use \"LTI platform public JWK URL\" as most platforms use multiple private keys to sign their JWTs, which is not supported when using the file upload method. Therefore, Autolab will default to using \"LTI platform public JWK URL\", even if a platform public JWK is uploaded. Upload Autolab's private JWK as a JSON file in the \"Upload Autolab's JWK as a JSON file\" field. The format of the JWK is shown in the sample below: { \"p\": \"-ZtUu_QDlz9XX218fNCUkGLwggD9-w86WRE5R7uON9W6p7mme8l2z2dOyTTftkiHTee4dN2Cww3c5-EV-6QCiIapPCwbIFQxsXeMNatPjA2IthiqQPUM1R7Rc9KkwRYIBz6J-1sjWsAQlJDM3WOGgabr_VY13KsG6wOPw2lrCF0\", \"kty\": \"RSA\", \"q\": \"yDAwjdLNfzZXXGK3b_Unmx_jvlCzKqF4ysYVtB6ULg3LgKGLBgGhcYMHnCU2-KeTLRsJYaE8CbigiJaWtC2QfkYbLZ6yU8v8kM5wum7hENEgm8PINbCNxRTTYJM9Y5LTzKnNMPYu1DXUx7iC_TkeAX0Gj-7rKBaTAdmdXvmbieM\", \"d\": \"CJYgyd0xavwIjOocVL4lYj2FsdMxRDhu13457a5v3Sq_OCfPMTL5_pzE187DN0eCBSOGUTmTYbsiJVkOeO-cHZoOjsRFIM2hrDPLAjud00a6dPoLNlPNwQ5IQppHXMbP2r7Uq30Y1hXXA7aQMnH0205Wv7bgWVd1yBO-d0LlqgA1jqQx_8XTIOdv3CQgBKOXWtvYhBgrOpUoeozL9xm2qbl3bjwvr1cniaJhhP_vUNVjphdxpA1aczOr0ZYJNebwrEoNndnEGlIYr0MkQmatnGmw-al5cn20_DhGgzmxykwXMYfWpt9xM5S7q8DgtC0v-kdh2sF3-KPW5yIwXL-NqQ\", \"e\": \"AQAB\", \"use\": \"sig\", \"kid\": \"PdwZOKsxtweAiBua4y_aBYmKfMe8L2dBj60AmhzR0bw\", \"qi\": \"q82CtY2kzU_k-EaIeaEsJ6mTaG0YgKHtAmLohY5GQSOgYPMnvN7vLSCdgilXzVZCmNo4zu5CuHSO7cOhbfPFr2VP9rkNnhxG2bIhfuUOWn5vap2AQz5K6KD9HuRW6h4g8JJR1zL2FtdYkOZArQAEuwg9YDesHk6xUTIKR3pemK4\", \"dp\": \"Pe6pnp0UCwIfZsEew0Vpp021STx_yDxmCNV6Ne82gWoZjyZERbCeNyX16XyiCXODhvP4055mpIkbB7nUn4R5UHDBKvnynRnm3pbABk0ERsbQ5gXGsKlczsB_zdI1KOeThGCjEefyJMFFG-e1vTTFmgPVyB0M7jzNUaCnmh_c-80\", \"alg\": \"RS256\", \"dq\": \"q23pBmpxI_ErGqhGog90XTkP1FhTNbyVLkA3McnF5zJVBNBRt1EKKaSljaeozYLjXAr9G6fxO_npL06Vu7IRPLFYcNanq27R2EeQ7XYqMjaEEB-2gZOxtAXDhb5RIcYIrgjy-Gy5aWy3zFhLhAG3mlqwle1pXykFtt3eEAj8kzU\", \"n\": \"wzBWqiHbQQn-IXI2xjxkjURHVZb2ROEauJ0eRXiiSN4GeTWp9QEVL5Lo4BZKPM0OtXWbXxdoRjHkhA7m3rVqZZkwHKp6z9ncHIuEk0ep8l_gXL92OJCRWONvhJ7xQ8RihlIsbTLowGSdfDBMGWjOeZRiNqpRLeQKQmCon_RcbZIDDA28f6zZnC59nkxE7SA-MsCpF_JDgPYvzXoBytU2gqif2YRCFZSlgMBX-jqVybeaqjxDq3-IXNVCwpsWSMChU3JyIWm7K6czNbkOEytf9qnMV7yHg9_-n7ty8arTDhN1N4y7yXiM0gQx-UQsZe80Jy0Um6U2UHU2NRfHRrEvdw\" } Once all settings have been filled in, click \"Update Configuration\", and the LTI Configuration should be live on the server. it should be possible to launch Autolab from your specified platform, given your configuration is correct.","title":"LTI Advantage Integration Setup"},{"location":"installation/lti_integration/#lti-integration-setup","text":"Autolab supports a limited integration with Learning Tools Interoperability , or LTI. Specifically, Autolab supports LTI Advantage, with support for linking courses on an LTI platform to Autolab in order to pull roster information and sync the member lists of Autolab with the LTI platform. The LTI Advantage integration features have been built to mainly work with Canvas, but it should be possible to integrate with other platforms such as Moodle, though this has not been tested. Please contact us if you run into any issues with the integration process as this feature is still a work-in-progress.","title":"LTI Integration Setup"},{"location":"installation/lti_integration/#definitions","text":"For the purposes of LTI Integration, a \"platform\" is an LMS such as Canvas or Moodle, while Autolab falls under the \"tool\" category.","title":"Definitions"},{"location":"installation/lti_integration/#generate-a-jwk-for-your-autolab-instance","text":"JSON Web Keys (JWKs) make up the backbone of LTI Advantage's security specification. As part of the LTI Advantage Specification, Autolab requires a public-private keypair of RSA JWKs in order to sign and verify any signatures originating from Autolab towards other LTI platforms. To generate a JWK keypair, you may use a website , or a library such as ruby-jwt . Make sure that you specify that the key use is for signatures, using the RS256 algorithm, and include a key-id. An example JWK keypair is below. Save the keypair for later configuration steps. Public-Private Keypair: { \"p\" : \"_VqrjPDQHEce2zwxnzlBVu_6gZwfwBftu8yX8Q1np7nw9StC16e4oi8vKtTHc6hy-byOU-JyKV0Dj9LZXF_r5_HZlCEVCg9J4oopsINAwi1ekWRWj3pGCJaJ6M9QdWTd3Q0zzWVowdeDmwfWGQKesoM7O4JkxzFRV1w-8GqQYyM\" , \"kty\" : \"RSA\" , \"q\" : \"jzrFTIhNP80RUzqX4lYESsBjBhkyXY7nSsOJXLtUmie0LG_-aqJ0nSRi5Q4hhDsou3VjMWj-7QvwrgYw4GF5ktlDdEN874TuIRY2LeuWa1WlsYg7QN31G89tPFb7IxlxY9D0KG0Tg3NkXkZ1t2OgQZZeY_uTsY5yJ9e1Zb0Lpec\" , \"d\" : \"BSZb18GcM7lo72-sB84aOpUrkfhgmECsWBMfXF9fbIv8jsYt7mc5leduANHSf7aFTS39XQtGZUoZ6DLT2b-DhF2_VJCjm0r2P-YnzHV8QJ9iQKu9tEeclRFR7xeFh0HE39f28n_DKSsvUfkMjPY79Jyw7ctYezLoUJFEP1UI9W1zVbJgRn8GHv438qKGzwJWKWfNVMbNuW9eLLkR5ejPZMts_0cLF3e6JLOZCL2x6yrrqwE8bXehUHDIsVnE-YT0yHWiuNge5YZaicHhQDRqEQZ80KYUzLCecstEPCCyB5UPVP-dhuZhkM0xfadwI61oGFaF5HSNjsW-gjxsE7RIHQ\" , \"e\" : \"AQAB\" , \"use\" : \"sig\" , \"kid\" : \"Nm4-PaiSvgIwk7OlFn0wK_64bt402rsHmu5zz8fTOyE\" , \"qi\" : \"PFp5xgEbWmzjgtxHBGvsBxm8YXKUf3xhm3fpBiXYIZo7LMf7PERQzIWb4amFqq0CBfShYgm9tFwdt2ldIQXvhqY2n3bp-Jp1M5P_YD89FR5-YJLk9PnFfEOEJnD1fqRILMWXUGJxL1YxLMIXXkbbhozrh2-2C-Rku7MFdIMA4UI\" , \"dp\" : \"10SkQPQYj_5qHHPP4e9fMkRjwJ7sZ2atjAVyENKIAX6_W0Gv4NdUzRF0bVswQXiegM4SzPm6cKTkst6_63phUsovSmq1mr-U0tT8SUusyZCNKtJrriuBuChY86S7Q5Q542olWt9QnGvHGgIVi1iriRhySsUnZgzkhWsO225gUqs\" , \"alg\" : \"RS256\" , \"dq\" : \"LKWoTPQMCIBXDzerks03Z0nTVFpUXd-m76JGVla6x5bqzhNXHkxnx3hJL5eZEQwT5WNxOTy-gov_SW_6mmcoK4N4SGylLFCmnj-7QdM3P1wiW1XZCp5lwnaFWZLPlCBhPTksctGVjJtSxSR3m2P915QU9lu8rVN2-D6AeCdW0y8\" , \"n\" : \"jb_PoRxF0KaCMASsZwQyZFtoF6X19geXJJsx8QbAcsQEKt8RX34eAueVXn1K49UaGDkK-G9UCDmLDYTKMgjz1mtFKuV2J6CwowplkBq9rE_fUgkSY0XfLC3pCRSaQ6kjwwUjbFjF7tWVQHFhTgjqQ85HA5Pd3ix1yHnMPaNZ08CwucAx1st_WLauEmqdkmfXNIA65S5CO8EXxo94CVJ-DIZ3X7HDJq0m28SRKMR7sPM1q8A3a3z_n7DzIytjRyQkLcCWQq9oLT5dTuvAHz3Hasb1hqGqy9uS3RCFvjXk3GW3JMonVfhJ7310gUCAojEqsQ06vtoLp0g0QsjTUbADlQ\" } Public key: { \"kty\" : \"RSA\" , \"e\" : \"AQAB\" , \"use\" : \"sig\" , \"kid\" : \"Nm4-PaiSvgIwk7OlFn0wK_64bt402rsHmu5zz8fTOyE\" , \"alg\" : \"RS256\" , \"n\" : \"jb_PoRxF0KaCMASsZwQyZFtoF6X19geXJJsx8QbAcsQEKt8RX34eAueVXn1K49UaGDkK-G9UCDmLDYTKMgjz1mtFKuV2J6CwowplkBq9rE_fUgkSY0XfLC3pCRSaQ6kjwwUjbFjF7tWVQHFhTgjqQ85HA5Pd3ix1yHnMPaNZ08CwucAx1st_WLauEmqdkmfXNIA65S5CO8EXxo94CVJ-DIZ3X7HDJq0m28SRKMR7sPM1q8A3a3z_n7DzIytjRyQkLcCWQq9oLT5dTuvAHz3Hasb1hqGqy9uS3RCFvjXk3GW3JMonVfhJ7310gUCAojEqsQ06vtoLp0g0QsjTUbADlQ\" }","title":"Generate a JWK for your Autolab instance"},{"location":"installation/lti_integration/#set-up-autolab-as-a-tool-on-the-lti-platform","text":"In order for Autolab to interface with LTI platforms, it will need to have a client_id generated for it by the platform. Canvas sometimes calls their client_id s as \"developer keys\".","title":"Set up Autolab as a tool on the LTI platform"},{"location":"installation/lti_integration/#generating-a-client_id-on-canvas","text":"To generate a developer key, follow the guide provided by Canvas and the instructions below. NOTE: You must be a Canvas Administrator in order to generate a developer key. During the process, fill in the JSON below with your Autolab domain, and also include your public JWK generated in the previous step. Then, in the step \"Select Configuration Method\" use the \"Paste JSON\" method and paste in this JSON object. Save your configuration, and then save the client_id generated by Canvas. This will be used to fill in the configuration file on Autolab's end as well. { \"title\" : \"Autolab Integration\" , \"description\" : \"Autolab is an open-source autograding service developed by students, for students\" , \"oidc_initiation_url\" : \"https://<your-autolab-domain>/lti_launch/oidc_login\" , \"target_link_uri\" : \"https://<your-autolab-domain>/lti_launch/launch\" , \"scopes\" : [ \"https://purl.imsglobal.org/spec/lti-nrps/scope/contextmembership.readonly\" ], \"extensions\" :[ { \"tool_id\" : \"autolab-integration\" , \"platform\" : \"canvas.instructure.com\" , \"domain\" : \"<your-autolab-domain>\" , \"privacy_level\" : \"public\" , \"settings\" : { \"text\" : \"Launch Autolab Integration\" , \"icon_url\" : \"<your-icon>\" , \"placements\" : [ { \"text\" : \"Autolab\" , \"enabled\" : true , \"placement\" : \"course_navigation\" , \"message_type\" : \"LtiResourceLinkRequest\" , \"target_link_uri\" : \"https://<your-autolab-domain>/lti_launch/launch\" , \"icon_url\" : \"<your-icon>\" , \"windowTarget\" : \"_blank\" } ] } } ], \"public_jwk\" :{ \"kty\" : \"RSA\" , \"e\" : \"AQAB\" , \"use\" : \"sig\" , \"kid\" : \"vLkAvsYBQk5KF3lRxYaUgOWodo_-PL5WrzKLNMcwke0\" , \"alg\" : \"RS256\" , \"n\" : \"iHN9QvbOTCTlyQwiyUKCAF5iLyAMGPZSnS46okSH5EZv0k3B65k0DdQr8b454RfwOABp7FgXKOEG4oMG62GiFoWebf1nKVBF5O80QOHZquTZLXYPMBKW9FVB0oDol-pzzNmqX0iDPBnCsoII3S8_sDn5V4ur3LUKM2j7oBBphhAPiin8Oh64gnAPS5nlnJmaV8VIbOdpQgzLLHPH4jIfjFhvIKzwRf1kqQGZsUaGYhrGZTusPOLJ0nBHlNh5cEEjbfp0oEvsNJoMzF0COZaMt2d89G7-oaVE64vcEc4rRbW4g1nL4NbeO8xh1Vkhp4rsqL8Zw__DHNue-8kJQt2LUw\" } }","title":"Generating a client_id on Canvas"},{"location":"installation/lti_integration/#add-autolab-to-your-course-on-canvas","text":"Once Autolab has been configured and a client_id has been generated, Instructors are then able to add Autolab via the client_id . Go to \"Settings\", and click on the \"Apps\" tab, then click on \"+ App\". Select Configuration Type \"By Client ID\", and paste in the client_id generated in the previous step. You should receive a confirmation pop-up similar to \"Tool \"Autolab Integration Tool\" found for client ID . Would you like to install it?\" Click \"Install\". Refresh your Course page and Autolab should appear on the Course Navigation Panel on the left-hand side.","title":"Add Autolab to your Course on Canvas"},{"location":"installation/lti_integration/#set-up-autolabs-lti-configuration-settings","text":"Setting up Autolab to function as an LTI Advantage Tool requires setting up the LTI Tool Configuration specified in the \"LTI Configuration Settings\" Page. Go to \"LTI Configuration Settings\" via the \"Manage Autolab\" Dropdown or by navigating to /lti_config/index The \"LTI Configuration Settings\" should look like the following image. Here, Admins are able to specify a single LTI platform which they can link with. Fill in the text-fields with your platform-specific information. For example, you should paste in the client_id created by your platform into the \"Developer Key\" field. To find the right values for \"iss URL\", \"LTI Launch Auth URL\", \"LTI platform public JWK URL\", and \"Platform OAuth2 Endpoint\", please consult your respective platforms' guides on LTI integration. For example, for Canvas please consult \"Configuring Canvas in the Tool\" on this page . A summary of some of the values needed for configuring a Canvas integration in Autolab are provided below: iss URL: https://canvas.instructure.com LTI Launch Auth URL: https://<your-canvas-domain>/api/lti/authorize_redirect Platform OAuth2 Endpoint: https://<your-canvas-domain>/login/oauth2/token LTI platform public JWK URL: https://<your-canvas-domain>/api/lti/security/jwks There is a choice of using the \"LTI platform public JWK URL\" field or uploading a public JWK as a JSON file field, depending on which is defined in the Configuration. However, it is highly recommended to use \"LTI platform public JWK URL\" as most platforms use multiple private keys to sign their JWTs, which is not supported when using the file upload method. Therefore, Autolab will default to using \"LTI platform public JWK URL\", even if a platform public JWK is uploaded. Upload Autolab's private JWK as a JSON file in the \"Upload Autolab's JWK as a JSON file\" field. The format of the JWK is shown in the sample below: { \"p\": \"-ZtUu_QDlz9XX218fNCUkGLwggD9-w86WRE5R7uON9W6p7mme8l2z2dOyTTftkiHTee4dN2Cww3c5-EV-6QCiIapPCwbIFQxsXeMNatPjA2IthiqQPUM1R7Rc9KkwRYIBz6J-1sjWsAQlJDM3WOGgabr_VY13KsG6wOPw2lrCF0\", \"kty\": \"RSA\", \"q\": \"yDAwjdLNfzZXXGK3b_Unmx_jvlCzKqF4ysYVtB6ULg3LgKGLBgGhcYMHnCU2-KeTLRsJYaE8CbigiJaWtC2QfkYbLZ6yU8v8kM5wum7hENEgm8PINbCNxRTTYJM9Y5LTzKnNMPYu1DXUx7iC_TkeAX0Gj-7rKBaTAdmdXvmbieM\", \"d\": \"CJYgyd0xavwIjOocVL4lYj2FsdMxRDhu13457a5v3Sq_OCfPMTL5_pzE187DN0eCBSOGUTmTYbsiJVkOeO-cHZoOjsRFIM2hrDPLAjud00a6dPoLNlPNwQ5IQppHXMbP2r7Uq30Y1hXXA7aQMnH0205Wv7bgWVd1yBO-d0LlqgA1jqQx_8XTIOdv3CQgBKOXWtvYhBgrOpUoeozL9xm2qbl3bjwvr1cniaJhhP_vUNVjphdxpA1aczOr0ZYJNebwrEoNndnEGlIYr0MkQmatnGmw-al5cn20_DhGgzmxykwXMYfWpt9xM5S7q8DgtC0v-kdh2sF3-KPW5yIwXL-NqQ\", \"e\": \"AQAB\", \"use\": \"sig\", \"kid\": \"PdwZOKsxtweAiBua4y_aBYmKfMe8L2dBj60AmhzR0bw\", \"qi\": \"q82CtY2kzU_k-EaIeaEsJ6mTaG0YgKHtAmLohY5GQSOgYPMnvN7vLSCdgilXzVZCmNo4zu5CuHSO7cOhbfPFr2VP9rkNnhxG2bIhfuUOWn5vap2AQz5K6KD9HuRW6h4g8JJR1zL2FtdYkOZArQAEuwg9YDesHk6xUTIKR3pemK4\", \"dp\": \"Pe6pnp0UCwIfZsEew0Vpp021STx_yDxmCNV6Ne82gWoZjyZERbCeNyX16XyiCXODhvP4055mpIkbB7nUn4R5UHDBKvnynRnm3pbABk0ERsbQ5gXGsKlczsB_zdI1KOeThGCjEefyJMFFG-e1vTTFmgPVyB0M7jzNUaCnmh_c-80\", \"alg\": \"RS256\", \"dq\": \"q23pBmpxI_ErGqhGog90XTkP1FhTNbyVLkA3McnF5zJVBNBRt1EKKaSljaeozYLjXAr9G6fxO_npL06Vu7IRPLFYcNanq27R2EeQ7XYqMjaEEB-2gZOxtAXDhb5RIcYIrgjy-Gy5aWy3zFhLhAG3mlqwle1pXykFtt3eEAj8kzU\", \"n\": \"wzBWqiHbQQn-IXI2xjxkjURHVZb2ROEauJ0eRXiiSN4GeTWp9QEVL5Lo4BZKPM0OtXWbXxdoRjHkhA7m3rVqZZkwHKp6z9ncHIuEk0ep8l_gXL92OJCRWONvhJ7xQ8RihlIsbTLowGSdfDBMGWjOeZRiNqpRLeQKQmCon_RcbZIDDA28f6zZnC59nkxE7SA-MsCpF_JDgPYvzXoBytU2gqif2YRCFZSlgMBX-jqVybeaqjxDq3-IXNVCwpsWSMChU3JyIWm7K6czNbkOEytf9qnMV7yHg9_-n7ty8arTDhN1N4y7yXiM0gQx-UQsZe80Jy0Um6U2UHU2NRfHRrEvdw\" } Once all settings have been filled in, click \"Update Configuration\", and the LTI Configuration should be live on the server. it should be possible to launch Autolab from your specified platform, given your configuration is correct.","title":"Set up Autolab's LTI configuration settings"},{"location":"installation/mailing/","text":"Mailing Setup Autolab requires mailing to allow users to register accounts and reset passwords. You will also be able to make announcements through Autolab as well. The recommended approach is to setup Autolab to use a SMTP server, such as mailgun , SendGrid , Amazon SES or any other valid SMTP mail servers to send out email. We intend these instructions mainly for production usage. Mailing for Autolab version >=3.0.0 To set up Autolab (>=3.0.0) for a custom SMTP Server, configure settings by clicking Manage Autolab > Configure Autolab > SMTP Config . I don't have a domain name, will mailing work? Mailing has been tested to work with SendGrid without a domain name (using the IP of the server as the domain name for the purposes of the configuration above), although the absence of a domain name will likely result in the email to be flagged as spam. For the purpose of testing, a testing mailbox service like MailTrap is recommended. What if I'm running an outdated version of Autolab? If you are running an Autolab version less than v3.0.0, refer to the documentation down below to configure SMTP. Mailing for Autolab Docker Installation To set Autolab Docker up for a custom SMTP Server, update the following in .env that was created for you. Update the host domain HOST_PROTOCOL=http HOST_DOMAIN=example.com Update custom smtp settings SMTP_SETTINGS_ADDRESS=smtp.example.com SMTP_SETTINGS_PORT=25 SMTP_SETTINGS_ENABLE_STARTTLS_AUTO=true SMTP_SETTINGS_AUTHENTICATION=login SMTP_SETTINGS_USER_NAME=example SMTP_SETTINGS_PASSWORD=example SMTP_SETTINGS_DOMAIN=example.com Refer to the SMTP settings instructions that your selected service provides you such as SendGrid SMTP for Ruby on Rails , Amazon SES . Update from setting SMTP_DEFAULT_FROM = from @example . com Here the from address must be an address that your SMTP service permits you to send from. Oftentimes it is the same as your user_name in the smtp settings. After which, doing a docker-compose down followed by docker-compose up -d will allow you to see the changes. Mailing for Autolab Manual Installation To set Autolab up to use a custom SMTP Server, you will need to make edits to the .env and production.rb files that you have created. (If you would like to test it in development, add the following settings into development.rb ). Both production.rb and development.rb are located under config/environments Update the host domain of your Autolab instance in .env MAILER_HOST=yourhost.com The value should be the domain in which Autolab is hosted on. (e.g. autolab.andrew.cmu.edu ) Update the custom smtp server settings in production.rb config . action_mailer . smtp_settings = { address : 'smtp.example.com' , port : 25 , enable_starttls_auto : true , authentication : 'plain' , # Other options include: 'login', 'cram_md5', 'none' user_name : 'example' , password : 'example' , domain : 'example.com' , } Refer to the SMTP settings instructions that your selected service provides you such as SendGrid SMTP for Ruby on Rails , Amazon SES . Update the \"from\" setting in production.rb ActionMailer :: Base . default :from => 'something@example.com' Here the from address must be an address that your SMTP service permits you to send from. Oftentimes it is the same as your user_name in the smtp settings. Make sure to restart your Autolab client to see the changes.","title":"Mailing Setup"},{"location":"installation/mailing/#mailing-setup","text":"Autolab requires mailing to allow users to register accounts and reset passwords. You will also be able to make announcements through Autolab as well. The recommended approach is to setup Autolab to use a SMTP server, such as mailgun , SendGrid , Amazon SES or any other valid SMTP mail servers to send out email. We intend these instructions mainly for production usage.","title":"Mailing Setup"},{"location":"installation/mailing/#mailing-for-autolab-version-300","text":"To set up Autolab (>=3.0.0) for a custom SMTP Server, configure settings by clicking Manage Autolab > Configure Autolab > SMTP Config .","title":"Mailing for Autolab version &gt;=3.0.0"},{"location":"installation/mailing/#i-dont-have-a-domain-name-will-mailing-work","text":"Mailing has been tested to work with SendGrid without a domain name (using the IP of the server as the domain name for the purposes of the configuration above), although the absence of a domain name will likely result in the email to be flagged as spam. For the purpose of testing, a testing mailbox service like MailTrap is recommended.","title":"I don't have a domain name, will mailing work?"},{"location":"installation/mailing/#what-if-im-running-an-outdated-version-of-autolab","text":"If you are running an Autolab version less than v3.0.0, refer to the documentation down below to configure SMTP.","title":"What if I'm running an outdated version of Autolab?"},{"location":"installation/mailing/#mailing-for-autolab-docker-installation","text":"To set Autolab Docker up for a custom SMTP Server, update the following in .env that was created for you. Update the host domain HOST_PROTOCOL=http HOST_DOMAIN=example.com Update custom smtp settings SMTP_SETTINGS_ADDRESS=smtp.example.com SMTP_SETTINGS_PORT=25 SMTP_SETTINGS_ENABLE_STARTTLS_AUTO=true SMTP_SETTINGS_AUTHENTICATION=login SMTP_SETTINGS_USER_NAME=example SMTP_SETTINGS_PASSWORD=example SMTP_SETTINGS_DOMAIN=example.com Refer to the SMTP settings instructions that your selected service provides you such as SendGrid SMTP for Ruby on Rails , Amazon SES . Update from setting SMTP_DEFAULT_FROM = from @example . com Here the from address must be an address that your SMTP service permits you to send from. Oftentimes it is the same as your user_name in the smtp settings. After which, doing a docker-compose down followed by docker-compose up -d will allow you to see the changes.","title":"Mailing for Autolab Docker Installation"},{"location":"installation/mailing/#mailing-for-autolab-manual-installation","text":"To set Autolab up to use a custom SMTP Server, you will need to make edits to the .env and production.rb files that you have created. (If you would like to test it in development, add the following settings into development.rb ). Both production.rb and development.rb are located under config/environments Update the host domain of your Autolab instance in .env MAILER_HOST=yourhost.com The value should be the domain in which Autolab is hosted on. (e.g. autolab.andrew.cmu.edu ) Update the custom smtp server settings in production.rb config . action_mailer . smtp_settings = { address : 'smtp.example.com' , port : 25 , enable_starttls_auto : true , authentication : 'plain' , # Other options include: 'login', 'cram_md5', 'none' user_name : 'example' , password : 'example' , domain : 'example.com' , } Refer to the SMTP settings instructions that your selected service provides you such as SendGrid SMTP for Ruby on Rails , Amazon SES . Update the \"from\" setting in production.rb ActionMailer :: Base . default :from => 'something@example.com' Here the from address must be an address that your SMTP service permits you to send from. Oftentimes it is the same as your user_name in the smtp settings. Make sure to restart your Autolab client to see the changes.","title":"Mailing for Autolab Manual Installation"},{"location":"installation/osx/","text":"This page provides instructions on installing Autolab for development on Mac OSX 10.11+. If you encounter any issue along the way, check out Troubleshooting . Follow the step-by-step instructions below: Install one of two database options SQLite should only be used in development MySQL can be used in development or production Install homebrew Install rbenv and ruby-build using homebrew: brew install rbenv ruby-build Restart your shell at this point in order to start using your newly installed rbenv. Clone the Autolab repo into home directory and enter it: cd ~/ git clone https://github.com/autolab/Autolab.git && cd Autolab Install the correct version of ruby: rbenv install $( cat .ruby-version ) At this point, confirm that rbenv is working (you might need to restart your shell): $ which ruby ~/.rbenv/shims/ruby $ which rake ~/.rbenv/shims/rake Note that Mac OSX comes with its own installation of ruby. You might need to switch your ruby from the system version to the rbenv installed version. One option is to add the following lines to ~/.bash_profile: export RBENV_ROOT = <rbenv folder path on your local machine> eval \" $( rbenv init - ) \" Install bundler : gem install bundler rbenv rehash Install the required gems (run the following commands in the cloned Autolab repo): cd bin bundle install Refer to Troubleshooting for issues installing gems. Install the universal-ctags package: brew install --HEAD universal-ctags/universal-ctags/universal-ctags Afterward, run which ctags to ensure that the package lies on your PATH and can be found. Initialize Autolab Configs cp config/database.yml.template config/database.yml cp config/school.yml.template config/school.yml cp config/autogradeConfig.rb.template config/autogradeConfig.rb Edit school.yml with your school/organization specific names and emails. Edit database.yml with the correct credentials for your chosen database. Refer to Troubleshooting for any issues and suggested development configurations . Create a .env file to store Autolab configuration constants. cp .env.template .env If you have not installed Tango yet, you do not need to do anything else in this stage. If you have already installed Tango, you should make sure to fill in the .env file with values consistent with Tango's config.py Initialize application secrets. ./bin/initialize_secrets.sh Create and initialize the database tables: bundle exec rails db:create bundle exec rails db:migrate Do not forget to use bundle exec in front of every rake/rails command. Create initial root user, pass the -d flag for developmental deployments: # For production: ./bin/initialize_user.sh # For development: ./bin/initialize_user.sh -d Populate dummy data (for development only): bundle exec rails autolab:populate Start the rails server: bundle exec rails s -p 3000 Go to localhost:3000 and login with either the credentials of the root user you just created, or choose Developer Login with: Email: \"admin@foo.bar\" . Install Tango , the backend autograding service. Information on linking Autolab to Tango can be found on this page as well. If you would like to configure Github integration to allow students to submit via Github, please follow the Github integration setup instructions . If you would like to configure LTI integration to link Autolab courses to LTI platforms, please follow the LTI integration setup instructions . Now you are all set to start using Autolab! Please fill out this form to join our registry so that we can provide you with news about the latest features, bug-fixes, and security updates. For more info, visit the Guide for Instructors and Guide for Lab Authors .","title":"Mac OSX 10.11+"},{"location":"installation/overview/","text":"The installation process comprises installing both Autolab and Tango (the autograding backend). MySQL and Redis will also need to be installed in most use cases. There are 2 different ways to install Autolab and Tango: The simplest and fastest way to get up and running is to use our Docker Compose installation , which is ideal for most workloads. Both production-ready and testing deployments are available. This will set-up containers for Autolab, Tango, and other required services. You can also install Autolab manually. There are instructions for installing Autolab on Ubuntu 22.04 and on Mac OSX 10.11+ . The instructions for installing Tango manually are the same for both environments. Most of our users prefer the Docker Compose installation method as it is simpler, production-ready, and comes deployed with MySQL and TLS/SSL. RAM requirements You may face issues on a machine with less than 2GB of RAM as the gem sassc takes a significant amount of RAM to install.","title":"Installation Overview"},{"location":"installation/tango-troubleshoot/","text":"This is a general list of Tango-related issues that we get often. If you are encountering or find a solution to an issue not mentioned here, please let us know on our Slack . Clearing Tango job queue Due to faulty configs or other reasons, you may have a large backlog of jobs waiting to run that are stuck. Restarting Tango does not solve this issue as the jobs are persisted on a Redis queue. You can drop everything in Redis using the redis-cli client as follows: $ redis-cli 127 .0.0.1:6379> FLUSHALL OK 127 .0.0.1:6379> Tango jobs completed but scores are not updated If you are accessing Autolab on localhost, Tango will attempt to send the autograder logs to its own localhost instead. To fix this, add 127.0.0.1 autolab to your /etc/hosts file and access Autolab via http://autolab instead of http://localhost . If you are accessing Autolab on a different host, add <your public_ip> <your fqdn> autolab to your /etc/hosts file and access Autolab via http://autolab .","title":"Troubleshooting"},{"location":"installation/tango-troubleshoot/#clearing-tango-job-queue","text":"Due to faulty configs or other reasons, you may have a large backlog of jobs waiting to run that are stuck. Restarting Tango does not solve this issue as the jobs are persisted on a Redis queue. You can drop everything in Redis using the redis-cli client as follows: $ redis-cli 127 .0.0.1:6379> FLUSHALL OK 127 .0.0.1:6379>","title":"Clearing Tango job queue"},{"location":"installation/tango-troubleshoot/#tango-jobs-completed-but-scores-are-not-updated","text":"If you are accessing Autolab on localhost, Tango will attempt to send the autograder logs to its own localhost instead. To fix this, add 127.0.0.1 autolab to your /etc/hosts file and access Autolab via http://autolab instead of http://localhost . If you are accessing Autolab on a different host, add <your public_ip> <your fqdn> autolab to your /etc/hosts file and access Autolab via http://autolab .","title":"Tango jobs completed but scores are not updated"},{"location":"installation/tango/","text":"Tango Installation This guide provides instructions for installing Tango on either a development environment or a production environment . Development Installation This guide shows how to setup Tango in a development environment . Use the production installation guide for installing in a production environment . Obtain the source code. git clone https://github.com/autolab/Tango.git ; cd Tango Install Redis following this guide . By default, Tango uses Redis as a stateless job queue. Learn more here . Create a config.py file from the given template. cp config.template.py config.py Create the course labs directory where job's output files will go, organized by key and lab name: mkdir courselabs By default the COURSELABS option in config.py points to the courselabs directory in the Tango directory. Change this to specify another path if you wish. Set up a VMMS for Tango to use. Docker ( recommended ) Amazon EC2 TashiVMMS (deprecated) Run the following commands to setup the Tango dev environment inside the Tango directory. Install pip if needed. pip install virtualenv virtualenv env source env/bin/activate pip install -r requirements.txt mkdir volumes If you are using Docker, set DOCKER_VOLUME_PATH in config.py to be the path to the volumes directory you just created. DOCKER_VOLUME_PATH = \"/path/to/Tango/volumes/\" Start Redis by running the following command: redis-server Run the following command to start the server (producer). If no port is given, the server will run on the port specified in config.py (default: 3000): python restful_tango/server.py <port> Open another terminal window and start the job manager (consumer): python jobManager.py For more information on the job producer/consumer model check out our blog post . Ensure Tango is running: curl localhost:<port> # Hello, world! RESTful Tango here! You can test the Tango setup using the command line client . If you are using Tango with Autolab, you have to configure Autolab to use Tango. Go to your Autolab directory and enter the following commands: cp config/autogradeConfig.rb.template config/autogradeConfig.rb Then in your Autolab installation's .env file, fill in the correct info for your Tango deployment, mainly the following: # Hostname for Tango RESTful API RESTFUL_HOST = \"foo.bar.edu\" #(if you are running Tango locally, then it is just \"localhost\") # Port for Tango RESTful API RESTFUL_PORT = \"3000\" # Key for Tango RESTful API RESTFUL_KEY = \"test\" # change this in production to a secret phrase Note that by default Autolab also uses a default port of 3000 , so be sure to change the port if you are developing on localhost . See below for instructions on how to deploy Tango in a standalone production environment. Production Installation This is a guide to setup a fully self-sufficient Tango deployment environment out-of-the-box using Docker. The suggested deployment pattern for Tango uses Nginx as a proxy and Supervisor as a process manager for Tango and all its dependencies. All requests to Nginx are rerouted to a Tango process. Details Nginx default port - 8600 Tango ports - 8610, 8611 Redis port - 6379 You can change any of these in the respective config files in deployment/config/ before you build the tango_deployment image. Steps Clone the Tango repo $ git clone https://github.com/autolab/Tango.git ; cd Tango Create a config.py file from the given template. $ cp config.template.py config.py Modify DOCKER_VOLUME_PATH in config.py as follows: DOCKER_VOLUME_PATH = '/opt/TangoService/Tango/volumes/' Install docker on the host machine by following instructions on the docker installation page . Then give yourself permissions to run docker without root (need to relog in after): $ sudo usermod -aG docker $USER Ensure docker is running: $ sudo service docker start $ docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Run the following command to build the Tango deployment image. $ docker build --tag = \"tango_deployment\" . Ensure the image was built by running. $ docker images # REPOSITORY TAG IMAGE ID CREATED SIZE # tango_deployment latest 3c0d4f4b4958 2 minutes ago 742.6 MB # ubuntu 15.04 d1b55fd07600 4 minutes ago 131.3 MB Run the following command to access the image in a container with a bash shell. The -p flag will map nginxPort on the docker container to localPort (8610 recommended) on your local machine (or on the VM that docker is running in on the local machine) so that Tango is accessible from outside the docker container. $ docker run --privileged -p <localPort>:<nginxPort> -it tango_deployment /bin/bash Set up a VMMS for Tango within the Docker container. Docker ( recommended ) Amazon EC2 Run the following command to start supervisor, which will then start Tango and all its dependencies. $ service supervisor start Check to see if Tango is responding to requests $ curl localhost:8610 # Hello, world! RESTful Tango here! Once you have a VMMS set up, leave the tango_deployment container by typing exit and once back in the host shell run the following command to get the name of your production container. $ docker ps -as # CONTAINER ID IMAGE COMMAND NAMES SIZE # c704d45c3737 tango_deployment \"/bin/bash\" erwin 40.26 MB The container created in this example has the name ` erwin ` . The name of the production container can be changed by running the following command and will be used to run the container and create services. $ docker rename <old_name> <new_name> To reopen the container once it has been built use the following command. This will reopen the interactive shell within the container and allow for configuration of the container after its initial run. $ docker start erwin $ docker attach erwin Once the container is set up with the autograding image, and the VMMS configured with any necessary software/environments needed for autograding (java, perl, etc), some configurations need to be changed to make the container daemon ready. Using the CONTAINER ID above, use the following commands to modify that containers config.v2.json file. $ sudo ls /var/lib/docker/containers c704d45c37372a034cb97761d99f6f3f362707cc23d689734895e017eda3e55b $ sudo vim /var/lib/docker/containers/c704d45c37372a034cb97761d99f6f3f362707cc23d689734895e017eda3e55b/config.v2.json Edit the \"Path\" field in the config.v2.json file from \"/bin/bash\" to \"/usr/bin/supervisord\" and save the file. Run the following commands to verify the changes were successful. The COMMAND field should now be \"/usr/bin/supervisord\" $ service docker restart $ docker ps -as # CONTAINER ID IMAGE COMMAND NAMES SIZE # c704d45c3737 tango_deployment \"/usr/bin/supervisord\" erwin 40.26 MB At this point when the container is started, the environment is fully set up and will no longer be an interactive shell. Instead, it will be the supervisor service that starts Tango and all its dependencies. Test this with the following commands and ensure Tango is functioning properly. $ docker start erwin # (Test tango environment) $ docker stop erwin Test the setup by running sample jobs using the testing guide . The following steps are optional and should only be used if you would like the Tango container to start on system boot. To ensure Tango starts with the system in the production environment, the container needs to be configured as a service. Below is a sample service config file that needs to be changed to suit your environment and placed in /etc/systemd/system/ . The file should be named <name>.service . For this example, it is erwin.service . [ Unit ] Description = Docker Service Managing Tango Container Requires = docker.service After = docker.service [ Service ] Restart = always ExecStart = /usr/bin/docker start -a erwin ExecStop = /usr/bin/docker stop -t 2 erwin [ Install ] WantedBy = default.target Test and ensure the service was set up correctly. The service should start successfully and remain running. $ systemctl daemon-reload $ service erwin start $ service erwin status Enable the service at system startup and reboot and ensure it starts with the host. $ systemctl enable erwin.service $ sudo reboot # (Server Reboots) $ service erwin status Docker VMMS Setup This is a guide to set up Tango to run jobs inside Docker containers. Install docker on host machine by following instructions on the docker installation page . Then give yourself permissions to run docker without root (need to relog in after): sudo usermod -aG docker $USER Ensure docker is running: sudo service docker start docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Build base Docker image from root Tango directory. cd path/to/Tango docker build -t autograding_image vmms/ docker images autograding_image # Check if image built Update VMMS_NAME in config.py . # in config.py VMMS_NAME = \"localDocker\" Amazon EC2 VMMS Setup This is a guide to set up Tango to run jobs on an Amazon EC2 VM. Create an AWS Account or use an existing one. Obtain your access_key_id and secret_access_key by following the instructions here . Add AWS Credentials to a file called ~/.boto using the following format: [ Credentials ] aws_access_key_id = MYAMAZONTESTKEY12345 aws_secret_access_key = myawssecretaccesskey12345 Tango uses the Boto Python package to interface with Amazon Web Services In the AWS EC2 console, create an Ubuntu 14.04+ EC2 instance and save the .pem file in a safe location. Copy the directory and contents of autodriver/ in the Tango repo into the EC2 VM. For more help connecting to the EC2 instance follow this guide . chmod 400 /path/my-key-pair.pem scp -i /path/my-key-pair.pem -r autodriver/ ubuntu@<ec2-host-name>.compute-1.amazonaws.com:~/ The autodriver is used as a sandbox environment to run the job inside the VM. It limits Disk I/O, Disk Usage, monitors security, and controls other valuable sudo level resources. In the EC2 VM, compile the autodriver. $ cd autodriver/ $ make clean ; make $ cp -p autodriver /usr/bin/autodriver Create the autograde Linux user and directory. All jobs will be run under this user. $ useradd autograde $ mkdir autograde $ chown autograde autograde $ chown :autograde autograde In the AWS EC2 console, create an AMI image from your EC2 VM. Use this guide to create a custom AMI. Exit the EC2 instance and edit the following values in config.py in the Tango directory. # VMMS to use. Must be set to a VMMS implemented in vmms/ before # starting Tango. Options are: \"localDocker\", \"distDocker\", # \"tashiSSH\", and \"ec2SSH\" VMMS_NAME = \"ec2SSH\" ###### # Part 5: EC2 Constants # EC2_REGION = 'us-east-1' # EC2 Region EC2_USER_NAME = 'ubuntu' # EC2 username DEFAULT_AMI = 'ami-4c99c35b' # Custom AMI Id DEFAULT_INST_TYPE = 't2.micro' # Instance Type DEFAULT_SECURITY_GROUP = 'autolab-autograde-ec2' # Security Group with full access to EC2 SECURITY_KEY_PATH = '/path/to/my-key-pair.pem' # Absolute path to my-key-pair.pem DYNAMIC_SECURITY_KEY_PATH = '' # Leave blank SECURITY_KEY_NAME = 'my-key-pair' # Name of the key file. Ex: if file name is 'my-key-pair.pem', fill value with 'my-key-pair' TANGO_RESERVATION_ID = '1' # Leave as 1 INSTANCE_RUNNING = 16 # Status code of a running instance, leave as 16 You should now be ready to run Tango jobs on EC2! Use the Tango CLI to test your setup.","title":"Installation"},{"location":"installation/tango/#tango-installation","text":"This guide provides instructions for installing Tango on either a development environment or a production environment .","title":"Tango Installation"},{"location":"installation/tango/#development-installation","text":"This guide shows how to setup Tango in a development environment . Use the production installation guide for installing in a production environment . Obtain the source code. git clone https://github.com/autolab/Tango.git ; cd Tango Install Redis following this guide . By default, Tango uses Redis as a stateless job queue. Learn more here . Create a config.py file from the given template. cp config.template.py config.py Create the course labs directory where job's output files will go, organized by key and lab name: mkdir courselabs By default the COURSELABS option in config.py points to the courselabs directory in the Tango directory. Change this to specify another path if you wish. Set up a VMMS for Tango to use. Docker ( recommended ) Amazon EC2 TashiVMMS (deprecated) Run the following commands to setup the Tango dev environment inside the Tango directory. Install pip if needed. pip install virtualenv virtualenv env source env/bin/activate pip install -r requirements.txt mkdir volumes If you are using Docker, set DOCKER_VOLUME_PATH in config.py to be the path to the volumes directory you just created. DOCKER_VOLUME_PATH = \"/path/to/Tango/volumes/\" Start Redis by running the following command: redis-server Run the following command to start the server (producer). If no port is given, the server will run on the port specified in config.py (default: 3000): python restful_tango/server.py <port> Open another terminal window and start the job manager (consumer): python jobManager.py For more information on the job producer/consumer model check out our blog post . Ensure Tango is running: curl localhost:<port> # Hello, world! RESTful Tango here! You can test the Tango setup using the command line client . If you are using Tango with Autolab, you have to configure Autolab to use Tango. Go to your Autolab directory and enter the following commands: cp config/autogradeConfig.rb.template config/autogradeConfig.rb Then in your Autolab installation's .env file, fill in the correct info for your Tango deployment, mainly the following: # Hostname for Tango RESTful API RESTFUL_HOST = \"foo.bar.edu\" #(if you are running Tango locally, then it is just \"localhost\") # Port for Tango RESTful API RESTFUL_PORT = \"3000\" # Key for Tango RESTful API RESTFUL_KEY = \"test\" # change this in production to a secret phrase Note that by default Autolab also uses a default port of 3000 , so be sure to change the port if you are developing on localhost . See below for instructions on how to deploy Tango in a standalone production environment.","title":"Development Installation"},{"location":"installation/tango/#production-installation","text":"This is a guide to setup a fully self-sufficient Tango deployment environment out-of-the-box using Docker. The suggested deployment pattern for Tango uses Nginx as a proxy and Supervisor as a process manager for Tango and all its dependencies. All requests to Nginx are rerouted to a Tango process.","title":"Production Installation"},{"location":"installation/tango/#details","text":"Nginx default port - 8600 Tango ports - 8610, 8611 Redis port - 6379 You can change any of these in the respective config files in deployment/config/ before you build the tango_deployment image.","title":"Details"},{"location":"installation/tango/#steps","text":"Clone the Tango repo $ git clone https://github.com/autolab/Tango.git ; cd Tango Create a config.py file from the given template. $ cp config.template.py config.py Modify DOCKER_VOLUME_PATH in config.py as follows: DOCKER_VOLUME_PATH = '/opt/TangoService/Tango/volumes/' Install docker on the host machine by following instructions on the docker installation page . Then give yourself permissions to run docker without root (need to relog in after): $ sudo usermod -aG docker $USER Ensure docker is running: $ sudo service docker start $ docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Run the following command to build the Tango deployment image. $ docker build --tag = \"tango_deployment\" . Ensure the image was built by running. $ docker images # REPOSITORY TAG IMAGE ID CREATED SIZE # tango_deployment latest 3c0d4f4b4958 2 minutes ago 742.6 MB # ubuntu 15.04 d1b55fd07600 4 minutes ago 131.3 MB Run the following command to access the image in a container with a bash shell. The -p flag will map nginxPort on the docker container to localPort (8610 recommended) on your local machine (or on the VM that docker is running in on the local machine) so that Tango is accessible from outside the docker container. $ docker run --privileged -p <localPort>:<nginxPort> -it tango_deployment /bin/bash Set up a VMMS for Tango within the Docker container. Docker ( recommended ) Amazon EC2 Run the following command to start supervisor, which will then start Tango and all its dependencies. $ service supervisor start Check to see if Tango is responding to requests $ curl localhost:8610 # Hello, world! RESTful Tango here! Once you have a VMMS set up, leave the tango_deployment container by typing exit and once back in the host shell run the following command to get the name of your production container. $ docker ps -as # CONTAINER ID IMAGE COMMAND NAMES SIZE # c704d45c3737 tango_deployment \"/bin/bash\" erwin 40.26 MB The container created in this example has the name ` erwin ` . The name of the production container can be changed by running the following command and will be used to run the container and create services. $ docker rename <old_name> <new_name> To reopen the container once it has been built use the following command. This will reopen the interactive shell within the container and allow for configuration of the container after its initial run. $ docker start erwin $ docker attach erwin Once the container is set up with the autograding image, and the VMMS configured with any necessary software/environments needed for autograding (java, perl, etc), some configurations need to be changed to make the container daemon ready. Using the CONTAINER ID above, use the following commands to modify that containers config.v2.json file. $ sudo ls /var/lib/docker/containers c704d45c37372a034cb97761d99f6f3f362707cc23d689734895e017eda3e55b $ sudo vim /var/lib/docker/containers/c704d45c37372a034cb97761d99f6f3f362707cc23d689734895e017eda3e55b/config.v2.json Edit the \"Path\" field in the config.v2.json file from \"/bin/bash\" to \"/usr/bin/supervisord\" and save the file. Run the following commands to verify the changes were successful. The COMMAND field should now be \"/usr/bin/supervisord\" $ service docker restart $ docker ps -as # CONTAINER ID IMAGE COMMAND NAMES SIZE # c704d45c3737 tango_deployment \"/usr/bin/supervisord\" erwin 40.26 MB At this point when the container is started, the environment is fully set up and will no longer be an interactive shell. Instead, it will be the supervisor service that starts Tango and all its dependencies. Test this with the following commands and ensure Tango is functioning properly. $ docker start erwin # (Test tango environment) $ docker stop erwin Test the setup by running sample jobs using the testing guide . The following steps are optional and should only be used if you would like the Tango container to start on system boot. To ensure Tango starts with the system in the production environment, the container needs to be configured as a service. Below is a sample service config file that needs to be changed to suit your environment and placed in /etc/systemd/system/ . The file should be named <name>.service . For this example, it is erwin.service . [ Unit ] Description = Docker Service Managing Tango Container Requires = docker.service After = docker.service [ Service ] Restart = always ExecStart = /usr/bin/docker start -a erwin ExecStop = /usr/bin/docker stop -t 2 erwin [ Install ] WantedBy = default.target Test and ensure the service was set up correctly. The service should start successfully and remain running. $ systemctl daemon-reload $ service erwin start $ service erwin status Enable the service at system startup and reboot and ensure it starts with the host. $ systemctl enable erwin.service $ sudo reboot # (Server Reboots) $ service erwin status","title":"Steps"},{"location":"installation/tango/#docker-vmms-setup","text":"This is a guide to set up Tango to run jobs inside Docker containers. Install docker on host machine by following instructions on the docker installation page . Then give yourself permissions to run docker without root (need to relog in after): sudo usermod -aG docker $USER Ensure docker is running: sudo service docker start docker ps # CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Build base Docker image from root Tango directory. cd path/to/Tango docker build -t autograding_image vmms/ docker images autograding_image # Check if image built Update VMMS_NAME in config.py . # in config.py VMMS_NAME = \"localDocker\"","title":"Docker VMMS Setup"},{"location":"installation/tango/#amazon-ec2-vmms-setup","text":"This is a guide to set up Tango to run jobs on an Amazon EC2 VM. Create an AWS Account or use an existing one. Obtain your access_key_id and secret_access_key by following the instructions here . Add AWS Credentials to a file called ~/.boto using the following format: [ Credentials ] aws_access_key_id = MYAMAZONTESTKEY12345 aws_secret_access_key = myawssecretaccesskey12345 Tango uses the Boto Python package to interface with Amazon Web Services In the AWS EC2 console, create an Ubuntu 14.04+ EC2 instance and save the .pem file in a safe location. Copy the directory and contents of autodriver/ in the Tango repo into the EC2 VM. For more help connecting to the EC2 instance follow this guide . chmod 400 /path/my-key-pair.pem scp -i /path/my-key-pair.pem -r autodriver/ ubuntu@<ec2-host-name>.compute-1.amazonaws.com:~/ The autodriver is used as a sandbox environment to run the job inside the VM. It limits Disk I/O, Disk Usage, monitors security, and controls other valuable sudo level resources. In the EC2 VM, compile the autodriver. $ cd autodriver/ $ make clean ; make $ cp -p autodriver /usr/bin/autodriver Create the autograde Linux user and directory. All jobs will be run under this user. $ useradd autograde $ mkdir autograde $ chown autograde autograde $ chown :autograde autograde In the AWS EC2 console, create an AMI image from your EC2 VM. Use this guide to create a custom AMI. Exit the EC2 instance and edit the following values in config.py in the Tango directory. # VMMS to use. Must be set to a VMMS implemented in vmms/ before # starting Tango. Options are: \"localDocker\", \"distDocker\", # \"tashiSSH\", and \"ec2SSH\" VMMS_NAME = \"ec2SSH\" ###### # Part 5: EC2 Constants # EC2_REGION = 'us-east-1' # EC2 Region EC2_USER_NAME = 'ubuntu' # EC2 username DEFAULT_AMI = 'ami-4c99c35b' # Custom AMI Id DEFAULT_INST_TYPE = 't2.micro' # Instance Type DEFAULT_SECURITY_GROUP = 'autolab-autograde-ec2' # Security Group with full access to EC2 SECURITY_KEY_PATH = '/path/to/my-key-pair.pem' # Absolute path to my-key-pair.pem DYNAMIC_SECURITY_KEY_PATH = '' # Leave blank SECURITY_KEY_NAME = 'my-key-pair' # Name of the key file. Ex: if file name is 'my-key-pair.pem', fill value with 'my-key-pair' TANGO_RESERVATION_ID = '1' # Leave as 1 INSTANCE_RUNNING = 16 # Status code of a running instance, leave as 16 You should now be ready to run Tango jobs on EC2! Use the Tango CLI to test your setup.","title":"Amazon EC2 VMMS Setup"},{"location":"installation/troubleshoot/","text":"This is a general list of questions that we get often. If you find a solution to an issue not mentioned here, please contact us at autolab-dev@andrew.cmu.edu Ubuntu Script Bugs If you get the following error Failed to fetch http://dl.google.com/linux/chrome/deb/dists/stable/Release Unable to find expected entry 'main/binary-i386/Packages' in Release file ( Wrong sources.list entry or malformed file ) then follow the solution in this post . Where do I find the MySQL username and password? If this is your first time logging into MySQL, your username is 'root'. You may also need to set the root password: Start the server: sudo /usr/local/mysql/support-files/mysql.server start Set the password: mysqladmin -u root password \"[New_Password]\" If you lost your root password, refer to the MySQL wiki Bundle Install Errors This happens as gems get updated. These fixes are gem-specific, but two common ones are eventmachine bundle config build.eventmachine --with-cppflags = -I/usr/local/opt/openssl/include libv8 bundle config build.libv8 --with-system-v8 Run bundle install again If this does not work, another option would be bundle update libv8 Because updating libv8 has dependency on other gems, it might fail due to a need to update other gems. Just do bundle update <gem> according to the error messages until all gems are up to date. Run bundle install again If neither of these works, try exploring this StackOverflow link mimemagic Another error that may occur on macOS when running bundle install is that the dependancy mimemagic may fail to install, causing bundle install to fail. A possible fix is if you have homebrew installed, run: brew install shared-mime-info bundle install For more information, read the following github issues link . Can't connect to local MySQL server through socket Make sure you've started the MySQL server and double-check the socket in config/database.yml The default socket location is /tmp/mysql.sock . I forgot my MySQL root password You can reset it following the instructions on this Stack Overflow post If mysql complains that the password is expired, follow the instructions on the second answer on this post MySQL Syntax Error If you get the following error Mysql2::Error: You have an error in your SQL syntax this may be an issue with using an incompatible version of MySQL. Try switching to MySQL 5.7 if you are currently using a different version. Undefined method 'devise' for User You most likely missed the step of copying config/initializers/devise.rb.template to config/initializers/devise.rb and setting your secret key in the setup instructions. Suggested Development Configuration for config/database.yml MySQL Change the and fields in config/database.yml to the username and password that has been set up for the mysql. For example if your username is user1 , and your password is 123456 , then your yml would be development : adapter : mysql2 database : autolab_development pool : 5 username : user1 password : '123456' socket : /var/run/mysqld/mysqld.sock # /tmp/ mysql . sock on Mac OSX host : localhost variables : sql_mode : NO_ENGINE_SUBSTITUTION test : adapter : mysql2 database : autolab_test pool : 5 username : user1 password : '123456' socket : /var/run/mysqld/mysqld.sock # /tmp/ mysql . sock on Mac OSX host : localhost variables : sql_mode : NO_ENGINE_SUBSTITUTION SQLite Comment out the configurations meant for MySQL in config/database.yml , and insert the following development : adapter : sqlite3 database : db / autolab_development pool : 5 timeout : 5000 test : adapter : sqlite3 database : db / autolab_test pool : 5 timeout : 5000 No space left on device If you get the following error No space left on device @ rb_sysopen - /path/to/autolab/tmp/cache/.raw_score_includes_unreleased [ ... ] it is likely that you need to purge the cache. This is because FileStore caches are not purged automatically. To purge the cache, click on Manage Autolab followed by Clear Cache to clear expired entries. Alternatively, run rake user:cleanup_cache (to clear expired entries) or rake user:clear_cache (to clear all entries) in your terminal. MacOS: OpenSSL error If you get the following error when trying to run rake or rails Library not loaded: libssl.1.1.dylib ( LoadError ) paths to OpenSSL may not be properly set up. To enable proper linking: brew install openssl@1.1 brew link openssl@1.1. --force You may need to export paths as well during this process, which homebrew will display. You may also need to reinstall the mysql2 gem (the following command is for Apple Silicon): gem install mysql2 -v \"{version}\" -- --with-ldflags = -L/opt/homebrew/opt/openssl@1.1/lib --with-cppflags = -I/opt/homebrew/opt/openssl@1.1/include","title":"Troubleshooting"},{"location":"installation/troubleshoot/#ubuntu-script-bugs","text":"If you get the following error Failed to fetch http://dl.google.com/linux/chrome/deb/dists/stable/Release Unable to find expected entry 'main/binary-i386/Packages' in Release file ( Wrong sources.list entry or malformed file ) then follow the solution in this post .","title":"Ubuntu Script Bugs"},{"location":"installation/troubleshoot/#where-do-i-find-the-mysql-username-and-password","text":"If this is your first time logging into MySQL, your username is 'root'. You may also need to set the root password: Start the server: sudo /usr/local/mysql/support-files/mysql.server start Set the password: mysqladmin -u root password \"[New_Password]\" If you lost your root password, refer to the MySQL wiki","title":"Where do I find the MySQL username and password?"},{"location":"installation/troubleshoot/#bundle-install-errors","text":"This happens as gems get updated. These fixes are gem-specific, but two common ones are","title":"Bundle Install Errors"},{"location":"installation/troubleshoot/#eventmachine","text":"bundle config build.eventmachine --with-cppflags = -I/usr/local/opt/openssl/include","title":"eventmachine"},{"location":"installation/troubleshoot/#libv8","text":"bundle config build.libv8 --with-system-v8 Run bundle install again If this does not work, another option would be bundle update libv8 Because updating libv8 has dependency on other gems, it might fail due to a need to update other gems. Just do bundle update <gem> according to the error messages until all gems are up to date. Run bundle install again If neither of these works, try exploring this StackOverflow link","title":"libv8"},{"location":"installation/troubleshoot/#mimemagic","text":"Another error that may occur on macOS when running bundle install is that the dependancy mimemagic may fail to install, causing bundle install to fail. A possible fix is if you have homebrew installed, run: brew install shared-mime-info bundle install For more information, read the following github issues link .","title":"mimemagic"},{"location":"installation/troubleshoot/#cant-connect-to-local-mysql-server-through-socket","text":"Make sure you've started the MySQL server and double-check the socket in config/database.yml The default socket location is /tmp/mysql.sock .","title":"Can't connect to local MySQL server through socket"},{"location":"installation/troubleshoot/#i-forgot-my-mysql-root-password","text":"You can reset it following the instructions on this Stack Overflow post If mysql complains that the password is expired, follow the instructions on the second answer on this post","title":"I forgot my MySQL root password"},{"location":"installation/troubleshoot/#mysql-syntax-error","text":"If you get the following error Mysql2::Error: You have an error in your SQL syntax this may be an issue with using an incompatible version of MySQL. Try switching to MySQL 5.7 if you are currently using a different version.","title":"MySQL Syntax Error"},{"location":"installation/troubleshoot/#undefined-method-devise-for-user","text":"You most likely missed the step of copying config/initializers/devise.rb.template to config/initializers/devise.rb and setting your secret key in the setup instructions.","title":"Undefined method 'devise' for User"},{"location":"installation/troubleshoot/#suggested-development-configuration-for-configdatabaseyml","text":"MySQL Change the and fields in config/database.yml to the username and password that has been set up for the mysql. For example if your username is user1 , and your password is 123456 , then your yml would be development : adapter : mysql2 database : autolab_development pool : 5 username : user1 password : '123456' socket : /var/run/mysqld/mysqld.sock # /tmp/ mysql . sock on Mac OSX host : localhost variables : sql_mode : NO_ENGINE_SUBSTITUTION test : adapter : mysql2 database : autolab_test pool : 5 username : user1 password : '123456' socket : /var/run/mysqld/mysqld.sock # /tmp/ mysql . sock on Mac OSX host : localhost variables : sql_mode : NO_ENGINE_SUBSTITUTION SQLite Comment out the configurations meant for MySQL in config/database.yml , and insert the following development : adapter : sqlite3 database : db / autolab_development pool : 5 timeout : 5000 test : adapter : sqlite3 database : db / autolab_test pool : 5 timeout : 5000","title":"Suggested Development Configuration for config/database.yml"},{"location":"installation/troubleshoot/#no-space-left-on-device","text":"If you get the following error No space left on device @ rb_sysopen - /path/to/autolab/tmp/cache/.raw_score_includes_unreleased [ ... ] it is likely that you need to purge the cache. This is because FileStore caches are not purged automatically. To purge the cache, click on Manage Autolab followed by Clear Cache to clear expired entries. Alternatively, run rake user:cleanup_cache (to clear expired entries) or rake user:clear_cache (to clear all entries) in your terminal.","title":"No space left on device"},{"location":"installation/troubleshoot/#macos-openssl-error","text":"If you get the following error when trying to run rake or rails Library not loaded: libssl.1.1.dylib ( LoadError ) paths to OpenSSL may not be properly set up. To enable proper linking: brew install openssl@1.1 brew link openssl@1.1. --force You may need to export paths as well during this process, which homebrew will display. You may also need to reinstall the mysql2 gem (the following command is for Apple Silicon): gem install mysql2 -v \"{version}\" -- --with-ldflags = -L/opt/homebrew/opt/openssl@1.1/lib --with-cppflags = -I/opt/homebrew/opt/openssl@1.1/include","title":"MacOS: OpenSSL error"},{"location":"installation/ubuntu/","text":"This page provides instructions on installing Autolab for development on Ubuntu 22.04 LTS. If you encounter any issue along the way, check out Troubleshooting . Upgrade system packages and installing prerequisites sudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential git libffi-dev zlib1g-dev autoconf bison libssl-dev libyaml-dev libreadline-dev libncurses5-dev libgdbm6 libgdbm-dev libmysqlclient-dev libjansson-dev universal-ctags sqlite3 libsqlite3-dev pkg-config Cloning Autolab repo from Github to ~/Autolab cd ~/ git clone https://github.com/autolab/Autolab.git cd Autolab Setting up rbenv and ruby-build plugin (You may ignore the message about adding a line to bashrc) git clone https://github.com/rbenv/rbenv.git ~/.rbenv echo 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' >> ~/.bashrc echo 'eval \"$(rbenv init -)\"' >> ~/.bashrc source ~/.bashrc ~/.rbenv/bin/rbenv init git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build Installing Ruby (Based on ruby version) cd Autolab rbenv install ` cat .ruby-version ` Installing MySQL (If you would just like to test Autolab, then you can skip this step by using SQLite) Following instructions from How to Install MySQL on Ubuntu sudo apt install mysql-server sudo service mysql start sudo mysql mysql> ALTER USER 'root' @ 'localhost' IDENTIFIED WITH mysql_native_password BY '<password>' ; mysql> exit ; sudo mysql_secure_installation > There will be a few questions asked during the MySQL setup. * Validate Password Plugin? N * Remove Anonymous Users? Y * Disallow Root Login Remotely? Y * Remove Test Database and Access to it? Y * Reload Privilege Tables Now? Y sudo mysql -u root -p mysql> ALTER USER 'root' @ 'localhost' IDENTIFIED WITH auth_socket ; mysql> exit ; (If you are using MySQL) Create a new user with access to autolab_test and autolab_development databases. Because a password rather than auth_socket is needed, we need to ensure that user uses mysql_native_password sudo mysql mysql> CREATE USER '<username>' @ 'localhost' IDENTIFIED WITH mysql_native_password BY '<password>' ; mysql> FLUSH PRIVILEGES ; mysql> exit ; Installing Rails cd Autolab gem install bundler -v $( tail -n1 Gemfile.lock ) bundle install Refer to Troubleshooting for issues installing gems. Initializing Autolab Configs cp config/database.yml.template config/database.yml cp config/school.yml.template config/school.yml cp config/autogradeConfig.rb.template config/autogradeConfig.rb Edit school.yml with your school/organization specific names and emails. Edit database.yml with the correct credentials for your chosen database. Refer to Troubleshooting for any issues and suggested development configurations . Create a .env file to store Autolab configuration constants cp .env.template .env If you have not installed Tango yet, you do not need to do anything else in this stage. If you have already installed Tango, you should make sure to fill in the .env file with values consistent with Tango's config.py Initialize application secrets ./bin/initialize_secrets.sh (Using MySQL) Editing Database YML. Change the and fields in config/database.yml to the username and password that has been set up for the mysql. For example if your username is user1 , and your password is 123456 , then your yml would be development : adapter : mysql2 database : autolab_development pool : 5 username : user1 password : '123456' socket : /var/run/mysqld/ mysqld . sock host : localhost variables : sql_mode : NO_ENGINE_SUBSTITUTION test : adapter : mysql2 database : autolab_test pool : 5 username : user1 password : '123456' socket : /var/run/mysqld/ mysqld . sock host : localhost variables : sql_mode : NO_ENGINE_SUBSTITUTION (Using SQLite) Editing Database YML. Comment out the configurations meant for MySQL in config/database.yml, and insert the following development : adapter : sqlite3 database : db / autolab_development pool : 5 timeout : 5000 test : adapter : sqlite3 database : db / autolab_test pool : 5 timeout : 5000 Granting permissions on the databases. Setting global sql mode is important to relax the rules of mysql when it comes to group by mode ( access mysql using your root first to grant permissions ) mysql> grant all privileges on autolab_development.* to '<username>' @localhost ; mysql> grant all privileges on autolab_test.* to '<username>' @localhost ; mysql> SET GLOBAL sql_mode =( SELECT REPLACE ( @@sql_mode, 'ONLY_FULL_GROUP_BY' , '' )) ; mysql> exit Initializing Autolab Database cd Autolab bundle exec rails db:create bundle exec rails db:reset bundle exec rails db:migrate Create initial root user, pass the -d flag for developmental deployments: # For production: ./bin/initialize_user.sh # For development: ./bin/initialize_user.sh -d If you are just testing Autolab, you can populate the database with sample course & students cd Autolab bundle exec rails autolab:populate Run Autolab! cd Autolab bundle exec rails s -p 3000 --binding = 0 .0.0.0 Visit localhost:3000 on your browser to view your local deployment of Autolab, and login with either the credentials of the root user you just created, or choose Developer Login with Email : \"admin@foo.bar\" Install Tango , the backend autograding service. Information on linking Autolab to Tango can be found on this page as well. If you would like to deploy the server, you can try out Phusion Passenger . If you would like to configure Github integration to allow students to submit via Github, please follow the Github integration setup instructions . If you would like to configure LTI integration to link Autolab courses to LTI platforms, please follow the LTI integration setup instructions . Now you are all set to start using Autolab! Please fill out this form to join our registry so that we can provide you with news about the latest features, bug-fixes, and security updates. For more info, visit the Guide for Instructors and Guide for Lab Authors .","title":"Ubuntu 22.04"}]}